{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8feef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Encoder Reranker with LoRA fine-tuning\n",
    "\n",
    "- Uses LoRA adapters (injected into all nn.Linear layers of the encoder)\n",
    "- Freezes base pretrained weights; trains only LoRA adapters + classifier head\n",
    "- Uses 90% train, 10% test split\n",
    "- Saves trained model (weights include LoRA adapters) and tokenizer to output_dir\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, queries: List[str], chunks: List[str], labels: List[int]):\n",
    "        assert len(queries) == len(chunks) == len(labels)\n",
    "        self.queries = queries\n",
    "        self.chunks = chunks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"query\": self.queries[idx],\n",
    "            \"chunk\": self.chunks[idx],\n",
    "            \"label\": float(self.labels[idx]),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict], tokenizer, max_length: int):\n",
    "    queries = [b[\"query\"] for b in batch]\n",
    "    chunks = [b[\"chunk\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.float)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        queries, chunks, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    if \"token_type_ids\" not in enc:\n",
    "        enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"token_type_ids\": enc[\"token_type_ids\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# ------------------- LoRA Implementation -------------------\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, orig_linear: nn.Linear, r: int = 8, alpha: float = 32.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.in_features = orig_linear.in_features\n",
    "        self.out_features = orig_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / max(1, r)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        self.weight = nn.Parameter(orig_linear.weight.data.clone(), requires_grad=False)\n",
    "        if orig_linear.bias is not None:\n",
    "            self.bias = nn.Parameter(orig_linear.bias.data.clone(), requires_grad=False)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        if r > 0:\n",
    "            self.A = nn.Parameter(torch.randn(r, self.in_features) * 0.01)\n",
    "            self.B = nn.Parameter(torch.zeros(self.out_features, r))\n",
    "        else:\n",
    "            self.A = None\n",
    "            self.B = None\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_p) if self.dropout_p > 0.0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = F.linear(x, self.weight, self.bias)\n",
    "        if self.r > 0:\n",
    "            orig_shape = x.shape\n",
    "            if x.dim() == 3:\n",
    "                N, S, D = x.shape\n",
    "                x_flat = x.reshape(-1, D)\n",
    "            else:\n",
    "                x_flat = x\n",
    "            x_drop = self.dropout(x_flat) if self.dropout is not None else x_flat\n",
    "            low_rank = (x_drop @ self.A.t()) @ self.B.t()\n",
    "            low_rank = low_rank * self.scaling\n",
    "            if x.dim() == 3:\n",
    "                low_rank = low_rank.view(N, S, self.out_features)\n",
    "                out = base + low_rank\n",
    "            else:\n",
    "                out = base + low_rank\n",
    "            return out\n",
    "        else:\n",
    "            return base\n",
    "\n",
    "def replace_linear_with_lora(module: nn.Module, r: int, alpha: float, dropout: float):\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.Linear):\n",
    "            lora_linear = LoRALinear(child, r=r, alpha=alpha, dropout=dropout)\n",
    "            setattr(module, name, lora_linear)\n",
    "        else:\n",
    "            replace_linear_with_lora(child, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "def inject_lora(model: nn.Module, r: int, alpha: float, dropout: float):\n",
    "    encoder = model.encoder\n",
    "    replace_linear_with_lora(encoder, r=r, alpha=alpha, dropout=dropout)\n",
    "    for name, p in encoder.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    for name, module in encoder.named_modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            if module.A is not None:\n",
    "                module.A.requires_grad = True\n",
    "            if module.B is not None:\n",
    "                module.B.requires_grad = True\n",
    "    return model\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "\n",
    "class CrossEncoderLoRA(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name_or_path)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        nn.init.normal_(self.classifier.weight, mean=0.0, std=self.encoder.config.initializer_range)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_emb)\n",
    "        logits = self.classifier(x).squeeze(-1)\n",
    "        return logits, cls_emb\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "\n",
    "def load_csv_dataset(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    assert {\"query\", \"chunk\", \"label\"} <= set(df.columns)\n",
    "    return df\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    all_logits, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.0\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"loss\": total_loss / len(labels)}\n",
    "\n",
    "def train_one_epoch(train_loader, model, optimizer, scheduler, device, max_grad_norm):\n",
    "    model.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "\n",
    "def main(args):\n",
    "    df = load_csv_dataset(args.data_csv)\n",
    "\n",
    "    # Split 90% train, 10% test\n",
    "    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    train_ds = PairDataset(train_df[\"query\"].tolist(), train_df[\"chunk\"].tolist(), train_df[\"label\"].tolist())\n",
    "    test_ds = PairDataset(test_df[\"query\"].tolist(), test_df[\"chunk\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
    "                              collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length))\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.eval_batch_size, shuffle=False,\n",
    "                             collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length))\n",
    "\n",
    "    # Build model and inject LoRA\n",
    "    model = CrossEncoderLoRA(args.model_name_or_path, dropout_prob=args.dropout)\n",
    "    model = inject_lora(model, r=args.lora_rank, alpha=args.lora_alpha, dropout=args.lora_dropout)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Ensure classifier is trainable\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Optimizer + scheduler\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(trainable_params, lr=args.lr)\n",
    "    total_steps = len(train_loader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(args.warmup_steps_ratio * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(args.epochs):\n",
    "        train_one_epoch(train_loader, model, optimizer, scheduler, args.device, args.max_grad_norm)\n",
    "        metrics = evaluate(model, train_loader, args.device)\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs} - Train AUC: {metrics['auc']:.4f}, Acc: {metrics['acc']:.4f}, Loss: {metrics['loss']:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_metrics = evaluate(model, test_loader, args.device)\n",
    "    print(\"\\nTest Set Results:\", test_metrics)\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(args.output_dir, \"model_with_lora.pt\"))\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(\"Saved model (with LoRA adapters) and tokenizer to\", args.output_dir)\n",
    "\n",
    "# ------------------- Args -------------------\n",
    "\n",
    "class Args:\n",
    "    data_csv = \"./training_pairs.csv\"\n",
    "    model_name_or_path = \"bert-base-uncased\"\n",
    "    output_dir = \"./crossenc_lora_out\"\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 16\n",
    "    eval_batch_size = 64\n",
    "    max_length = 256\n",
    "    lr = 3e-5\n",
    "    warmup_steps_ratio = 0.06\n",
    "    max_grad_norm = 1.0\n",
    "    dropout = 0.1\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    lora_rank = 32\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    args.device = torch.device(args.device)\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
