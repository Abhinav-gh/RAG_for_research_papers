{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "Grid params: {'dropout': 0.1, 'lr': 2e-05}\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 399\u001b[39m\n\u001b[32m    397\u001b[39m args = Args()\n\u001b[32m    398\u001b[39m args.device = torch.device(args.device)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 333\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    321\u001b[39m train_loader = DataLoader(\n\u001b[32m    322\u001b[39m     train_subset,\n\u001b[32m    323\u001b[39m     batch_size=args.batch_size,\n\u001b[32m    324\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    325\u001b[39m     collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m b: collate_fn(b, tokenizer, args.max_length),\n\u001b[32m    326\u001b[39m )\n\u001b[32m    327\u001b[39m val_loader = DataLoader(\n\u001b[32m    328\u001b[39m     val_subset,\n\u001b[32m    329\u001b[39m     batch_size=args.eval_batch_size,\n\u001b[32m    330\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    331\u001b[39m     collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m b: collate_fn(b, tokenizer, args.max_length),\n\u001b[32m    332\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m model, val_metrics = \u001b[43mtrain_one_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdropout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m fold_aucs.append(val_metrics[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    344\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36mtrain_one_fold\u001b[39m\u001b[34m(train_loader, val_loader, args, lr, dropout, lora_rank, lora_alpha, lora_dropout)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# inject LoRA into encoder\u001b[39;00m\n\u001b[32m    252\u001b[39m model = inject_lora(model, r=lora_rank, alpha=lora_alpha, dropout=lora_dropout)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Ensure classifier head is trainable\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.classifier.parameters():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Encoder Reranker with LoRA fine-tuning + Grid Search and K-Fold CV\n",
    "\n",
    "- Uses LoRA adapters (injected into all nn.Linear layers of the encoder)\n",
    "- Freezes base pretrained weights; trains only LoRA adapters + classifier head\n",
    "- Grid search over learning rate and dropout, 10-fold CV on train+val, held-out test (10%)\n",
    "- Saves best model (weights include LoRA adapters) and tokenizer to output_dir\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, queries: List[str], chunks: List[str], labels: List[int]):\n",
    "        assert len(queries) == len(chunks) == len(labels)\n",
    "        self.queries = queries\n",
    "        self.chunks = chunks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"query\": self.queries[idx],\n",
    "            \"chunk\": self.chunks[idx],\n",
    "            \"label\": float(self.labels[idx]),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict], tokenizer, max_length: int):\n",
    "    queries = [b[\"query\"] for b in batch]\n",
    "    chunks = [b[\"chunk\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.float)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        queries, chunks, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    # Some tokenizers/models (e.g., RoBERTa) don't return token_type_ids -> create zeros\n",
    "    if \"token_type_ids\" not in enc:\n",
    "        enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"token_type_ids\": enc[\"token_type_ids\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# ------------------- LoRA Implementation -------------------\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Replacement for nn.Linear that adds a low-rank update:\n",
    "      W_eff = W (frozen) + (B @ A) * scaling\n",
    "    Where:\n",
    "      - A: (r, in_features)\n",
    "      - B: (out_features, r)\n",
    "      - scaling = alpha / r\n",
    "    Only A and B (and optional dropout) are trainable by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, orig_linear: nn.Linear, r: int = 8, alpha: float = 32.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.in_features = orig_linear.in_features\n",
    "        self.out_features = orig_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / max(1, r)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        # copy original weight and bias and freeze them\n",
    "        self.weight = nn.Parameter(orig_linear.weight.data.clone(), requires_grad=False)\n",
    "        if orig_linear.bias is not None:\n",
    "            self.bias = nn.Parameter(orig_linear.bias.data.clone(), requires_grad=False)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # LoRA parameters (trainable)\n",
    "        if r > 0:\n",
    "            # A: (r, in_features)\n",
    "            self.A = nn.Parameter(torch.randn(r, self.in_features) * 0.01)\n",
    "            # B: (out_features, r)\n",
    "            self.B = nn.Parameter(torch.zeros(self.out_features, r))\n",
    "        else:\n",
    "            self.A = None\n",
    "            self.B = None\n",
    "\n",
    "        # optional dropout on input to adapter\n",
    "        self.dropout = nn.Dropout(self.dropout_p) if self.dropout_p > 0.0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, in_features) or (batch, in_features)\n",
    "        # compute base linear: using frozen weight\n",
    "        # use torch.nn.functional.linear to use weight.T semantics\n",
    "        base = F.linear(x, self.weight, self.bias)  # (.., out_features)\n",
    "\n",
    "        if self.r > 0:\n",
    "            # compute adapter output: (.., out_features)\n",
    "            # x_flat: (N, in_features) where N = batch*seq_len maybe\n",
    "            orig_shape = x.shape\n",
    "            if x.dim() == 3:\n",
    "                N, S, D = x.shape\n",
    "                x_flat = x.reshape(-1, D)  # (N*S, D)\n",
    "            else:\n",
    "                x_flat = x  # (N, D)\n",
    "\n",
    "            if self.dropout is not None:\n",
    "                x_drop = self.dropout(x_flat)\n",
    "            else:\n",
    "                x_drop = x_flat\n",
    "\n",
    "            # A @ x^T -> (r, N) then B @ (A x)^T -> (out_features, N) -> transpose -> (N, out_features)\n",
    "            # Efficient: (x_drop @ A.T) -> (N, r), then @ B.T -> (N, out_features)\n",
    "            low_rank = (x_drop @ self.A.t()) @ self.B.t()  # (N, out_features)\n",
    "            low_rank = low_rank * self.scaling\n",
    "            if x.dim() == 3:\n",
    "                low_rank = low_rank.view(N, S, self.out_features)\n",
    "                # match base shape (N,S,out)\n",
    "                out = base + low_rank\n",
    "            else:\n",
    "                out = base + low_rank\n",
    "            return out\n",
    "        else:\n",
    "            return base\n",
    "\n",
    "def replace_linear_with_lora(module: nn.Module, r: int, alpha: float, dropout: float, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively replace nn.Linear modules **inside** the given module with LoRALinear wrappers.\n",
    "    The original linear layers are replaced in-place.\n",
    "    \"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        child_prefix = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # create LoRA-wrapped module\n",
    "            lora_linear = LoRALinear(child, r=r, alpha=alpha, dropout=dropout)\n",
    "            # assign to parent\n",
    "            setattr(module, name, lora_linear)\n",
    "        else:\n",
    "            replace_linear_with_lora(child, r=r, alpha=alpha, dropout=dropout, prefix=child_prefix)\n",
    "\n",
    "def inject_lora(model: nn.Module, r: int, alpha: float, dropout: float):\n",
    "    \"\"\"\n",
    "    Inject LoRA into the encoder of the CrossEncoder (AutoModel).\n",
    "    We'll operate on model.encoder (AutoModel instance).\n",
    "    Freeze original base parameters and ensure only LoRA params + classifier are trainable.\n",
    "    \"\"\"\n",
    "    # Replace all nn.Linear inside encoder with LoRALinear\n",
    "    encoder = model.encoder\n",
    "    replace_linear_with_lora(encoder, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "    # Freeze all parameters of base encoder (the original weights are now frozen in LoRALinear,\n",
    "    # but other params may exist like LayerNorm, embeddings â€” to keep stable we freeze them)\n",
    "    for name, p in encoder.named_parameters():\n",
    "        # LoRA adapter parameters inside LoRALinear are trainable by default (A,B)\n",
    "        # They appear as parameters of encoder because LoRALinear registers A/B as parameters.\n",
    "        # Only freeze parameters that were originally from pretrained model: we detect them by attribute names\n",
    "        # but that's tricky; simpler: set requires_grad=False for all, then set True for LoRA and classifier.\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Now unfreeze LoRA adapter params (A and B) by searching for them\n",
    "    for name, module in encoder.named_modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            if module.A is not None:\n",
    "                module.A.requires_grad = True\n",
    "            if module.B is not None:\n",
    "                module.B.requires_grad = True\n",
    "            # LoRALinear has weight and bias frozen by design (requires_grad=False)\n",
    "    # Return model (modified in place)\n",
    "    return model\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "\n",
    "class CrossEncoderLoRA(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name_or_path)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        # initialize classifier\n",
    "        nn.init.normal_(self.classifier.weight, mean=0.0, std=self.encoder.config.initializer_range)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_emb)\n",
    "        logits = self.classifier(x).squeeze(-1)\n",
    "        return logits, cls_emb\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "\n",
    "def load_csv_dataset(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    assert {\"query\", \"chunk\", \"label\"} <= set(df.columns)\n",
    "    return df\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    all_logits, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.0\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"loss\": total_loss / len(labels)}\n",
    "\n",
    "def train_one_fold(train_loader, val_loader, args, lr, dropout, lora_rank, lora_alpha, lora_dropout):\n",
    "    # build model with LoRA injection\n",
    "    model = CrossEncoderLoRA(args.model_name_or_path, dropout_prob=dropout)\n",
    "    # inject LoRA into encoder\n",
    "    model = inject_lora(model, r=lora_rank, alpha=lora_alpha, dropout=lora_dropout)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Ensure classifier head is trainable\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Build optimizer with only trainable parameters (LoRA A/B and classifier)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(trainable_params, lr=lr)\n",
    "\n",
    "    total_steps = len(train_loader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(args.warmup_steps_ratio * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for _ in range(args.epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(args.device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(args.device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(args.device)\n",
    "            labels = batch[\"labels\"].to(args.device)\n",
    "\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    val_metrics = evaluate(model, val_loader, args.device)\n",
    "    return model, val_metrics\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "\n",
    "def main(args):\n",
    "    df = load_csv_dataset(args.data_csv)\n",
    "\n",
    "    # Split 90% train+val, 10% test\n",
    "    trainval_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "\n",
    "    trainval_ds = PairDataset(trainval_df[\"query\"].tolist(), trainval_df[\"chunk\"].tolist(), trainval_df[\"label\"].tolist())\n",
    "    test_ds = PairDataset(test_df[\"query\"].tolist(), test_df[\"chunk\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    param_grid = {\n",
    "        \"lr\": [2e-5, 3e-5, 5e-5],\n",
    "        \"dropout\": [0.1, 0.2],\n",
    "        # LoRA hyperparams could be included in grid if desired. We add them as command-line args.\n",
    "    }\n",
    "\n",
    "    best_auc = -1\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_aucs = []\n",
    "        print(f\"\\nGrid params: {params}\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(trainval_ds)))):\n",
    "            train_subset = Subset(trainval_ds, train_idx)\n",
    "            val_subset = Subset(trainval_ds, val_idx)\n",
    "            train_loader = DataLoader(\n",
    "                train_subset,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subset,\n",
    "                batch_size=args.eval_batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "            )\n",
    "            model, val_metrics = train_one_fold(\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                args,\n",
    "                lr=params[\"lr\"],\n",
    "                dropout=params[\"dropout\"],\n",
    "                lora_rank=args.lora_rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                lora_dropout=args.lora_dropout,\n",
    "            )\n",
    "            fold_aucs.append(val_metrics[\"auc\"])\n",
    "            print(f\"Fold {fold+1} AUC: {val_metrics['auc']:.4f}\")\n",
    "        mean_auc = np.mean(fold_aucs)\n",
    "        print(f\"Mean AUC for {params}: {mean_auc:.4f}\")\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_params = params\n",
    "            best_model = model  # last fold model (trained for this param setting). You may choose to re-train final model later.\n",
    "\n",
    "    print(\"\\nBest params:\", best_params)\n",
    "    print(\"Best CV AUC:\", best_auc)\n",
    "\n",
    "    # Evaluate on test set using best_model\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "    )\n",
    "    test_metrics = evaluate(best_model, test_loader, args.device)\n",
    "    print(\"\\nTest Set Results:\", test_metrics)\n",
    "\n",
    "    # Save model and tokenizer (best_model includes LoRA adapters)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    # Save entire model state_dict and tokenizer\n",
    "    torch.save(best_model.state_dict(), os.path.join(args.output_dir, \"best_model_with_lora.pt\"))\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(\"Saved best model (with LoRA adapters) and tokenizer to\", args.output_dir)\n",
    "\n",
    "# ------------------- Args -------------------\n",
    "\n",
    "class Args:\n",
    "    # file paths\n",
    "    data_csv = \"./training_pairs.csv\"\n",
    "    model_name_or_path = \"bert-base-uncased\"\n",
    "    output_dir = \"./crossenc_lora_out\"\n",
    "\n",
    "    # training config\n",
    "    epochs = 5\n",
    "    batch_size = 16\n",
    "    eval_batch_size = 64\n",
    "    max_length = 256\n",
    "    warmup_steps_ratio = 0.06\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "    # LoRA config\n",
    "    lora_rank = 32\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    args.device = torch.device(args.device)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39346e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device count: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
