{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a080b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data_with_Fine_tuned_BERT\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"HP_Chunks_BERT_Finetuned_collection\"\n",
    "# Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff78577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and embedding helper definitions (required before initializing embedding_fn)\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, itos, max_length=512):\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = stoi.get('[PAD]', 0)\n",
    "        self.cls_token_id = stoi.get('[CLS]', 1)\n",
    "        self.sep_token_id = stoi.get('[SEP]', 2)\n",
    "        self.unk_token_id = stoi.get('[UNK]', 3)\n",
    "    \n",
    "    def tokenize(self, text: str) -> list:\n",
    "        tokens = text.strip().split()\n",
    "        ids = [self.stoi.get(tok, self.unk_token_id) for tok in tokens]\n",
    "        ids = ids[:self.max_length - 2]\n",
    "        ids = [self.cls_token_id] + ids + [self.sep_token_id]\n",
    "        return ids\n",
    "    \n",
    "    def __call__(self, texts, padding=True, max_length=None):\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        all_ids = [self.tokenize(text) for text in texts]\n",
    "        if padding:\n",
    "            max_len = min(max(len(ids) for ids in all_ids), max_length)\n",
    "            padded_ids = []\n",
    "            attention_masks = []\n",
    "            for ids in all_ids:\n",
    "                ids = ids[:max_len]\n",
    "                pad_len = max_len - len(ids)\n",
    "                padded_ids.append(ids + [self.pad_token_id] * pad_len)\n",
    "                attention_masks.append([1] * len(ids) + [0] * pad_len)\n",
    "            return {'input_ids': torch.tensor(padded_ids, dtype=torch.long), 'attention_mask': torch.tensor(attention_masks, dtype=torch.long)}\n",
    "        else:\n",
    "            return {'input_ids': torch.tensor(all_ids, dtype=torch.long)}\n",
    "\n",
    "class MyBERTEmbeddingFunction:\n",
    "    def __init__(self, model, tokenizer, device, batch_size=16):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        # Ensure model is on the correct device and in eval mode to avoid device-mismatch errors\n",
    "        try:\n",
    "            self.model.to(self.device)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            self.model.eval()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    def _embed_texts(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i+self.batch_size]\n",
    "            encoded = self.tokenizer(batch_texts, padding=True, max_length=MAX_SEQ_LEN)\n",
    "            # Move inputs to the device where the model's parameters live to avoid device mismatch\n",
    "            param_device = next(self.model.parameters()).device\n",
    "            input_ids = encoded['input_ids'].to(param_device)\n",
    "            attention_mask = encoded.get('attention_mask', (input_ids!=0).long()).to(param_device)\n",
    "            with torch.no_grad():\n",
    "                emb = self.model.get_pooled_embeddings(input_ids, mask=attention_mask, exclude_special=True, normalize=True)\n",
    "            all_embeddings.extend(emb.cpu().numpy().tolist())\n",
    "        return all_embeddings\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        return self._embed_texts(input)\n",
    "    \n",
    "    def embed_query(self, input=None, **kwargs):\n",
    "        if input is None and 'input' in kwargs:\n",
    "            input = kwargs['input']\n",
    "        if input is None:\n",
    "            raise ValueError('No input provided to embed_query')\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        return self._embed_texts(input)\n",
    "\n",
    "# Initialize tokenizer and embedding function now that model is present\n",
    "tokenizer = SimpleTokenizer(stoi, itos, max_length=MAX_SEQ_LEN)\n",
    "embedding_fn = MyBERTEmbeddingFunction(model, tokenizer, DEVICE, batch_size=16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")# Set API keycollection_name = \"HP_Chunks_BERT_Embeddings_collection\"# Create a new collection with a unique namefile_path = \"../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl\"Absolute_Database_path = Path(Relative_Database_path).resolve()Relative_Database_path = \"./chroma_Data_with_Fine_tuned_BERT\"multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"load_dotenv()from dotenv import load_dotenvimport osimport pickleimport chromadbfrom pathlib import Path[SUCCESS] Tokenizer and embedding function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476086c3",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41c09fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: /home/tanish/ANLP_Proj/RAG_for_research_papers/VectorDB/chroma_Data_with_Fine_tuned_BERT\n",
      "Existing collections: ['HP_Chunks_BERT_Embeddings_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "933c14e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4014 chunks from '../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../harrypotter.pdf', 'page_number': 14, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98480f00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use custom pre-trained BERT model to generate embeddings. Location for BERT is ../Encoder/saved_bert_encoder_moe_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d895c04",
   "metadata": {},
   "source": [
    "#### Recreate BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60d98d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded vocab with 45706 tokens\n",
      "Loading BERT model...\n",
      "Loaded vocab with 45706 tokens\n",
      "Loading BERT model...\n",
      "[INFO] Loading checkpoint from ../Encoder/saved_bert_encoder_moe_pooling/bert_encoder_moe_pooling.pt\n",
      "[INFO] Loading checkpoint from ../Encoder/saved_bert_encoder_moe_pooling/bert_encoder_moe_pooling.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load vocab\n",
    "# Set MODEL_DIR to the directory that contains vocab.json and any .pt checkpoints\n",
    "# Prefer the saved model directory (not the .pt file path)\n",
    "MODEL_DIR = \"../Encoder/saved_bert_encoder_moe_pooling\"\n",
    "with open(f\"../Encoder/saved_bert_encoder_moe_pooling/vocab.json\", \"r\") as f:\n",
    "    vocab_data = json.load(f)\n",
    "    stoi = vocab_data[\"stoi\"]\n",
    "    itos = vocab_data[\"itos\"]\n",
    "    \n",
    "vocab_size = len(itos)\n",
    "print(f\"Loaded vocab with {vocab_size} tokens\")\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Model configuration (must match training config)\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "MAX_SEQ_LEN = 512  # Changed from 1024 to 512 to match saved model\n",
    "MAX_POSITION_EMBEDDINGS = 512  # This is what the saved model was trained with\n",
    "\n",
    "# -------------------------\n",
    "# Recreate Model Architecture\n",
    "# -------------------------\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, H = x.size()\n",
    "        logits = self.router(x)\n",
    "        probs_all = F.softmax(logits, dim=-1)\n",
    "        importance = probs_all.sum(dim=(0, 1))\n",
    "        total_tokens = float(B * S)\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "        \n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "        \n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)\n",
    "        \n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)\n",
    "        \n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)\n",
    "        flat_idx = topk_idx.view(-1, self.k)\n",
    "        flat_w = topk_weights.view(-1, self.k)\n",
    "        gating_flat = gating.view(-1, self.num_experts)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)\n",
    "        \n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)\n",
    "        return out, aux_loss\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, \n",
    "                 ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.hidden_size = hidden_size\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=DROPOUT, \n",
    "                                   moe_experts=moe_experts, moe_k=moe_k) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.nsp_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "    \n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux = layer(x, mask)\n",
    "            total_aux = total_aux + aux\n",
    "        return x, total_aux\n",
    "    \n",
    "    def get_pooled_embeddings(self, ids, mask=None, exclude_special=True, normalize=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings with mask-aware mean pooling\n",
    "        \"\"\"\n",
    "        seq_out, _ = self.encode(ids, tt=None, mask=mask)\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        \n",
    "        # Mask-aware mean pooling\n",
    "        mask_float = mask.unsqueeze(-1).to(seq_out.dtype)\n",
    "        \n",
    "        if exclude_special:\n",
    "            # Exclude special tokens from pooling\n",
    "            special_upper = len(SPECIAL_TOKENS)\n",
    "            special_flags = (ids < special_upper).to(seq_out.dtype)\n",
    "            mask_float = mask_float * (1.0 - special_flags.unsqueeze(-1))\n",
    "        \n",
    "        summed = (seq_out * mask_float).sum(dim=1)\n",
    "        denom = mask_float.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = summed / denom\n",
    "        \n",
    "        if normalize:\n",
    "            pooled = F.normalize(pooled, p=2, dim=1)\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "# Load model with matching max_position_embeddings\n",
    "print(\"Loading BERT model...\")\n",
    "model = BertEncoderModel(vocab_size, max_position_embeddings=MAX_POSITION_EMBEDDINGS, moe_experts=5, moe_k=2)\n",
    "# Robust checkpoint loader: handle different key naming (q/k/v vs in_proj, LoRA wrappers, o_proj/out_proj)\n",
    "import os, re\n",
    "from collections import OrderedDict, defaultdict\n",
    "# Resolve checkpoint path: support MODEL_DIR being either a file or a directory\n",
    "ckpt_path = None\n",
    "if os.path.isfile(MODEL_DIR):\n",
    "    ckpt_path = MODEL_DIR\n",
    "elif os.path.isdir(MODEL_DIR):\n",
    "    # look for common checkpoint filenames first\n",
    "    candidates = [os.path.join(MODEL_DIR, 'bert_encoder_moe_pooling.pt'), os.path.join(MODEL_DIR, 'bert_encoder.pt'), os.path.join(MODEL_DIR, 'lora_bert.pt')]\n",
    "    found = [p for p in candidates if os.path.exists(p)]\n",
    "    if found:\n",
    "        ckpt_path = found[0]\n",
    "    else:\n",
    "        # fallback: first .pt file in directory\n",
    "        pfiles = [os.path.join(MODEL_DIR, f) for f in os.listdir(MODEL_DIR) if f.endswith('.pt') or f.endswith('.pth')]\n",
    "        if pfiles:\n",
    "            ckpt_path = pfiles[0]\n",
    "else:\n",
    "    # treat MODEL_DIR as a path candidate (in case user passed a filename)\n",
    "    ckpt_path = MODEL_DIR if os.path.exists(MODEL_DIR) else None\n",
    "if ckpt_path is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint found under MODEL_DIR={MODEL_DIR}\")\n",
    "print(f\"[INFO] Loading checkpoint from {ckpt_path}\")\n",
    "raw = torch.load(ckpt_path, map_location='cpu')\n",
    "# extract state dict if wrapped\n",
    "if isinstance(raw, dict) and ('model_state_dict' in raw or 'state_dict' in raw):\n",
    "    sd = raw.get('model_state_dict', raw.get('state_dict'))\n",
    "else:\n",
    "    sd = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c6d6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding function...\n",
      "Generated 2 embeddings\n",
      "Embedding shape: 768 dimensions\n",
      "First embedding (first 5 values): [0.03645609691739082, -0.05256899073719978, -0.0013257176615297794, 0.0320611335337162, -0.00854380801320076]\n",
      "\n",
      "[SUCCESS] Embedding function ready for ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom BERT embedding function\n",
    "embedding_fn = MyBERTEmbeddingFunction(model, tokenizer, DEVICE, batch_size=16)\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"Testing embedding function...\")\n",
    "test_texts = [\"This is a test sentence.\", \"Another example text.\"]\n",
    "test_embeddings = embedding_fn(test_texts)\n",
    "print(f\"Generated {len(test_embeddings)} embeddings\")\n",
    "print(f\"Embedding shape: {len(test_embeddings[0])} dimensions\")\n",
    "print(f\"First embedding (first 5 values): {test_embeddings[0][:5]}\")\n",
    "print(\"\\n[SUCCESS] Embedding function ready for ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4adae5",
   "metadata": {},
   "source": [
    "### Create Collection with BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e08be9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No existing collection named 'HP_Chunks_BERT_Finetuned_collection' to delete.\n",
      "[SUCCESS] Fresh collection 'HP_Chunks_BERT_Finetuned_collection' created successfully\n",
      "Current count in collection: 0\n",
      "[SUCCESS] Fresh collection 'HP_Chunks_BERT_Finetuned_collection' created successfully\n",
      "Current count in collection: 0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# FORCE DELETE the collection if it exists\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(f\"[INFO] Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] No existing collection named '{collection_name}' to delete.\")\n",
    "\n",
    "# Create a FRESH collection with BERT embedding function\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter Chunks with custom BERT embeddings (MoE + Mask-aware pooling)\",\n",
    "        \"created\": str(datetime.now()),\n",
    "        \"model\": \"Custom BERT with MoE\",\n",
    "        \"embedding_dim\": HIDDEN_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Fresh collection '{collection_name}' created successfully\")\n",
    "print(f\"Current count in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c59db0",
   "metadata": {},
   "source": [
    "### Add Documents to Collection\n",
    "Prepare and add all chunks with BERT-generated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7ed847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prepared 4014 documents for embedding\n",
      "Sample document: . yes, that would be it. The traffic moved on and a few minutes\n",
      "later, Mr. Dursley arrived in the Gr...\n",
      "Sample metadata: {'source': '../harrypotter.pdf', 'page_number': 14, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for ChromaDB\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for idx, doc in enumerate(loaded_docs):\n",
    "    documents.append(doc.page_content)\n",
    "    metadatas.append(doc.metadata)\n",
    "    ids.append(f\"hp_chunk_{idx}\")\n",
    "\n",
    "print(f\"[INFO] Prepared {len(documents)} documents for embedding\")\n",
    "print(f\"Sample document: {documents[0][:100]}...\")\n",
    "print(f\"Sample metadata: {metadatas[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27215cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adding documents in 9 batches...\n",
      "  Batch 1/9 added (500 documents)\n",
      "  Batch 1/9 added (500 documents)\n",
      "  Batch 2/9 added (500 documents)\n",
      "  Batch 2/9 added (500 documents)\n",
      "  Batch 3/9 added (500 documents)\n",
      "  Batch 3/9 added (500 documents)\n",
      "  Batch 4/9 added (500 documents)\n",
      "  Batch 4/9 added (500 documents)\n",
      "  Batch 5/9 added (500 documents)\n",
      "  Batch 5/9 added (500 documents)\n",
      "  Batch 6/9 added (500 documents)\n",
      "  Batch 6/9 added (500 documents)\n",
      "  Batch 7/9 added (500 documents)\n",
      "  Batch 7/9 added (500 documents)\n",
      "  Batch 8/9 added (500 documents)\n",
      "  Batch 8/9 added (500 documents)\n",
      "  Batch 9/9 added (14 documents)\n",
      "\n",
      "[SUCCESS] All documents added!\n",
      "Total documents in collection: 4014\n",
      "  Batch 9/9 added (14 documents)\n",
      "\n",
      "[SUCCESS] All documents added!\n",
      "Total documents in collection: 4014\n"
     ]
    }
   ],
   "source": [
    "# Add documents to collection in batches\n",
    "# ChromaDB will automatically call our embedding_fn to generate embeddings\n",
    "batch_size = 500\n",
    "total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"[INFO] Adding documents in {total_batches} batches...\")\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_metas = metadatas[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        metadatas=batch_metas,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "    \n",
    "    batch_num = (i // batch_size) + 1\n",
    "    print(f\"  Batch {batch_num}/{total_batches} added ({len(batch_docs)} documents)\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] All documents added!\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec5292",
   "metadata": {},
   "source": [
    "### Test the Collection\n",
    "Query the collection to verify embeddings are working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9947795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'Who is Harry Potter?'\n",
      "\n",
      "Searching with BERT embeddings...\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "1. Distance: 0.2702\n",
      "   Text: Raising an arm in cheery farewell, he headed out of the front doors into the\n",
      "darkness. Harry and Ron looked at each other....\n",
      "\n",
      "2. Distance: 0.2729\n",
      "   Text: “Yeh shouldn’ve come!” Hagrid whispered. He stood back, then shut the\n",
      "door quickly. “This is the weirdest thing we’ve ever done,” Harry said fervently...\n",
      "\n",
      "3. Distance: 0.2745\n",
      "   Text: There you go, Sirius, Harry thought dully. Nothing rash. Kept my nose\n",
      "clean. Exactly the opposite of what you’d have done . ....\n",
      "\n",
      "4. Distance: 0.2758\n",
      "   Text: “The new password’s ‘Fortuna Major’!”\n",
      "“Oh no,” said Neville Longbottom sadly. He always had trouble\n",
      "remembering the passwords....\n",
      "\n",
      "5. Distance: 0.2825\n",
      "   Text: “Did you see his face, the great lump?”\n",
      "The other Slytherins joined in. “Shut up, Malfoy,” snapped Parvati Patil. ...\n",
      "\n",
      "[SUCCESS] Collection is working correctly with BERT embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"Who is Harry Potter?\"\n",
    "\n",
    "print(f\"Test Query: '{test_query}'\")\n",
    "print(\"\\nSearching with BERT embeddings...\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[test_query],\n",
    "    n_results=5    \n",
    ")\n",
    "\n",
    "print(f\"\\nTop 5 Results:\")\n",
    "for idx, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\n{idx+1}. Distance: {distance:.4f}\")\n",
    "    print(f\"   Text: {doc[:150]}...\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Collection is working correctly with BERT embeddings!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e928e",
   "metadata": {},
   "source": [
    "### Add queries to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19901 questions from ../LLM Caller/generated_pairs_without_commas.csv\n",
      "Added question batch: 0 to 499 (500 questions)\n",
      "Added question batch: 0 to 499 (500 questions)\n",
      "Added question batch: 500 to 999 (500 questions)\n",
      "Added question batch: 500 to 999 (500 questions)\n",
      "Added question batch: 1000 to 1499 (500 questions)\n",
      "Added question batch: 1000 to 1499 (500 questions)\n",
      "Added question batch: 1500 to 1999 (500 questions)\n",
      "Added question batch: 1500 to 1999 (500 questions)\n",
      "Added question batch: 2000 to 2499 (500 questions)\n",
      "Added question batch: 2000 to 2499 (500 questions)\n",
      "Added question batch: 2500 to 2999 (500 questions)\n",
      "Added question batch: 2500 to 2999 (500 questions)\n",
      "Added question batch: 3000 to 3499 (500 questions)\n",
      "Added question batch: 3000 to 3499 (500 questions)\n",
      "Added question batch: 3500 to 3999 (500 questions)\n",
      "Added question batch: 3500 to 3999 (500 questions)\n",
      "Added question batch: 4000 to 4499 (500 questions)\n",
      "Added question batch: 4000 to 4499 (500 questions)\n",
      "Added question batch: 4500 to 4999 (500 questions)\n",
      "Added question batch: 4500 to 4999 (500 questions)\n",
      "Added question batch: 5000 to 5499 (500 questions)\n",
      "Added question batch: 5000 to 5499 (500 questions)\n",
      "Added question batch: 5500 to 5999 (500 questions)\n",
      "Added question batch: 5500 to 5999 (500 questions)\n",
      "Added question batch: 6000 to 6499 (500 questions)\n",
      "Added question batch: 6000 to 6499 (500 questions)\n",
      "Added question batch: 6500 to 6999 (500 questions)\n",
      "Added question batch: 6500 to 6999 (500 questions)\n",
      "Added question batch: 7000 to 7499 (500 questions)\n",
      "Added question batch: 7000 to 7499 (500 questions)\n",
      "Added question batch: 7500 to 7999 (500 questions)\n",
      "Added question batch: 7500 to 7999 (500 questions)\n",
      "Added question batch: 8000 to 8499 (500 questions)\n",
      "Added question batch: 8000 to 8499 (500 questions)\n",
      "Added question batch: 8500 to 8999 (500 questions)\n",
      "Added question batch: 8500 to 8999 (500 questions)\n",
      "Added question batch: 9000 to 9499 (500 questions)\n",
      "Added question batch: 9000 to 9499 (500 questions)\n",
      "Added question batch: 9500 to 9999 (500 questions)\n",
      "Added question batch: 9500 to 9999 (500 questions)\n",
      "Added question batch: 10000 to 10499 (500 questions)\n",
      "Added question batch: 10000 to 10499 (500 questions)\n",
      "Added question batch: 10500 to 10999 (500 questions)\n",
      "Added question batch: 10500 to 10999 (500 questions)\n",
      "Added question batch: 11000 to 11499 (500 questions)\n",
      "Added question batch: 11000 to 11499 (500 questions)\n",
      "Added question batch: 11500 to 11999 (500 questions)\n",
      "Added question batch: 11500 to 11999 (500 questions)\n",
      "Added question batch: 12000 to 12499 (500 questions)\n",
      "Added question batch: 12000 to 12499 (500 questions)\n",
      "Added question batch: 12500 to 12999 (500 questions)\n",
      "Added question batch: 12500 to 12999 (500 questions)\n",
      "Added question batch: 13000 to 13499 (500 questions)\n",
      "Added question batch: 13000 to 13499 (500 questions)\n",
      "Added question batch: 13500 to 13999 (500 questions)\n",
      "Added question batch: 13500 to 13999 (500 questions)\n",
      "Added question batch: 14000 to 14499 (500 questions)\n",
      "Added question batch: 14000 to 14499 (500 questions)\n",
      "Added question batch: 14500 to 14999 (500 questions)\n",
      "Added question batch: 14500 to 14999 (500 questions)\n",
      "Added question batch: 15000 to 15499 (500 questions)\n",
      "Added question batch: 15000 to 15499 (500 questions)\n",
      "Added question batch: 15500 to 15999 (500 questions)\n",
      "Added question batch: 15500 to 15999 (500 questions)\n",
      "Added question batch: 16000 to 16499 (500 questions)\n",
      "Added question batch: 16000 to 16499 (500 questions)\n",
      "Added question batch: 16500 to 16999 (500 questions)\n",
      "Added question batch: 16500 to 16999 (500 questions)\n",
      "Added question batch: 17000 to 17499 (500 questions)\n",
      "Added question batch: 17000 to 17499 (500 questions)\n",
      "Added question batch: 17500 to 17999 (500 questions)\n",
      "Added question batch: 17500 to 17999 (500 questions)\n",
      "Added question batch: 18000 to 18499 (500 questions)\n",
      "Added question batch: 18000 to 18499 (500 questions)\n",
      "Added question batch: 18500 to 18999 (500 questions)\n",
      "Added question batch: 18500 to 18999 (500 questions)\n",
      "Added question batch: 19000 to 19499 (500 questions)\n",
      "Added question batch: 19000 to 19499 (500 questions)\n",
      "Added question batch: 19500 to 19900 (401 questions)\n",
      "\n",
      "Successfully added 19901 questions to collection 'HP_Chunks_BERT_Finetuned_collection'\n",
      "Total documents in collection now: 23915\n",
      "Added question batch: 19500 to 19900 (401 questions)\n",
      "\n",
      "Successfully added 19901 questions to collection 'HP_Chunks_BERT_Finetuned_collection'\n",
      "Total documents in collection now: 23915\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the generated questions from CSV\n",
    "questions_csv_path = \"../LLM Caller/generated_pairs_without_commas.csv\"  # Adjust path as needed\n",
    "df_questions = pd.read_csv(questions_csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df_questions)} questions from {questions_csv_path}\")\n",
    "\n",
    "# Prepare questions for ChromaDB\n",
    "question_ids = []\n",
    "question_documents = []\n",
    "question_metadatas = []\n",
    "\n",
    "# Process each question\n",
    "for idx, row in df_questions.iterrows():\n",
    "    \n",
    "    # Get the question text and chunk id\n",
    "    question_text, chunk_id = row['query'], row['chunk_id']\n",
    "    question_id = f\"query_{idx}_{chunk_id}\"\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"id\": question_id,\n",
    "        \"ischunk\": False,\n",
    "        \"chunk_id\": chunk_id\n",
    "    }\n",
    "    \n",
    "    question_ids.append(question_id)\n",
    "    question_documents.append(question_text)\n",
    "    question_metadatas.append(metadata)\n",
    "\n",
    "# Add questions to collection in batches\n",
    "batch_size = 500\n",
    "total_questions_added = 0\n",
    "\n",
    "for i in range(0, len(question_ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(question_ids))\n",
    "    \n",
    "    \n",
    "    collection.add(\n",
    "        ids=question_ids[i:end_idx],\n",
    "        documents=question_documents[i:end_idx],\n",
    "        metadatas=question_metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_questions_added += end_idx - i\n",
    "    print(f\"Added question batch: {i} to {end_idx-1} ({end_idx-i} questions)\")\n",
    "\n",
    "print(f\"\\nSuccessfully added {total_questions_added} questions to collection '{collection_name}'\")\n",
    "print(f\"Total documents in collection now: {collection.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
