{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dd5d21",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c14e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff78577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data_with_BERT_embeddings\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"HP_Chunks_BERT_Embeddings_collection\"\n",
    "# Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476086c3",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c09fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: /home/tanish/ANLP_Proj/RAG_for_research_papers/VectorDB/chroma_Data_with_BERT_embeddings\n",
      "Existing collections: ['HP_Chunks_BERT_Embeddings_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933c14e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4014 chunks from '../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../harrypotter.pdf', 'page_number': 14, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98480f00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use custom pre-trained BERT model to generate embeddings. Location for BERT is ../Encoder/saved_bert_encoder_moe_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d895c04",
   "metadata": {},
   "source": [
    "#### Recreate BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d98d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded vocab with 45706 tokens\n",
      "Loading BERT model...\n",
      "Model loaded successfully!\n",
      "Tokenizer initialized!\n",
      "\n",
      "[SUCCESS] BERT model and tokenizer ready!\n",
      "Embedding dimension: 768\n",
      "Max sequence length: 512\n",
      "Model loaded successfully!\n",
      "Tokenizer initialized!\n",
      "\n",
      "[SUCCESS] BERT model and tokenizer ready!\n",
      "Embedding dimension: 768\n",
      "Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load vocab\n",
    "MODEL_DIR = \"../Encoder/saved_bert_encoder_moe_pooling\"\n",
    "with open(f\"{MODEL_DIR}/vocab.json\", \"r\") as f:\n",
    "    vocab_data = json.load(f)\n",
    "    stoi = vocab_data[\"stoi\"]\n",
    "    itos = vocab_data[\"itos\"]\n",
    "    \n",
    "vocab_size = len(itos)\n",
    "print(f\"Loaded vocab with {vocab_size} tokens\")\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Model configuration (must match training config)\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "MAX_SEQ_LEN = 512  # Changed from 1024 to 512 to match saved model\n",
    "MAX_POSITION_EMBEDDINGS = 512  # This is what the saved model was trained with\n",
    "\n",
    "# -------------------------\n",
    "# Recreate Model Architecture\n",
    "# -------------------------\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, H = x.size()\n",
    "        logits = self.router(x)\n",
    "        probs_all = F.softmax(logits, dim=-1)\n",
    "        importance = probs_all.sum(dim=(0, 1))\n",
    "        total_tokens = float(B * S)\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "        \n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "        \n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)\n",
    "        \n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)\n",
    "        \n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)\n",
    "        flat_idx = topk_idx.view(-1, self.k)\n",
    "        flat_w = topk_weights.view(-1, self.k)\n",
    "        gating_flat = gating.view(-1, self.num_experts)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)\n",
    "        \n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)\n",
    "        return out, aux_loss\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, \n",
    "                 ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.hidden_size = hidden_size\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=DROPOUT, \n",
    "                                   moe_experts=moe_experts, moe_k=moe_k) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.nsp_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "    \n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux = layer(x, mask)\n",
    "            total_aux = total_aux + aux\n",
    "        return x, total_aux\n",
    "    \n",
    "    def get_pooled_embeddings(self, ids, mask=None, exclude_special=True, normalize=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings with mask-aware mean pooling\n",
    "        \"\"\"\n",
    "        seq_out, _ = self.encode(ids, tt=None, mask=mask)\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        \n",
    "        # Mask-aware mean pooling\n",
    "        mask_float = mask.unsqueeze(-1).to(seq_out.dtype)\n",
    "        \n",
    "        if exclude_special:\n",
    "            # Exclude special tokens from pooling\n",
    "            special_upper = len(SPECIAL_TOKENS)\n",
    "            special_flags = (ids < special_upper).to(seq_out.dtype)\n",
    "            mask_float = mask_float * (1.0 - special_flags.unsqueeze(-1))\n",
    "        \n",
    "        summed = (seq_out * mask_float).sum(dim=1)\n",
    "        denom = mask_float.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = summed / denom\n",
    "        \n",
    "        if normalize:\n",
    "            pooled = F.normalize(pooled, p=2, dim=1)\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "# Load model with matching max_position_embeddings\n",
    "print(\"Loading BERT model...\")\n",
    "model = BertEncoderModel(vocab_size, max_position_embeddings=MAX_POSITION_EMBEDDINGS, moe_experts=5, moe_k=2)\n",
    "model.load_state_dict(torch.load(f\"{MODEL_DIR}/bert_encoder_moe_pooling.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Simple tokenizer class\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, itos, max_length=512):\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = stoi[PAD_TOKEN]\n",
    "        self.cls_token_id = stoi[CLS_TOKEN]\n",
    "        self.sep_token_id = stoi[SEP_TOKEN]\n",
    "        self.unk_token_id = stoi[UNK_TOKEN]\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        \"\"\"Tokenize text to IDs\"\"\"\n",
    "        tokens = text.strip().split()\n",
    "        ids = [self.stoi.get(tok, self.unk_token_id) for tok in tokens]\n",
    "        # Truncate if needed (reserve space for CLS and SEP)\n",
    "        ids = ids[:self.max_length - 2]\n",
    "        # Add CLS and SEP\n",
    "        ids = [self.cls_token_id] + ids + [self.sep_token_id]\n",
    "        return ids\n",
    "    \n",
    "    def __call__(self, texts: List[str], padding=True, max_length=None):\n",
    "        \"\"\"Batch tokenization\"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        \n",
    "        all_ids = [self.tokenize(text) for text in texts]\n",
    "        \n",
    "        if padding:\n",
    "            max_len = min(max(len(ids) for ids in all_ids), max_length)\n",
    "            padded_ids = []\n",
    "            attention_masks = []\n",
    "            \n",
    "            for ids in all_ids:\n",
    "                # Truncate if needed\n",
    "                ids = ids[:max_len]\n",
    "                # Pad\n",
    "                pad_len = max_len - len(ids)\n",
    "                padded_ids.append(ids + [self.pad_token_id] * pad_len)\n",
    "                attention_masks.append([1] * len(ids) + [0] * pad_len)\n",
    "            \n",
    "            return {\n",
    "                'input_ids': torch.tensor(padded_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_masks, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(all_ids, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "tokenizer = SimpleTokenizer(stoi, itos, max_length=MAX_SEQ_LEN)\n",
    "print(\"Tokenizer initialized!\")\n",
    "\n",
    "# Custom embedding function for ChromaDB\n",
    "class MyBERTEmbeddingFunction:\n",
    "    def __init__(self, model, tokenizer, device, batch_size=16):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def _embed_texts(self, texts: Union[str, List[str]]) -> List[List[float]]:\n",
    "        \"\"\"Internal method to generate embeddings for text(s)\"\"\"\n",
    "        # Handle single string input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = self.tokenizer(batch_texts, padding=True, max_length=MAX_SEQ_LEN)\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model.get_pooled_embeddings(\n",
    "                    input_ids, \n",
    "                    mask=attention_mask, \n",
    "                    exclude_special=True, \n",
    "                    normalize=True\n",
    "                )\n",
    "            \n",
    "            # Convert to list\n",
    "            embeddings_list = embeddings.cpu().numpy().tolist()\n",
    "            all_embeddings.extend(embeddings_list)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts (used for documents)\n",
    "        \"\"\"\n",
    "        return self._embed_texts(input)\n",
    "    \n",
    "    def embed_query(self, input = None, **kwargs) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embedding for query text(s) (required by ChromaDB)\n",
    "        Returns List[List[float]] to match ChromaDB's expected format\n",
    "        \"\"\"\n",
    "        # Handle keyword argument 'input' \n",
    "        if input is None and 'input' in kwargs:\n",
    "            input = kwargs['input']\n",
    "        \n",
    "        if input is None:\n",
    "            raise ValueError(\"No input provided to embed_query\")\n",
    "        \n",
    "        # Handle both string and list inputs\n",
    "        # ChromaDB sometimes passes a list even for single queries\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        \n",
    "        # Return embeddings as List[List[float]]\n",
    "        embeddings = self._embed_texts(input)\n",
    "        return embeddings\n",
    "\n",
    "print(\"\\n[SUCCESS] BERT model and tokenizer ready!\")\n",
    "print(f\"Embedding dimension: {HIDDEN_SIZE}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LEN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e114b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding function...\n",
      "Generated 2 embeddings\n",
      "Embedding shape: 768 dimensions\n",
      "First embedding (first 5 values): [0.03787987679243088, -0.01927832141518593, 0.007840707898139954, 0.030228307470679283, 0.009925552643835545]\n",
      "\n",
      "[SUCCESS] Embedding function ready for ChromaDB!\n",
      "Generated 2 embeddings\n",
      "Embedding shape: 768 dimensions\n",
      "First embedding (first 5 values): [0.03787987679243088, -0.01927832141518593, 0.007840707898139954, 0.030228307470679283, 0.009925552643835545]\n",
      "\n",
      "[SUCCESS] Embedding function ready for ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom BERT embedding function\n",
    "embedding_fn = MyBERTEmbeddingFunction(model, tokenizer, DEVICE, batch_size=16)\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"Testing embedding function...\")\n",
    "test_texts = [\"This is a test sentence.\", \"Another example text.\"]\n",
    "test_embeddings = embedding_fn(test_texts)\n",
    "print(f\"Generated {len(test_embeddings)} embeddings\")\n",
    "print(f\"Embedding shape: {len(test_embeddings[0])} dimensions\")\n",
    "print(f\"First embedding (first 5 values): {test_embeddings[0][:5]}\")\n",
    "print(\"\\n[SUCCESS] Embedding function ready for ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2db337",
   "metadata": {},
   "source": [
    "### Create Collection with BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0ef987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted existing collection 'HP_Chunks_BERT_Embeddings_collection'\n",
      "[SUCCESS] Fresh collection 'HP_Chunks_BERT_Embeddings_collection' created successfully\n",
      "Current count in collection: 0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# FORCE DELETE the collection if it exists\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(f\"[INFO] Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] No existing collection named '{collection_name}' to delete.\")\n",
    "\n",
    "# Create a FRESH collection with BERT embedding function\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter Chunks with custom BERT embeddings (MoE + Mask-aware pooling)\",\n",
    "        \"created\": str(datetime.now()),\n",
    "        \"model\": \"Custom BERT with MoE\",\n",
    "        \"embedding_dim\": HIDDEN_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Fresh collection '{collection_name}' created successfully\")\n",
    "print(f\"Current count in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df2eb7",
   "metadata": {},
   "source": [
    "### Add Documents to Collection\n",
    "Prepare and add all chunks with BERT-generated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4018bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prepared 4014 documents for embedding\n",
      "Sample document: . yes, that would be it. The traffic moved on and a few minutes\n",
      "later, Mr. Dursley arrived in the Gr...\n",
      "Sample metadata: {'source': '../harrypotter.pdf', 'page_number': 14, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for ChromaDB\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for idx, doc in enumerate(loaded_docs):\n",
    "    documents.append(doc.page_content)\n",
    "    metadatas.append(doc.metadata)\n",
    "    ids.append(f\"hp_chunk_{idx}\")\n",
    "\n",
    "print(f\"[INFO] Prepared {len(documents)} documents for embedding\")\n",
    "print(f\"Sample document: {documents[0][:100]}...\")\n",
    "print(f\"Sample metadata: {metadatas[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a872d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adding documents in 9 batches...\n",
      "  Batch 1/9 added (500 documents)\n",
      "  Batch 1/9 added (500 documents)\n",
      "  Batch 2/9 added (500 documents)\n",
      "  Batch 2/9 added (500 documents)\n",
      "  Batch 3/9 added (500 documents)\n",
      "  Batch 3/9 added (500 documents)\n",
      "  Batch 4/9 added (500 documents)\n",
      "  Batch 4/9 added (500 documents)\n",
      "  Batch 5/9 added (500 documents)\n",
      "  Batch 5/9 added (500 documents)\n",
      "  Batch 6/9 added (500 documents)\n",
      "  Batch 6/9 added (500 documents)\n",
      "  Batch 7/9 added (500 documents)\n",
      "  Batch 7/9 added (500 documents)\n",
      "  Batch 8/9 added (500 documents)\n",
      "  Batch 8/9 added (500 documents)\n",
      "  Batch 9/9 added (14 documents)\n",
      "\n",
      "[SUCCESS] All documents added!\n",
      "Total documents in collection: 4014\n",
      "  Batch 9/9 added (14 documents)\n",
      "\n",
      "[SUCCESS] All documents added!\n",
      "Total documents in collection: 4014\n"
     ]
    }
   ],
   "source": [
    "# Add documents to collection in batches\n",
    "# ChromaDB will automatically call our embedding_fn to generate embeddings\n",
    "batch_size = 500\n",
    "total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"[INFO] Adding documents in {total_batches} batches...\")\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_metas = metadatas[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        metadatas=batch_metas,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "    \n",
    "    batch_num = (i // batch_size) + 1\n",
    "    print(f\"  Batch {batch_num}/{total_batches} added ({len(batch_docs)} documents)\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] All documents added!\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232c99a",
   "metadata": {},
   "source": [
    "### Test the Collection\n",
    "Query the collection to verify embeddings are working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303501a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'Who is Harry Potter?'\n",
      "\n",
      "Searching with BERT embeddings...\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "1. Distance: 1.5522\n",
      "   Text: “Oh come on,” he said impatiently, “we need partners, we’re going to look\n",
      "really stupid if we haven’t got any, everyone else has . . .”\n",
      "“I can’t come ...\n",
      "\n",
      "2. Distance: 1.5566\n",
      "   Text: . see Dumbledore . . . my fault . . . all\n",
      "my fault . . . Bertha . . . dead . . . all my fault . . . my son . . . my fault . . . tell\n",
      "Dumbledore . . . ...\n",
      "\n",
      "3. Distance: 1.5804\n",
      "   Text: “Fine,” said Harry stiffly. “Oh, don’t lie, Harry,” she said impatiently. “Ron and Ginny say you’ve\n",
      "been hiding from everyone since you got back from ...\n",
      "\n",
      "4. Distance: 1.5882\n",
      "   Text: “Why can’t they help?”\n",
      "“What?”\n",
      "“They can help.” He dropped his voice and said, so that none of them could\n",
      "hear but Hermione, who stood between them, “...\n",
      "\n",
      "5. Distance: 1.5886\n",
      "   Text: . I don’t believe it . . . he crept up behind\n",
      "me. . . . I heard him, I turned around, and he had his wand on me. . . .”\n",
      "Cedric got up. He was still sh...\n",
      "\n",
      "[SUCCESS] Collection is working correctly with BERT embeddings!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"Who is Harry Potter?\"\n",
    "\n",
    "print(f\"Test Query: '{test_query}'\")\n",
    "print(\"\\nSearching with BERT embeddings...\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[test_query],\n",
    "    n_results=5    \n",
    ")\n",
    "\n",
    "print(f\"\\nTop 5 Results:\")\n",
    "for idx, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\n{idx+1}. Distance: {distance:.4f}\")\n",
    "    print(f\"   Text: {doc[:150]}...\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Collection is working correctly with BERT embeddings!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
