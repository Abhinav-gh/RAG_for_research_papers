{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] Starting Bayesian Optimization over hyperparameters with 5-fold CV per BO eval...\n",
      "[DATA SPLIT] total=4, train=2, val=1, test=1\n",
      "[BO] Using combined train+val pool of size 3 for 5-fold CV in BO.\n",
      "[BO] Starting initial random evaluations (random seed sampling)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BO Initial Samples:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BO] Init sample 1/3: {'learning_rate': 4.523556587070803e-06, 'moe_experts': 6, 'moe_k': 2, 'ffn_dim': 3584, 'num_layers': 12, 'num_heads': 12, 'word2vec_window': 3, 'mlm_mask_prob': 0.18062383981151098, 'batch_size': 4, 'word2vec_size': 768, 'word2vec_min_count': 1}\n",
      "[Run 1] Warning: Reducing k_folds from 5 to 3 due to small dataset size (3 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Run 1] K-Fold CV:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "BO Initial Samples:   0%|          | 0/3 [00:05<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 913\u001b[39m\n\u001b[32m    909\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(results[\u001b[33m\"\u001b[39m\u001b[33mtest_metrics\u001b[39m\u001b[33m\"\u001b[39m], indent=\u001b[32m2\u001b[39m))\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 905\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    903\u001b[39m corpus = get_default_corpus()\n\u001b[32m    904\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[MAIN] Starting Bayesian Optimization over hyperparameters with 5-fold CV per BO eval...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m results = \u001b[43mrun_bayesian_optimization_with_heldout_test_cv5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[MAIN] BO Completed. Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    907\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(results[\u001b[33m\"\u001b[39m\u001b[33mbest_record\u001b[39m\u001b[33m\"\u001b[39m], indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 757\u001b[39m, in \u001b[36mrun_bayesian_optimization_with_heldout_test_cv5\u001b[39m\u001b[34m(corpus)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;66;03m# ensure mandatory hyperparameters present (sample function sets them)\u001b[39;00m\n\u001b[32m    756\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[BO] Init sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBO_INIT_POINTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m val_loss, fold_details = \u001b[43mobjective_function_with_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_FOLDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m vec = hyperparams_to_vector(hp)\n\u001b[32m    759\u001b[39m X.append(vec)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 675\u001b[39m, in \u001b[36mobjective_function_with_kfold\u001b[39m\u001b[34m(hp, vocab_size, emb, pad_id, mask_id, combined_indices, ds, k_folds, run_id, verbose)\u001b[39m\n\u001b[32m    673\u001b[39m epoch_pbar = tqdm(\u001b[38;5;28mrange\u001b[39m(TRIAL_EPOCHS), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_k_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Epochs\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epoch_pbar:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     _train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlm_loss_fct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsp_loss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_coeff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m     epoch_pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m})\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 383\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, mlm_loss_fct, nsp_loss_fct, pad_id, mask_id, vocab_size, device, aux_coeff)\u001b[39m\n\u001b[32m    381\u001b[39m nsp_labels = batch[\u001b[33m\"\u001b[39m\u001b[33mnsp_labels\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m    382\u001b[39m btypes = batch[\u001b[33m\"\u001b[39m\u001b[33mbatch_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m ids_masked, mlm_labels = \u001b[43mcreate_mlm_labels_and_masked_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m optimizer.zero_grad()\n\u001b[32m    386\u001b[39m mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mcreate_mlm_labels_and_masked_input\u001b[39m\u001b[34m(input_ids, pad_id, mask_token_id, vocab_size, device, mask_prob)\u001b[39m\n\u001b[32m    222\u001b[39m input_ids_masked = input_ids.clone()\n\u001b[32m    223\u001b[39m rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m mask_replace = \u001b[43mmasked_positions\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_for_replace\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m random_replace = masked_positions & (rand_for_replace >= \u001b[32m0.8\u001b[39m) & (rand_for_replace < \u001b[32m0.9\u001b[39m)\n\u001b[32m    226\u001b[39m input_ids_masked[mask_replace] = mask_token_id\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# bert_encoder_from_scratch_with_pooling_multitype_allpairs_bo_cv5.py\n",
    "# Full script:\n",
    "# - MLM + NSP (unchanged logic)\n",
    "# - MoE Top-K routing (unchanged)\n",
    "# - Bayesian Optimization (Gaussian Process + EI), 3 epochs/fold, 5-fold CV per BO eval, 10 iterations\n",
    "# - Train/val/test split done once, test never used for BO\n",
    "# - Final retrain on train+val with best hp, final evaluation on test set\n",
    "# - Logs saved to bo_logs/\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import chromadb\n",
    "\n",
    "# for Gaussian Process BO\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# -------------------------\n",
    "# Config (DEFAULTS)\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 1024\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# BO Settings\n",
    "# -------------------------\n",
    "BO_ITERATIONS = 10\n",
    "BO_INIT_POINTS = 3   # random initial points\n",
    "TRIAL_EPOCHS = 1    # 3 epochs per fold as requested\n",
    "K_FOLDS = 5          # strong CV chosen by you\n",
    "BO_LOG_DIR = \"bo_logs\"\n",
    "\n",
    "os.makedirs(BO_LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BO_LOG_DIR, \"best_model\"), exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utility: Vocab builder\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    stoi, itos = {}, []\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (supports queries and chunks)\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], stoi: dict, max_seq_len=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: list of tuples [(text, discriminator)], where discriminator ∈ {'Q', 'C'}\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        for text, dtype in data:\n",
    "            if dtype == \"Q\":\n",
    "                # Single-sentence query (MLM only)\n",
    "                self.samples.append((text, dtype, None, None))\n",
    "            elif dtype == \"C\":\n",
    "                # Split chunk into sentences\n",
    "                sents = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "                if len(sents) < 2:\n",
    "                    sents = sents + sents  # duplicate if only one sentence\n",
    "                \n",
    "                # Debug: Print sentence count for first few chunks\n",
    "                if len(self.samples) < 10:\n",
    "                    print(f\"[DEBUG] Chunk has {len(sents)} sentences, will generate ~{len(sents)-1 + len(sents)*(len(sents)-1)//2} pairs\")\n",
    "                \n",
    "                # Positive pairs: consecutive sentences\n",
    "                for i in range(len(sents) - 1):\n",
    "                    self.samples.append((sents[i], \"C\", sents[i + 1], 1))\n",
    "                # Negative pairs: non-consecutive\n",
    "                for i in range(len(sents)):\n",
    "                    for j in range(len(sents)):\n",
    "                        if abs(i - j) > 1:  # skip consecutive\n",
    "                            self.samples.append((sents[i], \"C\", sents[j], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        return [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_a, dtype, sent_b, nsp_label = self.samples[idx]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 1: Query (MLM only)\n",
    "        # -------------------------------\n",
    "        if dtype == 'Q':\n",
    "            ids = self._tokenize_to_ids(sent_a)\n",
    "            ids = ids[:self.max_seq_len - 2]\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * len(input_ids)\n",
    "            nsp_label = -100  # dummy\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"Q\"\n",
    "            }\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 2: Chunk (MLM + NSP)\n",
    "        # -------------------------------\n",
    "        elif dtype == 'C':\n",
    "            ids_a = self._tokenize_to_ids(sent_a)\n",
    "            ids_b = self._tokenize_to_ids(sent_b)\n",
    "            while len(ids_a) + len(ids_b) > self.max_seq_len - 3:\n",
    "                if len(ids_a) > len(ids_b):\n",
    "                    ids_a.pop()\n",
    "                else:\n",
    "                    ids_b.pop()\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"C\"\n",
    "            }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "    batch_types = [b[\"batch_type\"] for b in batch]\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids, padded_token_types, attention_masks = [], [], []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"token_type_ids\": torch.stack(padded_token_types),\n",
    "        \"attention_mask\": torch.stack(attention_masks),\n",
    "        \"nsp_labels\": nsp_labels,\n",
    "        \"batch_type\": batch_types\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM Masking\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids, pad_id, mask_token_id, vocab_size, device=None, mask_prob=MLM_MASK_PROB):\n",
    "    if device is None:\n",
    "        device = input_ids.device\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, -100)\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob, device=device)\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "    \n",
    "    # Ensure at least some tokens can be masked\n",
    "    maskable_positions = (input_ids >= special_upper) & (input_ids != pad_id)\n",
    "    if maskable_positions.sum() == 0:\n",
    "        # No maskable tokens, return unmasked input\n",
    "        return input_ids.clone(), mlm_labels\n",
    "    \n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()\n",
    "    \n",
    "    # If no positions were masked, force at least one\n",
    "    if masked_positions.sum() == 0 and maskable_positions.sum() > 0:\n",
    "        # Randomly select one maskable position per sequence\n",
    "        for b in range(batch_size):\n",
    "            maskable_in_seq = maskable_positions[b].nonzero(as_tuple=True)[0]\n",
    "            if len(maskable_in_seq) > 0:\n",
    "                chosen_idx = maskable_in_seq[torch.randint(len(maskable_in_seq), (1,), device=device)]\n",
    "                masked_positions[b, chosen_idx] = True\n",
    "    \n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        rand_tokens = torch.randint(len(SPECIAL_TOKENS), vocab_size, (count,), device=device)\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Mixture-of-Experts Module (unchanged logic)\n",
    "# -------------------------\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # experts: each expert is a small Feed-Forward Network (H -> ffn_dim -> H)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # router: maps hidden vector to expert logits\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, S, H)\n",
    "        returns: out (B, S, H), aux_loss (scalar)\n",
    "        \"\"\"\n",
    "        B, S, H = x.size()\n",
    "        # ---- router logits (noiseless, for load-balancing) ----\n",
    "        logits = self.router(x)  # (B, S, E)\n",
    "        # soft probabilities for load balancing (use non-noisy softmax)\n",
    "        probs_all = F.softmax(logits, dim=-1)  # (B, S, E)\n",
    "        # importance per expert:\n",
    "        importance = probs_all.sum(dim=(0, 1))  # (E,)\n",
    "        total_tokens = float(B * S)\n",
    "        # aux_loss encourages balanced importance across experts\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "\n",
    "        # ---- noisy logits for selection (only add noise during training) ----\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "\n",
    "        # top-k selection on noisy logits\n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)  # shapes (B,S,k)\n",
    "        # convert topk vals to normalized weights via softmax over k\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)  # (B,S,k)\n",
    "\n",
    "        # Compute each expert's output on the full x (inefficient but simple)\n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))  # (B,S,H)\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)  # (B,S,E,H)\n",
    "\n",
    "        # Build a gating tensor of shape (B,S,E) with nonzero entries only at topk indices\n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)  # float\n",
    "        # scatter the topk_weights into gating at positions topk_idx\n",
    "        # topk_idx: (B,S,k), topk_weights: (B,S,k)\n",
    "        # We can flatten and scatter\n",
    "        flat_idx = topk_idx.view(-1, self.k)  # (B*S, k)\n",
    "        flat_w = topk_weights.view(-1, self.k)  # (B*S, k)\n",
    "        # For each row r in [0..B*S-1], scatter into gating_flat[r, idx] = weight\n",
    "        gating_flat = gating.view(-1, self.num_experts)  # (B*S, E)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)  # (B*S, k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)  # (B,S,E)\n",
    "\n",
    "        # Combine experts: out[b,s,:] = sum_e gating[b,s,e] * expert_stack[b,s,e,:]\n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)  # (B,S,H)\n",
    "\n",
    "        return out, aux_loss\n",
    "\n",
    "# -------------------------\n",
    "# Transformer encoder (unchanged logic)\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        # Replace ffn with MoE module\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        # MoE FFN\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=DROPOUT, moe_experts=moe_experts, moe_k=moe_k) for _ in range(num_layers)])\n",
    "        self.nsp_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 2))\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux = layer(x, mask)\n",
    "            total_aux = total_aux + aux\n",
    "        return x, total_aux\n",
    "    def forward(self, ids, tt=None, mask=None):\n",
    "        seq_out, total_aux = self.encode(ids, tt, mask)\n",
    "        pooled = seq_out[:, 0]\n",
    "        nsp_logits = self.nsp_classifier(pooled)\n",
    "        mlm_logits = F.linear(seq_out, self.token_embeddings.weight, self.mlm_bias)\n",
    "        return mlm_logits, nsp_logits, total_aux\n",
    "\n",
    "# -------------------------\n",
    "# Training / evaluation helpers (refactors of your loop)\n",
    "# -------------------------\n",
    "def train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                    mlm_loss_fct, nsp_loss_fct, pad_id: int, mask_id: int, vocab_size: int, device: torch.device,\n",
    "                    aux_coeff: float):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in pbar:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        tts = batch[\"token_type_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        nsp_labels = batch[\"nsp_labels\"].to(device)\n",
    "        btypes = batch[\"batch_type\"]\n",
    "        ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "        mlm_loss = mlm_loss_fct(mlm_logits.view(-1, mlm_logits.size(-1)), mlm_labels.view(-1))\n",
    "        if all(bt == \"C\" for bt in btypes):\n",
    "            nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "        else:\n",
    "            nsp_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "        \n",
    "        # Check for NaN loss before backprop\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"[WARNING] NaN/Inf loss detected, skipping batch\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_steps += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    if total_steps == 0:\n",
    "        return float('inf')  # Return inf instead of NaN for empty dataloader\n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, pad_id: int, mask_id: int, vocab_size: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Evaluates total loss (MLM + NSP + aux) averaged across dataloader.\n",
    "    Returns avg_loss (float).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    aux_coeff = 0.01\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in pbar:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            tts = batch[\"token_type_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(device)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size, device=device)\n",
    "\n",
    "            mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, mlm_logits.size(-1)), mlm_labels.view(-1))\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    if total_steps == 0:\n",
    "        return float('inf')  # Return inf instead of NaN for empty dataloader\n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss\n",
    "\n",
    "def compute_metrics(model: nn.Module, dataloader: DataLoader, pad_id: int, mask_id: int, vocab_size: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Compute MLM accuracy, NSP accuracy and average total loss on dataloader.\n",
    "    Returns: dict with keys: avg_loss, mlm_acc, nsp_acc\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    aux_coeff = 0.01\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    total_mlm_correct = 0\n",
    "    total_mlm_count = 0\n",
    "    total_nsp_correct = 0\n",
    "    total_nsp_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Computing Metrics\", leave=False)\n",
    "        for batch in pbar:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            tts = batch[\"token_type_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(device)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size, device=device)\n",
    "\n",
    "            mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, mlm_logits.size(-1)), mlm_labels.view(-1))\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "            # MLM accuracy\n",
    "            mlm_preds = mlm_logits.argmax(-1)\n",
    "            mask_positions = mlm_labels != -100\n",
    "            if mask_positions.sum().item() > 0:\n",
    "                total_mlm_correct += (mlm_preds[mask_positions] == mlm_labels[mask_positions]).sum().item()\n",
    "                total_mlm_count += mask_positions.sum().item()\n",
    "\n",
    "            # NSP accuracy\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_preds = nsp_logits.argmax(-1)\n",
    "                total_nsp_correct += (nsp_preds == nsp_labels).sum().item()\n",
    "                total_nsp_count += nsp_labels.numel()\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_steps)\n",
    "    mlm_acc = total_mlm_correct / max(1, total_mlm_count) if total_mlm_count > 0 else 0.0\n",
    "    nsp_acc = total_nsp_correct / max(1, total_nsp_count) if total_nsp_count > 0 else 0.0\n",
    "\n",
    "    return {\"avg_loss\": avg_loss, \"mlm_acc\": mlm_acc, \"nsp_acc\": nsp_acc}\n",
    "\n",
    "# -------------------------\n",
    "# Helper: build data, vocab, embeddings given hyperparams\n",
    "# -------------------------\n",
    "def prepare_data_and_model_artifacts(corpus: List[Tuple[str, str]], hyperparams: Dict[str, Any] = None):\n",
    "    \"\"\"\n",
    "    Build vocab, embeddings and dataset from corpus. This does NOT split train/val/test.\n",
    "    Returns: stoi, itos, vocab_size, emb (tensor), pad_id, mask_id, ds (dataset)\n",
    "    hyperparams may alter word2vec_window, WORD2VEC_SIZE, WORD2VEC_MIN_COUNT, MAX_SEQ_LEN, etc.\n",
    "    \"\"\"\n",
    "    hv = hyperparams or {}\n",
    "    word2vec_window = int(hv.get(\"word2vec_window\", WORD2VEC_WINDOW))\n",
    "    word2vec_size = int(hv.get(\"word2vec_size\", WORD2VEC_SIZE))\n",
    "    word2vec_min_count = int(hv.get(\"word2vec_min_count\", WORD2VEC_MIN_COUNT))\n",
    "    max_seq_len = int(hv.get(\"max_seq_len\", MAX_SEQ_LEN))\n",
    "\n",
    "    texts = [x[0] for x in corpus]\n",
    "    stoi, itos = build_vocab(texts, min_freq=VOCAB_MIN_FREQ)\n",
    "    vocab_size = len(itos)\n",
    "\n",
    "    w2v = train_word2vec(texts, vector_size=word2vec_size, window=word2vec_window, min_count=word2vec_min_count, epochs=5)\n",
    "    emb = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "    pad_id = stoi[PAD_TOKEN]; mask_id = stoi[MASK_TOKEN]\n",
    "    ds = BertPretrainingDataset(corpus, stoi, max_seq_len=max_seq_len)\n",
    "    print(f\"[DEBUG] Corpus size: {len(corpus)} documents\")\n",
    "    print(f\"[DEBUG] Dataset size (after pair generation): {len(ds)} samples\")\n",
    "    print(f\"[DEBUG] Vocab size: {vocab_size}\")\n",
    "    return stoi, itos, vocab_size, emb, pad_id, mask_id, ds\n",
    "\n",
    "# -------------------------\n",
    "# BO helpers: search space sampling, GP + EI\n",
    "# -------------------------\n",
    "def sample_random_hyperparams():\n",
    "    \"\"\"\n",
    "    Sample hyperparameters in the agreed search space.\n",
    "    \"\"\"\n",
    "    sample = {}\n",
    "    # learning_rate ∈ [1e-6, 1e-5] (log-uniform) - reduced upper bound to prevent instability\n",
    "    lr = 10 ** np.random.uniform(np.log10(1e-6), np.log10(1e-5))\n",
    "    sample[\"learning_rate\"] = float(lr)\n",
    "\n",
    "    # moe_experts ∈ {3,4,5,6}\n",
    "    sample[\"moe_experts\"] = int(np.random.choice([3,4,5,6]))\n",
    "\n",
    "    # moe_k ∈ {1,2}\n",
    "    sample[\"moe_k\"] = int(np.random.choice([1,2]))\n",
    "\n",
    "    # ffn_dim ∈ [1024,4096] (discrete set multiples of 256)\n",
    "    sample[\"ffn_dim\"] = int(int(np.random.choice(np.arange(1024, 4097, 256))))\n",
    "\n",
    "    # num_layers ∈ {6,8,12}\n",
    "    sample[\"num_layers\"] = int(np.random.choice([6,8,12]))\n",
    "\n",
    "    # num_heads ∈ {6,8,12}\n",
    "    sample[\"num_heads\"] = int(np.random.choice([6,8,12]))\n",
    "\n",
    "    # word2vec_window ∈ {3,5,7}\n",
    "    sample[\"word2vec_window\"] = int(np.random.choice([3,5,7]))\n",
    "\n",
    "    # mlm_mask_prob ∈ [0.10,0.20]\n",
    "    sample[\"mlm_mask_prob\"] = float(np.random.uniform(0.10, 0.20))\n",
    "\n",
    "    # batch_size choices\n",
    "    sample[\"batch_size\"] = int(np.random.choice([4, 8, 16]))\n",
    "\n",
    "    return sample\n",
    "\n",
    "def hyperparams_to_vector(hp: Dict[str,Any]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert hyperparameter dict to numeric vector for GP.\n",
    "    We'll use a consistent ordering:\n",
    "    [log_lr, moe_experts, moe_k, ffn_dim/256, num_layers, num_heads, word2vec_window, mlm_mask_prob, batch_size]\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    vec.append(np.log10(hp[\"learning_rate\"]))\n",
    "    vec.append(float(hp[\"moe_experts\"]))\n",
    "    vec.append(float(hp[\"moe_k\"]))\n",
    "    vec.append(float(hp[\"ffn_dim\"]) / 256.0)\n",
    "    vec.append(float(hp[\"num_layers\"]))\n",
    "    vec.append(float(hp[\"num_heads\"]))\n",
    "    vec.append(float(hp[\"word2vec_window\"]))\n",
    "    vec.append(float(hp[\"mlm_mask_prob\"]))\n",
    "    vec.append(float(hp.get(\"batch_size\", BATCH_SIZE)))\n",
    "    return np.array(vec, dtype=float)\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, sigma: np.ndarray, best: float, xi: float = 0.01):\n",
    "    \"\"\"\n",
    "    Expected Improvement (for minimization: improvement = best - f).\n",
    "    mu, sigma: arrays of shape (n_candidates,)\n",
    "    best: current best (lower is better)\n",
    "    We compute EI = E[max(best - f, 0)] where f ~ N(mu, sigma^2)\n",
    "    \"\"\"\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "    imp = best - mu - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma == 0.0] = 0.0\n",
    "    return ei\n",
    "\n",
    "# -------------------------\n",
    "# Objective function that runs K-fold CV on combined train+val dataset\n",
    "# -------------------------\n",
    "def objective_function_with_kfold(hp: Dict[str,Any],\n",
    "                                  vocab_size: int,\n",
    "                                  emb: torch.Tensor,\n",
    "                                  pad_id: int,\n",
    "                                  mask_id: int,\n",
    "                                  combined_indices: List[int],\n",
    "                                  ds: Dataset,\n",
    "                                  k_folds: int,\n",
    "                                  run_id: int,\n",
    "                                  verbose: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Given hyperparameters and the combined indices (train+val indices referring to original dataset ds),\n",
    "    perform k-fold CV on that combined set. For each fold:\n",
    "      - Create model from scratch\n",
    "      - Train for TRIAL_EPOCHS on training folds\n",
    "      - Evaluate on validation fold (compute avg total loss)\n",
    "    Return the average validation loss across folds.\n",
    "\n",
    "    combined_indices: list of indices into ds to be used for CV\n",
    "    ds: original dataset\n",
    "    \"\"\"\n",
    "    # Adjust k_folds if dataset is too small\n",
    "    n_samples = len(combined_indices)\n",
    "    actual_k_folds = min(k_folds, n_samples)\n",
    "    if actual_k_folds < k_folds:\n",
    "        if verbose:\n",
    "            print(f\"[Run {run_id}] Warning: Reducing k_folds from {k_folds} to {actual_k_folds} due to small dataset size ({n_samples} samples)\")\n",
    "    \n",
    "    # Set up KFold (shuffle for randomness but fixed random_state for reproducibility)\n",
    "    kf = KFold(n_splits=actual_k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_losses = []\n",
    "    fold_details = []\n",
    "\n",
    "    # For each fold, create train and val Subset objects\n",
    "    combined_array = np.array(combined_indices)\n",
    "    fold_pbar = tqdm(enumerate(kf.split(combined_array)), total=actual_k_folds, desc=f\"[Run {run_id}] K-Fold CV\", leave=False)\n",
    "    for fold_idx, (train_pos, val_pos) in fold_pbar:\n",
    "        # Map positions to original dataset indices\n",
    "        train_indices = combined_array[train_pos].tolist()\n",
    "        val_indices = combined_array[val_pos].tolist()\n",
    "\n",
    "        train_subset = Subset(ds, train_indices)\n",
    "        val_subset = Subset(ds, val_indices)\n",
    "\n",
    "        # dataloaders\n",
    "        batch_size = int(hp.get(\"batch_size\", BATCH_SIZE))\n",
    "        dl_train = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "        dl_val = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "        # Build model afresh with hyperparameters\n",
    "        hidden_size = HIDDEN_SIZE\n",
    "        model = BertEncoderModel(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=int(hp[\"num_layers\"]),\n",
    "            num_heads=int(hp[\"num_heads\"]),\n",
    "            ffn_dim=int(hp[\"ffn_dim\"]),\n",
    "            max_position_embeddings=MAX_SEQ_LEN,\n",
    "            pad_token_id=pad_id,\n",
    "            embedding_weights=emb,\n",
    "            moe_experts=int(hp[\"moe_experts\"]),\n",
    "            moe_k=int(hp[\"moe_k\"])\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "        lr = float(hp[\"learning_rate\"])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "        # override global MLM mask prob temporaily\n",
    "        global MLM_MASK_PROB\n",
    "        prev_mlm_mask_prob = MLM_MASK_PROB\n",
    "        MLM_MASK_PROB = float(hp.get(\"mlm_mask_prob\", MLM_MASK_PROB))\n",
    "        aux_coeff = 0.01\n",
    "\n",
    "        try:\n",
    "            epoch_pbar = tqdm(range(TRIAL_EPOCHS), desc=f\"Fold {fold_idx+1}/{actual_k_folds} Epochs\", leave=False)\n",
    "            for epoch in epoch_pbar:\n",
    "                _train_loss = train_one_epoch(model, dl_train, optimizer, mlm_loss_fct, nsp_loss_fct,\n",
    "                                              pad_id, mask_id, vocab_size, DEVICE, aux_coeff)\n",
    "                epoch_pbar.set_postfix({\"train_loss\": f\"{_train_loss:.6f}\"})\n",
    "                if verbose:\n",
    "                    print(f\"[Run {run_id}] Fold {fold_idx+1}/{actual_k_folds} Epoch {epoch+1}/{TRIAL_EPOCHS} - train_loss: {_train_loss:.6f}\")\n",
    "\n",
    "            # Evaluate on the fold's validation data\n",
    "            val_loss = evaluate_model(model, dl_val, pad_id, mask_id, vocab_size, DEVICE)\n",
    "            # Validate loss is finite\n",
    "            if not np.isfinite(val_loss):\n",
    "                if verbose:\n",
    "                    print(f\"[Run {run_id}] Warning: Fold {fold_idx+1} returned non-finite loss, using large penalty value\")\n",
    "                val_loss = 1e9\n",
    "            fold_losses.append(val_loss)\n",
    "            fold_details.append({\"fold\": fold_idx+1, \"val_loss\": float(val_loss), \"train_size\": len(train_subset), \"val_size\": len(val_subset)})\n",
    "            fold_pbar.set_postfix({\"val_loss\": f\"{val_loss:.6f}\"})\n",
    "            if verbose:\n",
    "                print(f\"[Run {run_id}] Fold {fold_idx+1}/{actual_k_folds} - val_loss: {val_loss:.6f}\")\n",
    "        finally:\n",
    "            MLM_MASK_PROB = prev_mlm_mask_prob\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # average val loss over folds\n",
    "    avg_val_loss = float(np.mean(fold_losses)) if fold_losses else float(1e9)\n",
    "    # Final check for NaN\n",
    "    if not np.isfinite(avg_val_loss):\n",
    "        avg_val_loss = 1e9\n",
    "    # for logging return also per-fold details if needed\n",
    "    if verbose:\n",
    "        print(f\"[Run {run_id}] K-Fold CV average val loss (k={actual_k_folds}): {avg_val_loss:.6f}\")\n",
    "    return avg_val_loss, fold_details\n",
    "\n",
    "# -------------------------\n",
    "# Main: Bayesian Optimization loop (does a single dataset split before BO)\n",
    "# -------------------------\n",
    "def run_bayesian_optimization_with_heldout_test_cv5(corpus: List[Tuple[str,str]]):\n",
    "    \"\"\"\n",
    "    - Prepare artifacts (vocab, emb, dataset)\n",
    "    - Split once into train/val/test (test held-out and never seen by BO)\n",
    "    - Run BO using train+val combined and K-fold CV on that combined set\n",
    "    - Retrain final model on train+val\n",
    "    - Evaluate final model on test and save metrics + model\n",
    "    \"\"\"\n",
    "    # Prepare artifacts (vocab, embeddings, dataset)\n",
    "    stoi, itos, vocab_size, emb, pad_id, mask_id, ds = prepare_data_and_model_artifacts(corpus)\n",
    "\n",
    "    # Split into Train/Val/Test once. Use 70/15/15 split (or fallback when small)\n",
    "    total_len = len(ds)\n",
    "    if total_len < 5:\n",
    "        # tiny dataset: split into train= max(1, total-2), val=1, test=1 where possible\n",
    "        test_len = 1\n",
    "        val_len = 1 if total_len - test_len - 1 > 0 else 0\n",
    "        train_len = total_len - val_len - test_len\n",
    "    else:\n",
    "        test_len = max(1, total_len // 10)  # ~10%\n",
    "        val_len = max(1, total_len // 10)\n",
    "        train_len = total_len - val_len - test_len\n",
    "    # Ensure non-negative\n",
    "    if train_len < 1:\n",
    "        train_len = max(1, total_len - val_len - test_len)\n",
    "    lengths = [train_len, val_len, test_len]\n",
    "    train_ds, val_ds, test_ds = random_split(ds, lengths)\n",
    "\n",
    "    print(f\"[DATA SPLIT] total={total_len}, train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
    "\n",
    "    # Combine train+val indices to be used for K-fold CV (BO uses only these)\n",
    "    # random_split returns Subset objects with .indices attribute referencing original dataset\n",
    "    train_indices = train_ds.indices if hasattr(train_ds, \"indices\") else list(range(len(train_ds)))\n",
    "    val_indices = val_ds.indices if hasattr(val_ds, \"indices\") else list(range(len(val_ds)))\n",
    "    combined_indices = list(train_indices) + list(val_indices)\n",
    "    random.shuffle(combined_indices)  # shuffle combined pool before KFold splitting\n",
    "\n",
    "    print(f\"[BO] Using combined train+val pool of size {len(combined_indices)} for {K_FOLDS}-fold CV in BO.\")\n",
    "\n",
    "    # BO bookkeeping\n",
    "    bo_records = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # initial random evaluations\n",
    "    print(\"[BO] Starting initial random evaluations (random seed sampling)...\")\n",
    "    init_pbar = tqdm(range(BO_INIT_POINTS), desc=\"BO Initial Samples\")\n",
    "    for i in init_pbar:\n",
    "        hp = sample_random_hyperparams()\n",
    "        # ensure fields exist for logging\n",
    "        hp[\"batch_size\"] = int(hp.get(\"batch_size\", BATCH_SIZE))\n",
    "        hp[\"word2vec_size\"] = WORD2VEC_SIZE\n",
    "        hp[\"word2vec_min_count\"] = WORD2VEC_MIN_COUNT\n",
    "        # ensure mandatory hyperparameters present (sample function sets them)\n",
    "        print(f\"[BO] Init sample {i+1}/{BO_INIT_POINTS}: {hp}\")\n",
    "        val_loss, fold_details = objective_function_with_kfold(hp, vocab_size, emb, pad_id, mask_id, combined_indices, ds, K_FOLDS, run_id=i+1, verbose=True)\n",
    "        vec = hyperparams_to_vector(hp)\n",
    "        X.append(vec)\n",
    "        y.append(val_loss)\n",
    "        rec = {\"iteration\": i+1, \"params\": hp, \"loss\": float(val_loss), \"folds\": fold_details}\n",
    "        bo_records.append(rec)\n",
    "        init_pbar.set_postfix({\"val_loss\": f\"{val_loss:.6f}\"})\n",
    "        # save intermediate logs\n",
    "        with open(os.path.join(BO_LOG_DIR, \"bo_results.json\"), \"w\") as f:\n",
    "            json.dump(bo_records, f, indent=2)\n",
    "\n",
    "    # GP surrogate\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(X[0].shape[0]), nu=2.5) + WhiteKernel(noise_level=1e-6)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=3, random_state=42)\n",
    "\n",
    "    # main BO iterations\n",
    "    bo_pbar = tqdm(range(BO_INIT_POINTS, BO_ITERATIONS), desc=\"BO Iterations\")\n",
    "    for it in bo_pbar:\n",
    "        print(f\"[BO] Iteration {it+1}/{BO_ITERATIONS}\")\n",
    "\n",
    "        # fit GP on current data\n",
    "        X_arr = np.vstack(X)\n",
    "        y_arr = np.array(y)\n",
    "        gp.fit(X_arr, y_arr)\n",
    "\n",
    "        # propose many random candidates and pick one that maximizes EI\n",
    "        n_candidates = 200\n",
    "        candidates = []\n",
    "        cand_vecs = []\n",
    "        for _ in range(n_candidates):\n",
    "            c = sample_random_hyperparams()\n",
    "            c[\"batch_size\"] = int(c.get(\"batch_size\", BATCH_SIZE))\n",
    "            c[\"word2vec_size\"] = WORD2VEC_SIZE\n",
    "            c[\"word2vec_min_count\"] = WORD2VEC_MIN_COUNT\n",
    "            candidates.append(c)\n",
    "            cand_vecs.append(hyperparams_to_vector(c))\n",
    "        cand_vecs = np.vstack(cand_vecs)\n",
    "\n",
    "        mu, sigma = gp.predict(cand_vecs, return_std=True)\n",
    "        best_so_far = np.min(y_arr)\n",
    "        eis = expected_improvement(mu, sigma, best_so_far, xi=0.01)\n",
    "        best_idx = int(np.argmax(eis))\n",
    "        chosen_hp = candidates[best_idx]\n",
    "        print(f\"[BO] Chosen hyperparams (EI max): {chosen_hp}\")\n",
    "\n",
    "        # evaluate chosen configuration via K-fold CV\n",
    "        val_loss, fold_details = objective_function_with_kfold(chosen_hp, vocab_size, emb, pad_id, mask_id, combined_indices, ds, K_FOLDS, run_id=it+1, verbose=True)\n",
    "        vec = hyperparams_to_vector(chosen_hp)\n",
    "        X.append(vec)\n",
    "        y.append(val_loss)\n",
    "\n",
    "        rec = {\"iteration\": it+1, \"params\": chosen_hp, \"loss\": float(val_loss), \"folds\": fold_details}\n",
    "        bo_pbar.set_postfix({\"val_loss\": f\"{val_loss:.6f}\", \"best\": f\"{np.min(y):.6f}\"})\n",
    "        # save logs\n",
    "        with open(os.path.join(BO_LOG_DIR, \"bo_results.json\"), \"w\") as f:\n",
    "            json.dump(bo_records, f, indent=2)\n",
    "\n",
    "    # compute best config\n",
    "    losses = [r[\"loss\"] for r in bo_records]\n",
    "    best_idx = int(np.argmin(losses))\n",
    "    best_record = bo_records[best_idx]\n",
    "    best_hp = best_record[\"params\"]\n",
    "    best_loss = best_record[\"loss\"]\n",
    "    print(f\"[BO] Best config found: {best_hp} with loss {best_loss:.6f}\")\n",
    "\n",
    "    # save best config to file\n",
    "    with open(os.path.join(BO_LOG_DIR, \"best_config.json\"), \"w\") as f:\n",
    "        json.dump({\"best_params\": best_hp, \"best_loss\": best_loss}, f, indent=2)\n",
    "\n",
    "    # Retrain final model on train+val with best hyperparams\n",
    "    print(\"[BO] Training final model on train+val with best hyperparameters...\")\n",
    "    # Combine train + val indices into a single dataset (Subset)\n",
    "    # random_split returned Subset objects; their indices refer to ds\n",
    "    train_indices = train_ds.indices if hasattr(train_ds, \"indices\") else list(range(len(train_ds)))\n",
    "    val_indices = val_ds.indices if hasattr(val_ds, \"indices\") else list(range(len(val_ds)))\n",
    "    combined_trainval_indices = list(train_indices) + list(val_indices)\n",
    "    dl_full = DataLoader(Subset(ds, combined_trainval_indices), batch_size=int(best_hp.get(\"batch_size\", BATCH_SIZE)), shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "    final_model = BertEncoderModel(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=int(best_hp[\"num_layers\"]),\n",
    "        num_heads=int(best_hp[\"num_heads\"]),\n",
    "        ffn_dim=int(best_hp[\"ffn_dim\"]),\n",
    "        max_position_embeddings=MAX_SEQ_LEN,\n",
    "        pad_token_id=pad_id,\n",
    "        embedding_weights=emb,\n",
    "        moe_experts=int(best_hp[\"moe_experts\"]),\n",
    "        moe_k=int(best_hp[\"moe_k\"])\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # optimizer\n",
    "    lr = float(best_hp[\"learning_rate\"])\n",
    "    optimizer = torch.optim.AdamW(final_model.parameters(), lr=lr)\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    # set global MLM mask prob to chosen\n",
    "    global MLM_MASK_PROB\n",
    "    prev_mlm_mask_prob = MLM_MASK_PROB\n",
    "    MLM_MASK_PROB = float(best_hp.get(\"mlm_mask_prob\", MLM_MASK_PROB))\n",
    "    aux_coeff = 0.01\n",
    "\n",
    "    try:\n",
    "        final_epoch_pbar = tqdm(range(TRIAL_EPOCHS), desc=\"Final Training\")\n",
    "        for epoch in final_epoch_pbar:\n",
    "            t_loss = train_one_epoch(final_model, dl_full, optimizer, mlm_loss_fct, nsp_loss_fct, pad_id, mask_id, vocab_size, DEVICE, aux_coeff)\n",
    "            final_epoch_pbar.set_postfix({\"loss\": f\"{t_loss:.6f}\"})\n",
    "            print(f\"[Final Train] Epoch {epoch+1}/{TRIAL_EPOCHS} - loss {t_loss:.6f}\")\n",
    "    finally:\n",
    "        MLM_MASK_PROB = prev_mlm_mask_prob\n",
    "\n",
    "    # Evaluate final model on the held-out test set (never used in BO)\n",
    "    test_dl = DataLoader(test_ds, batch_size=int(best_hp.get(\"batch_size\", BATCH_SIZE)), shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    test_metrics = compute_metrics(final_model, test_dl, pad_id, mask_id, vocab_size, DEVICE)\n",
    "    print(\"[Final Evaluation on TEST set]\")\n",
    "    print(json.dumps(test_metrics, indent=2))\n",
    "\n",
    "    # Save final model and vocab into bo_logs/best_model/\n",
    "    save_dir = os.path.join(BO_LOG_DIR, \"best_model\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(final_model.state_dict(), os.path.join(save_dir, \"bert_encoder.pt\"))\n",
    "    with open(os.path.join(save_dir, \"vocab.json\"), \"w\") as f:\n",
    "        json.dump({\"stoi\": stoi, \"itos\": itos}, f)\n",
    "    # Save test metrics and BO records\n",
    "    with open(os.path.join(save_dir, \"test_metrics.json\"), \"w\") as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    with open(os.path.join(BO_LOG_DIR, \"bo_results.json\"), \"w\") as f:\n",
    "        json.dump(bo_records, f, indent=2)\n",
    "    print(f\"[BO] Best model, vocab and test metrics saved to {save_dir}\")\n",
    "    print(f\"[BO] Best model, vocab and test metrics saved to {save_dir}\")\n",
    "    return {\"best_record\": best_record, \"test_metrics\": test_metrics, \"bo_records\": bo_records}\n",
    "    return {\"best_record\": best_record, \"test_metrics\": test_metrics, \"bo_records\": bo_records}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load corpus from ChromaDB\n",
    "# -------------------------\n",
    "def load_corpus_from_chromadb(db_path: str = \"../VectorDB/chroma_Data\", collection_name: str = \"harry_potter_collection\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Load corpus from ChromaDB collection.\n",
    "    Returns list of tuples: [(text, dtype), ...] where dtype is 'Q' for query or 'C' for chunk\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading corpus from ChromaDB at: {db_path}\")\n",
    "    print(f\"[INFO] Collection name: {collection_name}\")\n",
    "    \n",
    "    try:\n",
    "        client_db = chromadb.PersistentClient(path=db_path)\n",
    "        collection = client_db.get_collection(name=collection_name)\n",
    "        \n",
    "        # Get all documents with metadata\n",
    "        results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "        \n",
    "        corpus = []\n",
    "        chunk_count = 0\n",
    "        query_count = 0\n",
    "        \n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "            # Check if it's a chunk or query based on metadata\n",
    "            if meta.get(\"ischunk\") is True:\n",
    "                corpus.append((doc, \"C\"))\n",
    "                chunk_count += 1\n",
    "            elif meta.get(\"ischunk\") is False:\n",
    "                corpus.append((doc, \"Q\"))\n",
    "                query_count += 1\n",
    "            else:\n",
    "                # If metadata doesn't have ischunk, skip or assume it's a chunk\n",
    "                print(f\"[WARNING] Document without 'ischunk' metadata: {doc[:50]}...\")\n",
    "                corpus.append((doc, \"C\"))  # default to chunk\n",
    "                chunk_count += 1\n",
    "        \n",
    "        print(f\"[INFO] Loaded {len(corpus)} documents: {chunk_count} chunks, {query_count} queries\")\n",
    "        return corpus\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load from ChromaDB: {e}\")\n",
    "        print(\"[INFO] Falling back to default corpus\")\n",
    "        return get_default_corpus()\n",
    "\n",
    "# Entrypoint\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Load corpus from ChromaDB (or fall back to default if not available)\n",
    "    corpus = load_corpus_from_chromadb()\n",
    "    \n",
    "    # If ChromaDB loading failed and we only have 4 items, it's the default corpus\n",
    "    if len(corpus) == 4:\n",
    "        print(\"[INFO] Using small default corpus for testing\")\n",
    "    \n",
    "    print(\"[MAIN] Starting Bayesian Optimization over hyperparameters with 5-fold CV per BO eval...\")\n",
    "    results = run_bayesian_optimization_with_heldout_test_cv5(corpus)\n",
    "    print(\"[MAIN] BO Completed. Summary:\")\n",
    "    print(json.dumps(results[\"best_record\"], indent=2))\n",
    "    print(\"Test metrics:\")\n",
    "    print(json.dumps(results[\"test_metrics\"], indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
