{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Process 1559818 has 3.07 GiB memory in use. Of the allocated memory 6.83 GiB is allocated by PyTorch, and 691.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 508\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# RUN TRAINING\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mtrain_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 460\u001b[39m, in \u001b[36mtrain_lora\u001b[39m\u001b[34m(model_path, vocab_path, csv_path, batch_size, lr, epochs)\u001b[39m\n\u001b[32m    456\u001b[39m model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n\u001b[32m    458\u001b[39m apply_lora(model)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m ds = ContrastiveDataset(csv_path)\n\u001b[32m    463\u001b[39m dl = DataLoader(ds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_2/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Process 1559818 has 3.07 GiB memory in use. Of the allocated memory 6.83 GiB is allocated by PyTorch, and 691.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import csv\n",
    "import chromadb\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD YOUR PRETRAINED BERT ENCODER MODEL WITH MoE\n",
    "#    (This must match your previously saved architecture)\n",
    "# ============================================================\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# ------------------------\n",
    "# Mixture of Experts (MoE)\n",
    "# ------------------------\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, H = x.size()\n",
    "        logits = self.router(x)\n",
    "        probs_all = F.softmax(logits, dim=-1)\n",
    "        importance = probs_all.sum(dim=(0, 1))\n",
    "        total_tokens = float(B * S)\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "        \n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "        \n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)\n",
    "        \n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)\n",
    "        \n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)\n",
    "        flat_idx = topk_idx.view(-1, self.k)\n",
    "        flat_w = topk_weights.view(-1, self.k)\n",
    "        gating_flat = gating.view(-1, self.num_experts)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)\n",
    "        \n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)\n",
    "        return out, aux_loss\n",
    "\n",
    "# ------------------------\n",
    "# Transformer Layer with MoE\n",
    "# ------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "# ------------------------\n",
    "# Base BERT Encoder with MoE\n",
    "# ------------------------\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=768,\n",
    "                 num_layers=12, num_heads=12, ffn_dim=3072,\n",
    "                 max_position_embeddings=512, pad_token_id=0,\n",
    "                 moe_experts=5, moe_k=2, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size,\n",
    "                                             padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=0.1,\n",
    "                                   moe_experts=moe_experts, moe_k=moe_k)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # These are in the saved model but not used for fine-tuning\n",
    "        self.nsp_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = (self.token_embeddings(ids) +\n",
    "             self.position_embeddings(pos) +\n",
    "             self.segment_embeddings(tt))\n",
    "\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux_loss = layer(x, mask)\n",
    "            total_aux = total_aux + aux_loss\n",
    "\n",
    "        return x[:, 0]   # CLS embedding\n",
    "\n",
    "# ============================================================\n",
    "# 2. APPLY LoRA TO THE BERT ATTENTION MODULES\n",
    "# ============================================================\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"A LoRA wrapper for linear layers.\"\"\"\n",
    "    def __init__(self, base_layer, r=8, alpha=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.base = base_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.train_lora_only()\n",
    "\n",
    "    def train_lora_only(self):\n",
    "        \"\"\"Freeze base layer, train LoRA params only.\"\"\"\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.lora_A.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.lora_B.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "\n",
    "def apply_lora(model, r=8, alpha=8):\n",
    "    \"\"\"Wraps all MHA Q,K,V projection layers with LoRA.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        attn = layer.self_attn\n",
    "\n",
    "        attn.in_proj_weight = nn.Parameter(attn.in_proj_weight, requires_grad=False)\n",
    "        attn.in_proj_bias = nn.Parameter(attn.in_proj_bias, requires_grad=False)\n",
    "\n",
    "        hidden = attn.embed_dim\n",
    "        q_proj = nn.Linear(hidden, hidden)\n",
    "        k_proj = nn.Linear(hidden, hidden)\n",
    "        v_proj = nn.Linear(hidden, hidden)\n",
    "\n",
    "        attn.q_proj_weight = q_proj.weight\n",
    "        attn.k_proj_weight = k_proj.weight\n",
    "        attn.v_proj_weight = v_proj.weight\n",
    "\n",
    "        attn.q_proj = LoRALinear(q_proj, r=r, alpha=alpha)\n",
    "        attn.k_proj = LoRALinear(k_proj, r=r, alpha=alpha)\n",
    "        attn.v_proj = LoRALinear(v_proj, r=r, alpha=alpha)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# 3. CHROMADB LOADING\n",
    "# ============================================================\n",
    "client = chromadb.PersistentClient(path=\"../VectorDB/chroma_Data_with_BERT_embeddings\")\n",
    "collection = client.get_collection(\"HP_Chunks_BERT_Embeddings_collection\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. TRAINING DATASET (CSV WITH query, chunk_ID)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class QueryChunkPair:\n",
    "    query: str\n",
    "    positive_id: str\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.pairs: List[QueryChunkPair] = []\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                # Expecting: query, positive_id\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                self.pairs.append(QueryChunkPair(query=row[0], positive_id=row[1]))\n",
    "\n",
    "        # Query chroma for chunk entries. Adjust the where-clause if your metadata differs.\n",
    "        all_chunks = collection.get(where={\"ischunk\": True})\n",
    "        # Defensive: ensure \"ids\" key exists and is a list\n",
    "        ids = all_chunks.get(\"ids\") if isinstance(all_chunks, dict) else None\n",
    "        if not ids:\n",
    "            # No chunk ids found â€” raise a clear error with suggestions\n",
    "            raise RuntimeError(\n",
    "                \"No chunk ids found in Chroma collection for where={'ischunk': True}. \"\n",
    "                \"Check collection path and metadata keys. \"\n",
    "                \"Got response keys: {}. Response summary: {}\".format(\n",
    "                    list(all_chunks.keys()) if isinstance(all_chunks, dict) else str(type(all_chunks)),\n",
    "                    {k: (len(all_chunks[k]) if isinstance(all_chunks.get(k), list) else type(all_chunks.get(k)))\n",
    "                     for k in (all_chunks.keys() if isinstance(all_chunks, dict) else [])}\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.chunk_ids = list(ids)\n",
    "\n",
    "        # Optionally create a set for fast membership tests\n",
    "        self.chunk_id_set = set(self.chunk_ids)\n",
    "\n",
    "        # Basic sanity: confirm positive IDs from CSV exist (warn only)\n",
    "        missing_positives = [p.positive_id for p in self.pairs if p.positive_id not in self.chunk_id_set]\n",
    "        if missing_positives:\n",
    "            # Show a short sample to not flood logs\n",
    "            sample_missing = missing_positives[:10]\n",
    "            print(f\"[Warning] {len(missing_positives)} positive IDs from CSV not found in Chroma collection. Examples: {sample_missing}\")\n",
    "            # You may choose to raise here if this is critical:\n",
    "            # raise RuntimeError(\"Positive IDs mismatch between CSV and Chroma collection.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def sample_negatives(self, positive_id: str, k=5):\n",
    "        \"\"\"\n",
    "        Robust negative sampling:\n",
    "        - Build candidate list excluding the positive_id\n",
    "        - If there are >= k distinct candidates: use random.sample (no replacement)\n",
    "        - If there are between 1 and k-1 candidates: sample with replacement from candidates\n",
    "        - If there are 0 candidates: raise informative error\n",
    "        \"\"\"\n",
    "        # Build candidate pool excluding the positive\n",
    "        if not self.chunk_ids:\n",
    "            raise RuntimeError(\"No chunk ids available in dataset (self.chunk_ids is empty). Check Chroma collection population.\")\n",
    "        candidates = [cid for cid in self.chunk_ids if cid != positive_id]\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"No negative candidates available for positive_id={positive_id}. \"\n",
    "                \"Either the collection only contains that one id or positive_id doesn't exist in collection.\"\n",
    "            )\n",
    "\n",
    "        if len(candidates) >= k:\n",
    "            return random.sample(candidates, k)\n",
    "        else:\n",
    "            # sample with replacement to meet required k\n",
    "            return [random.choice(candidates) for _ in range(k)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "\n",
    "        # Validate positive id exists\n",
    "        if item.positive_id not in self.chunk_id_set:\n",
    "            # Return a helpful error rather than failing deep inside\n",
    "            raise RuntimeError(f\"Positive id '{item.positive_id}' from CSV not found in Chroma collection.\")\n",
    "\n",
    "        pos_chunk = collection.get(ids=[item.positive_id])\n",
    "        pos_documents = pos_chunk.get(\"documents\", [])\n",
    "        if not pos_documents:\n",
    "            raise RuntimeError(f\"No document found in Chroma for positive id {item.positive_id} (collection.get returned empty).\")\n",
    "\n",
    "        neg_ids = self.sample_negatives(item.positive_id, k=5)\n",
    "        neg_chunks = collection.get(ids=neg_ids)\n",
    "        neg_documents = neg_chunks.get(\"documents\", [])\n",
    "\n",
    "        # If some negatives are missing, fallback to empty strings (or raise if you prefer)\n",
    "        if len(neg_documents) < len(neg_ids):\n",
    "            # fill missing with empty strings to keep shapes consistent\n",
    "            filled = []\n",
    "            for i, cid in enumerate(neg_ids):\n",
    "                try:\n",
    "                    doc = neg_documents[i]\n",
    "                except Exception:\n",
    "                    doc = \"\"\n",
    "                filled.append(doc)\n",
    "            neg_documents = filled\n",
    "\n",
    "        return {\n",
    "            \"query\": item.query,\n",
    "            \"positive_text\": pos_documents[0],\n",
    "            \"negative_texts\": neg_documents\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# 5. TOKENIZER (simple whitespace tokenizer)\n",
    "# ============================================================\n",
    "\n",
    "def tokenize(text, stoi, max_len=64):\n",
    "    tokens = text.strip().split()\n",
    "    ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in tokens][: max_len-2]\n",
    "    ids = [stoi[CLS_TOKEN]] + ids + [stoi[SEP_TOKEN]]\n",
    "    tt = [0] * len(ids)\n",
    "    mask = [1] * len(ids)\n",
    "    return (\n",
    "        torch.tensor(ids, dtype=torch.long),\n",
    "        torch.tensor(tt, dtype=torch.long),\n",
    "        torch.tensor(mask, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "def collate(batch):\n",
    "    queries, q_tt, q_mask = [], [], []\n",
    "    pos, pt_tt, pt_mask = [], [], []\n",
    "    negs, nt_tt, nt_mask = [], [], []\n",
    "\n",
    "    # Find the true max length across all queries, positives, and negatives in the batch\n",
    "    pad_len = 0\n",
    "    for b in batch:\n",
    "        pad_len = max(pad_len, len(b[\"query\"].split())+2, len(b[\"positive_text\"].split())+2)\n",
    "        for nt in b[\"negative_texts\"]:\n",
    "            pad_len = max(pad_len, len(nt.strip().split())+2)\n",
    "\n",
    "    # === START PATCH: cap sequence length to avoid MoE OOM ===\n",
    "    MAX_LEN = 128\n",
    "    pad_len = min(pad_len, MAX_LEN)\n",
    "    # === END PATCH ===\n",
    "\n",
    "    def pad_tensor(tensor, length, pad_value):\n",
    "        if tensor.size(0) < length:\n",
    "            pad_amt = length - tensor.size(0)\n",
    "            return torch.cat([tensor, torch.full((pad_amt,), pad_value, dtype=tensor.dtype)])\n",
    "        elif tensor.size(0) > length:\n",
    "            return tensor[:length]\n",
    "        return tensor\n",
    "\n",
    "    for b in batch:\n",
    "        q_ids, qtt, qmask = tokenize(b[\"query\"], stoi, pad_len)\n",
    "        p_ids, ptt, pmask = tokenize(b[\"positive_text\"], stoi, pad_len)\n",
    "\n",
    "        # Always pad/truncate queries and positives\n",
    "        q_ids = pad_tensor(q_ids, pad_len, stoi[PAD_TOKEN])\n",
    "        qtt = pad_tensor(qtt, pad_len, 0)\n",
    "        qmask = pad_tensor(qmask, pad_len, 0)\n",
    "        p_ids = pad_tensor(p_ids, pad_len, stoi[PAD_TOKEN])\n",
    "        ptt = pad_tensor(ptt, pad_len, 0)\n",
    "        pmask = pad_tensor(pmask, pad_len, 0)\n",
    "\n",
    "        neg_ids_list = []\n",
    "        neg_tt_list = []\n",
    "        neg_mask_list = []\n",
    "\n",
    "        for nt in b[\"negative_texts\"]:\n",
    "            ids, tti, msk = tokenize(nt, stoi, pad_len)\n",
    "            ids = pad_tensor(ids, pad_len, stoi[PAD_TOKEN])\n",
    "            tti = pad_tensor(tti, pad_len, 0)\n",
    "            msk = pad_tensor(msk, pad_len, 0)\n",
    "            neg_ids_list.append(ids)\n",
    "            neg_tt_list.append(tti)\n",
    "            neg_mask_list.append(msk)\n",
    "\n",
    "        queries.append(q_ids)\n",
    "        q_tt.append(qtt)\n",
    "        q_mask.append(qmask)\n",
    "\n",
    "        pos.append(p_ids)\n",
    "        pt_tt.append(ptt)\n",
    "        pt_mask.append(pmask)\n",
    "\n",
    "        negs.append(torch.stack(neg_ids_list))\n",
    "        nt_tt.append(torch.stack(neg_tt_list))\n",
    "        nt_mask.append(torch.stack(neg_mask_list))\n",
    "\n",
    "    return {\n",
    "        \"queries\": torch.stack(queries),\n",
    "        \"queries_tt\": torch.stack(q_tt),\n",
    "        \"queries_mask\": torch.stack(q_mask),\n",
    "\n",
    "        \"pos\": torch.stack(pos),\n",
    "        \"pos_tt\": torch.stack(pt_tt),\n",
    "        \"pos_mask\": torch.stack(pt_mask),\n",
    "\n",
    "        \"neg\": torch.stack(negs),\n",
    "        \"neg_tt\": torch.stack(nt_tt),\n",
    "        \"neg_mask\": torch.stack(nt_mask),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 6. CONTRASTIVE LOSS (softmax over exp(cos()))\n",
    "# ============================================================\n",
    "\n",
    "def contrastive_loss(q, pos, negs):\n",
    "    \"\"\"\n",
    "    q: (B, H)\n",
    "    pos: (B, H)\n",
    "    negs: (B, 5, H)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_sim = F.cosine_similarity(q, pos)      # (B,)\n",
    "    neg_sim = F.cosine_similarity(\n",
    "        q.unsqueeze(1).repeat(1, negs.size(1), 1),\n",
    "        negs,\n",
    "        dim=-1\n",
    "    )  # (B, 5)\n",
    "\n",
    "    sims = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # (B, 6)\n",
    "    exp_sims = torch.exp(sims)\n",
    "    probs = exp_sims / exp_sims.sum(dim=1, keepdim=True)\n",
    "\n",
    "    loss = -torch.log(probs[:, 0]).mean()\n",
    "    return loss\n",
    "\n",
    "# ============================================================\n",
    "# 7. MAIN TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_lora(\n",
    "    model_path=\"../Encoder/saved_bert_encoder_moe_pooling/bert_encoder_moe_pooling.pt\",\n",
    "    vocab_path=\"../Encoder/saved_bert_encoder_moe_pooling/vocab.json\",\n",
    "    csv_path=\"../LLM Caller/generated_pairs_without_commas.csv\",\n",
    "    batch_size=1,\n",
    "    lr=1e-4,\n",
    "    epochs=5,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    global stoi, itos\n",
    "    stoi, itos = vocab[\"stoi\"], vocab[\"itos\"]\n",
    "    vocab_size = len(itos)\n",
    "\n",
    "    # Create model with MoE architecture matching the saved model\n",
    "    model = BertEncoderModel(vocab_size, max_position_embeddings=512, \n",
    "                            moe_experts=5, moe_k=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "    apply_lora(model)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    ds = ContrastiveDataset(csv_path)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dl:\n",
    "            q = batch[\"queries\"].to(DEVICE)\n",
    "            q_tt = batch[\"queries_tt\"].to(DEVICE)\n",
    "            q_mask = batch[\"queries_mask\"].to(DEVICE)\n",
    "\n",
    "            p = batch[\"pos\"].to(DEVICE)\n",
    "            p_tt = batch[\"pos_tt\"].to(DEVICE)\n",
    "            p_mask = batch[\"pos_mask\"].to(DEVICE)\n",
    "\n",
    "            n = batch[\"neg\"].to(DEVICE)\n",
    "            n_tt = batch[\"neg_tt\"].to(DEVICE)\n",
    "            n_mask = batch[\"neg_mask\"].to(DEVICE)\n",
    "\n",
    "            q_emb = model.encode(q, q_tt, q_mask)\n",
    "            p_emb = model.encode(p, p_tt, p_mask)\n",
    "\n",
    "            B, K, L = n.size()\n",
    "            n = n.view(B*K, L)\n",
    "            n_tt = n_tt.view(B*K, L)\n",
    "            n_mask = n_mask.view(B*K, L)\n",
    "\n",
    "            # micro-batch to prevent OOM\n",
    "            chunks = []\n",
    "            MB = 2  # micro-batch size (2 or even 1)\n",
    "            for i in range(0, B*K, MB):\n",
    "                part = n[i:i+MB]\n",
    "                part_tt = n_tt[i:i+MB]\n",
    "                part_mask = n_mask[i:i+MB]\n",
    "                chunks.append(model.encode(part, part_tt, part_mask))\n",
    "\n",
    "            n_emb = torch.cat(chunks, dim=0).view(B, K, -1)\n",
    "\n",
    "            loss = contrastive_loss(q_emb, p_emb, n_emb)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} Loss {loss.item():.4f}\")\n",
    "\n",
    "    os.makedirs(\"lora_finetuned\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"lora_finetuned/lora_bert.pt\")\n",
    "    print(\"LoRA fine-tuned model saved.\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_lora()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
