{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Encoder Reranker with Grid Search and K-Fold CV\n",
    "\n",
    "Changes from original:\n",
    " - Automatically splits input CSV into 90% (train+val) and 10% (test)\n",
    " - Performs 10-fold cross-validation with Grid Search for hyperparameter tuning\n",
    " - Uses BCEWithLogitsLoss and CLS embedding for relevance score\n",
    " - Evaluates final best model on test set\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, queries: List[str], chunks: List[str], labels: List[int]):\n",
    "        assert len(queries) == len(chunks) == len(labels)\n",
    "        self.queries = queries\n",
    "        self.chunks = chunks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"query\": self.queries[idx],\n",
    "            \"chunk\": self.chunks[idx],\n",
    "            \"label\": float(self.labels[idx]),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict], tokenizer, max_length: int):\n",
    "    queries = [b[\"query\"] for b in batch]\n",
    "    chunks = [b[\"chunk\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.float)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        queries, chunks, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    if \"token_type_ids\" not in enc:\n",
    "        enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"token_type_ids\": enc[\"token_type_ids\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "\n",
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name_or_path)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        nn.init.normal_(self.classifier.weight, mean=0.0, std=self.encoder.config.initializer_range)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_emb)\n",
    "        logits = self.classifier(x).squeeze(-1)\n",
    "        return logits, cls_emb\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "\n",
    "def load_csv_dataset(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    assert {\"query\", \"chunk\", \"label\"} <= set(df.columns)\n",
    "    return df\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    all_logits, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"loss\": total_loss / len(labels)}\n",
    "\n",
    "def train_one_fold(train_loader, val_loader, args, lr, dropout):\n",
    "    model = CrossEncoder(args.model_name_or_path, dropout_prob=dropout).to(args.device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(args.warmup_steps_ratio * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    for _ in range(args.epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(args.device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(args.device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(args.device)\n",
    "            labels = batch[\"labels\"].to(args.device)\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    val_metrics = evaluate(model, val_loader, args.device)\n",
    "    return model, val_metrics\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "\n",
    "def main(args):\n",
    "    df = load_csv_dataset(args.data_csv)\n",
    "\n",
    "    # Split 90% train+val, 10% test\n",
    "    trainval_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "\n",
    "    trainval_ds = PairDataset(trainval_df[\"query\"].tolist(), trainval_df[\"chunk\"].tolist(), trainval_df[\"label\"].tolist())\n",
    "    test_ds = PairDataset(test_df[\"query\"].tolist(), test_df[\"chunk\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    param_grid = {\"lr\": [2e-5, 3e-5, 5e-5], \"dropout\": [0.1, 0.2]}\n",
    "\n",
    "    best_auc = -1\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_aucs = []\n",
    "        print(f\"\\nGrid params: {params}\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(trainval_ds)))):\n",
    "            train_subset = Subset(trainval_ds, train_idx)\n",
    "            val_subset = Subset(trainval_ds, val_idx)\n",
    "            train_loader = DataLoader(\n",
    "                train_subset,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subset,\n",
    "                batch_size=args.eval_batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "            )\n",
    "            model, val_metrics = train_one_fold(train_loader, val_loader, args, lr=params[\"lr\"], dropout=params[\"dropout\"])\n",
    "            fold_aucs.append(val_metrics[\"auc\"])\n",
    "            print(f\"Fold {fold+1} AUC: {val_metrics['auc']:.4f}\")\n",
    "        mean_auc = np.mean(fold_aucs)\n",
    "        print(f\"Mean AUC for {params}: {mean_auc:.4f}\")\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    print(\"\\nBest params:\", best_params)\n",
    "    print(\"Best CV AUC:\", best_auc)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, args.max_length),\n",
    "    )\n",
    "    test_metrics = evaluate(best_model, test_loader, args.device)\n",
    "    print(\"\\nTest Set Results:\", test_metrics)\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    torch.save(best_model.state_dict(), os.path.join(args.output_dir, \"best_model.pt\"))\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(\"Saved best model and tokenizer to\", args.output_dir)\n",
    "\n",
    "# ------------------- Args -------------------\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Cross-Encoder fine-tuning with Grid Search + K-Fold CV\")\n",
    "    p.add_argument(\"--data_csv\", type=str, required=True, help=\"CSV with columns query,chunk,label\")\n",
    "    p.add_argument(\"--model_name_or_path\", type=str, required=True, help=\"Model name or local path\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"./crossenc_out\")\n",
    "    p.add_argument(\"--epochs\", type=int, default=2)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    p.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
    "    p.add_argument(\"--max_length\", type=int, default=256)\n",
    "    p.add_argument(\"--warmup_steps_ratio\", type=float, default=0.06)\n",
    "    p.add_argument(\"--max_grad_norm\", type=float, default=1.0)\n",
    "    p.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return p.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    args.device = torch.device(args.device)\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
