{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34579b38",
   "metadata": {},
   "source": [
    "1st Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Metadatas\n",
    "from google import genai\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "ABSOLUTE_DB_PATH = \"../VectorDB/chroma_Data\"\n",
    "COLLECTION_NAME = \"harry_potter_collection\"\n",
    "API_KEY = \"AIzaSyAyUmY_mL3O8qZk4jGXgEYM41A12kPtSm4\"\n",
    "BATCH_SIZE = 5                 # Smaller batch to respect RPM\n",
    "NUM_QUERIES_PER_CHUNK = 5\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "OUTPUT_CSV = \"generated_pairs.csv\"\n",
    "\n",
    "# Gemini 2.0 Flash rate limits\n",
    "MAX_RPM = 15                   # Requests per minute\n",
    "MAX_RPD = 200                  # Requests per day\n",
    "\n",
    "# Derived sleep time between requests in seconds\n",
    "SECONDS_PER_REQUEST = 60 / MAX_RPM\n",
    "\n",
    "# -------------------- Gemini Utilities --------------------\n",
    "def init_gemini_client(api_key: str):\n",
    "    os.environ[\"GENAI_API_KEY\"] = api_key\n",
    "    return genai.Client(api_key=api_key)\n",
    "\n",
    "def ask_gemini_generate_queries(client, chunk_text: str, chunk_id: str, num_queries: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ask Gemini to generate user-style queries for a given chunk.\n",
    "    Includes few-shot examples to ensure short, natural queries.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an AI that generates realistic search queries a user might input to an LLM or search system.\n",
    "Each query should be short, relevant, and reflect what someone might actually ask.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Chunk: \"Harry receives his first letter from Hogwarts, but Uncle Vernon tries to stop him.\"\n",
    "Queries:\n",
    "- \"How did Harry get his Hogwarts letter?\"\n",
    "- \"Why did Uncle Vernon hide Harry's letter?\"\n",
    "- \"First Hogwarts letter incident\"\n",
    "\n",
    "Example 2:\n",
    "Chunk: \"Hagrid visits Harry to explain that he is a wizard.\"\n",
    "Queries:\n",
    "- \"Who is Hagrid and why did he visit Harry?\"\n",
    "- \"How did Harry find out he is a wizard?\"\n",
    "- \"Hagrid tells Harry he's a wizard\"\n",
    "\n",
    "Now, generate {num_queries} short user queries for the following chunk:\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk Text: \"{chunk_text}\"\n",
    "Queries:\n",
    "- \n",
    "\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    text = response.text.strip()\n",
    "    queries = [q.strip(\"- \").strip() for q in text.split(\"\\n\") if q.strip()]\n",
    "    return queries[:num_queries]\n",
    "\n",
    "# -------------------- Main workflow --------------------\n",
    "def main():\n",
    "    # Initialize persistent ChromaDB client\n",
    "    client_db = chromadb.PersistentClient(path=ABSOLUTE_DB_PATH)\n",
    "    print(f\"[INFO] ChromaDB client initialized at: {ABSOLUTE_DB_PATH}\")\n",
    "\n",
    "    # Access the existing collection directly\n",
    "    collection = client_db.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"[INFO] Using existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    # Fetch all documents and their metadata\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    chunks = [\n",
    "        {\"id\": meta[\"id\"], \"text\": doc}  # Use ID stored in metadata\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])\n",
    "        if meta.get(\"ischunk\") is True\n",
    "    ]\n",
    "    print(f\"[INFO] Found {len(chunks)} chunks (ischunk=True)\")\n",
    "\n",
    "    # Initialize Gemini client\n",
    "    gemini = init_gemini_client(API_KEY)\n",
    "\n",
    "    all_pairs = []\n",
    "    total_requests_today = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunk-batches\"):\n",
    "        batch = chunks[i : i + BATCH_SIZE]\n",
    "\n",
    "        for chunk in batch:\n",
    "            if total_requests_today >= MAX_RPD:\n",
    "                print(f\"[INFO] Reached daily limit of {MAX_RPD} requests. Stopping.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                queries = ask_gemini_generate_queries(\n",
    "                    gemini, chunk[\"text\"], chunk[\"id\"], NUM_QUERIES_PER_CHUNK\n",
    "                )\n",
    "                total_requests_today += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to generate for chunk {chunk['id']}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for q in queries:\n",
    "                all_pairs.append({\"query\": q, \"chunk_id\": chunk[\"id\"]})\n",
    "\n",
    "            # Sleep to respect RPM\n",
    "            time.sleep(SECONDS_PER_REQUEST)\n",
    "\n",
    "        # Optional: extra pause between batches\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[INFO] Saved {len(df)} query-chunk pairs to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65632c0a",
   "metadata": {},
   "source": [
    "2nd Attempt => Gemini => Round Robin fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Metadatas\n",
    "from google import genai\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "ABSOLUTE_DB_PATH = \"../VectorDB/chroma_Data\"\n",
    "COLLECTION_NAME = \"harry_potter_collection\"\n",
    "\n",
    "# ***** 5 API KEYS FOR ROUND-ROBIN *****\n",
    "API_KEYS = [\n",
    "    \"API_KEY_1\",\n",
    "    \"API_KEY_2\",\n",
    "    \"API_KEY_3\",\n",
    "    \"API_KEY_4\",\n",
    "    \"API_KEY_5\"\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 5                 # Smaller batch to respect RPM\n",
    "NUM_QUERIES_PER_CHUNK = 5\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "OUTPUT_CSV = \"generated_pairs.csv\"\n",
    "\n",
    "# Gemini 2.0 Flash rate limits\n",
    "MAX_RPM = 15                   # Requests per minute\n",
    "MAX_RPD = 200                  # Requests per day\n",
    "\n",
    "# Derived sleep time between requests in seconds\n",
    "SECONDS_PER_REQUEST = 60 / MAX_RPM\n",
    "\n",
    "# -------------------- Gemini Utilities --------------------\n",
    "def init_gemini_client(api_key: str):\n",
    "    os.environ[\"GENAI_API_KEY\"] = api_key\n",
    "    return genai.Client(api_key=api_key)\n",
    "\n",
    "# Round-robin state\n",
    "rr_index = 0\n",
    "\n",
    "def get_next_client():\n",
    "    \"\"\"\n",
    "    Returns a Gemini client using the next API key in round robin fashion.\n",
    "    \"\"\"\n",
    "    global rr_index\n",
    "    key = API_KEYS[rr_index]\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return init_gemini_client(key)\n",
    "\n",
    "def ask_gemini_generate_queries(client, chunk_text: str, chunk_id: str, num_queries: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ask Gemini to generate user-style queries for a given chunk.\n",
    "    Includes few-shot examples to ensure short, natural queries.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an AI that generates realistic search queries a user might input to an LLM or search system.\n",
    "Each query should be short, relevant, and reflect what someone might actually ask.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Chunk: \"Harry receives his first letter from Hogwarts, but Uncle Vernon tries to stop him.\"\n",
    "Queries:\n",
    "- \"How did Harry get his Hogwarts letter?\"\n",
    "- \"Why did Uncle Vernon hide Harry's letter?\"\n",
    "- \"First Hogwarts letter incident\"\n",
    "\n",
    "Example 2:\n",
    "Chunk: \"Hagrid visits Harry to explain that he is a wizard.\"\n",
    "Queries:\n",
    "- \"Who is Hagrid and why did he visit Harry?\"\n",
    "- \"How did Harry find out he is a wizard?\"\n",
    "- \"Hagrid tells Harry he's a wizard\"\n",
    "\n",
    "Now, generate {num_queries} short user queries for the following chunk:\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk Text: \"{chunk_text}\"\n",
    "Queries:\n",
    "- \n",
    "\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    text = response.text.strip()\n",
    "    queries = [q.strip(\"- \").strip() for q in text.split(\"\\n\") if q.strip()]\n",
    "    return queries[:num_queries]\n",
    "\n",
    "# -------------------- Main workflow --------------------\n",
    "def main():\n",
    "    # Initialize persistent ChromaDB client\n",
    "    client_db = chromadb.PersistentClient(path=ABSOLUTE_DB_PATH)\n",
    "    print(f\"[INFO] ChromaDB client initialized at: {ABSOLUTE_DB_PATH}\")\n",
    "\n",
    "    # Access the existing collection directly\n",
    "    collection = client_db.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"[INFO] Using existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    # Fetch all documents and their metadata\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    chunks = [\n",
    "        {\"id\": meta[\"id\"], \"text\": doc}  # Use ID stored in metadata\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])\n",
    "        if meta.get(\"ischunk\") is True\n",
    "    ]\n",
    "    print(f\"[INFO] Found {len(chunks)} chunks (ischunk=True)\")\n",
    "\n",
    "    all_pairs = []\n",
    "    total_requests_today = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunk-batches\"):\n",
    "        batch = chunks[i : i + BATCH_SIZE]\n",
    "\n",
    "        for chunk in batch:\n",
    "            if total_requests_today >= MAX_RPD:\n",
    "                print(f\"[INFO] Reached daily limit of {MAX_RPD} requests. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # ***** Get next Gemini client using round robin *****\n",
    "            gemini = get_next_client()\n",
    "\n",
    "            try:\n",
    "                queries = ask_gemini_generate_queries(\n",
    "                    gemini, chunk[\"text\"], chunk[\"id\"], NUM_QUERIES_PER_CHUNK\n",
    "                )\n",
    "                total_requests_today += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to generate for chunk {chunk['id']}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for q in queries:\n",
    "                all_pairs.append({\"query\": q, \"chunk_id\": chunk[\"id\"]})\n",
    "\n",
    "            # Sleep to respect RPM\n",
    "            time.sleep(SECONDS_PER_REQUEST)\n",
    "\n",
    "        # Optional: extra pause between batches\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[INFO] Saved {len(df)} query-chunk pairs to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e5a65",
   "metadata": {},
   "source": [
    "3rd attempt => Groq => Round Robin Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8861491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Metadatas\n",
    "from groq import Groq\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "ABSOLUTE_DB_PATH = \"../VectorDB/chroma_Data\"\n",
    "COLLECTION_NAME = \"harry_potter_collection\"\n",
    "\n",
    "# ***** 5 API KEYS FOR ROUND-ROBIN (GROQ KEYS) *****\n",
    "API_KEYS = [\n",
    "    \"GROQ_KEY_1\",\n",
    "    \"GROQ_KEY_2\",\n",
    "    \"GROQ_KEY_3\",\n",
    "    \"GROQ_KEY_4\",\n",
    "    \"GROQ_KEY_5\"\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_QUERIES_PER_CHUNK = 5\n",
    "\n",
    "# You can use any Groq-supported model; let's use the fast LLaMA model\n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
    "\n",
    "OUTPUT_CSV = \"generated_pairs.csv\"\n",
    "\n",
    "# Groq rate limits (adjust manually to stay safe)\n",
    "MAX_RPM = 15\n",
    "MAX_RPD = 200\n",
    "\n",
    "SECONDS_PER_REQUEST = 60 / MAX_RPM\n",
    "\n",
    "# -------------------- Groq Utilities --------------------\n",
    "def init_groq_client(api_key: str):\n",
    "    return Groq(api_key=api_key)\n",
    "\n",
    "# Round-robin state\n",
    "rr_index = 0\n",
    "\n",
    "def get_next_client():\n",
    "    \"\"\"\n",
    "    Returns a Groq client using the next API key in round robin fashion.\n",
    "    \"\"\"\n",
    "    global rr_index\n",
    "    key = API_KEYS[rr_index]\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return init_groq_client(key)\n",
    "\n",
    "def ask_groq_generate_queries(client, chunk_text: str, chunk_id: str, num_queries: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ask Groq LLM to generate user-style queries for a given chunk.\n",
    "    Few-shot prompt unchanged.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an AI that generates realistic search queries a user might input to an LLM or search system.\n",
    "Each query should be short, relevant, and reflect what someone might actually ask.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Chunk: \"Harry receives his first letter from Hogwarts, but Uncle Vernon tries to stop him.\"\n",
    "Queries:\n",
    "- \"How did Harry get his Hogwarts letter?\"\n",
    "- \"Why did Uncle Vernon hide Harry's letter?\"\n",
    "- \"First Hogwarts letter incident\"\n",
    "\n",
    "Example 2:\n",
    "Chunk: \"Hagrid visits Harry to explain that he is a wizard.\"\n",
    "Queries:\n",
    "- \"Who is Hagrid and why did he visit Harry?\"\n",
    "- \"How did Harry find out he is a wizard?\"\n",
    "- \"Hagrid tells Harry he's a wizard\"\n",
    "\n",
    "Now, generate {num_queries} short user queries for the following chunk:\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk Text: \"{chunk_text}\"\n",
    "Queries:\n",
    "-\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=GROQ_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    queries = [q.strip(\"- \").strip() for q in text.split(\"\\n\") if q.strip()]\n",
    "    return queries[:num_queries]\n",
    "\n",
    "# -------------------- Main workflow --------------------\n",
    "def main():\n",
    "    # Initialize persistent ChromaDB client\n",
    "    client_db = chromadb.PersistentClient(path=ABSOLUTE_DB_PATH)\n",
    "    print(f\"[INFO] ChromaDB client initialized at: {ABSOLUTE_DB_PATH}\")\n",
    "\n",
    "    # Access the existing collection directly\n",
    "    collection = client_db.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"[INFO] Using existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    # Fetch all documents and their metadata\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    chunks = [\n",
    "        {\"id\": meta[\"id\"], \"text\": doc}\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])\n",
    "        if meta.get(\"ischunk\") is True\n",
    "    ]\n",
    "    print(f\"[INFO] Found {len(chunks)} chunks (ischunk=True)\")\n",
    "\n",
    "    all_pairs = []\n",
    "    total_requests_today = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunk-batches\"):\n",
    "        batch = chunks[i : i + BATCH_SIZE]\n",
    "\n",
    "        for chunk in batch:\n",
    "            if total_requests_today >= MAX_RPD:\n",
    "                print(f\"[INFO] Reached daily limit of {MAX_RPD} requests. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # ***** Get next Groq client using round robin *****\n",
    "            groq_client = get_next_client()\n",
    "\n",
    "            try:\n",
    "                queries = ask_groq_generate_queries(\n",
    "                    groq_client, chunk[\"text\"], chunk[\"id\"], NUM_QUERIES_PER_CHUNK\n",
    "                )\n",
    "                total_requests_today += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to generate for chunk {chunk['id']}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for q in queries:\n",
    "                all_pairs.append({\"query\": q, \"chunk_id\": chunk[\"id\"]})\n",
    "\n",
    "            # Sleep to respect RPM\n",
    "            time.sleep(SECONDS_PER_REQUEST)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[INFO] Saved {len(df)} query-chunk pairs to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
