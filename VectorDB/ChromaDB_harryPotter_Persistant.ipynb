{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"harry_potter_collection\"\n",
    "# Set API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\micro\\Desktop\\Abhinav college\\Resources\\Sem 7\\Advanced NLP\\RAG_for_research_papers\\VectorDB\\chroma_Data\n",
      "Existing collections: ['harry_potter_collection', 'my_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3382 chunks from '../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_1_section_0', 'parent_id': 'page_1', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized with model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "# !pip install sentence_transformers\n",
    "\n",
    "# Set up embedding function\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding function initialized with model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'harry_potter_collection' created or accessed successfully\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Get or create the collection\n",
    "client.delete_collection(name=collection_name)  \n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter book chunks\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created or accessed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch: 0 to 499 (500 items)\n",
      "Added batch: 500 to 999 (500 items)\n",
      "Added batch: 500 to 999 (500 items)\n",
      "Added batch: 1000 to 1499 (500 items)\n",
      "Added batch: 1000 to 1499 (500 items)\n",
      "Added batch: 1500 to 1999 (500 items)\n",
      "Added batch: 1500 to 1999 (500 items)\n",
      "Added batch: 2000 to 2499 (500 items)\n",
      "Added batch: 2000 to 2499 (500 items)\n",
      "Added batch: 2500 to 2999 (500 items)\n",
      "Added batch: 2500 to 2999 (500 items)\n",
      "Added batch: 3000 to 3381 (382 items)\n",
      "Successfully added 3382 documents to collection 'harry_potter_collection'\n",
      "Added batch: 3000 to 3381 (382 items)\n",
      "Successfully added 3382 documents to collection 'harry_potter_collection'\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a unique ID (you could use a more deterministic approach if needed)\n",
    "    doc_id = f\"hp_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # collection.update(\n",
    "    #     ids=ids[i:end_idx],\n",
    "    #     documents=documents[i:end_idx],\n",
    "    #     metadatas=metadatas[i:end_idx]\n",
    "    # )\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"Added batch: {i} to {end_idx-1} ({end_idx-i} items)\")\n",
    "\n",
    "print(f\"Successfully added {total_added} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 3382\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: hp_chunk_0\n",
      "Text: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to ...\n",
      "Metadata: {'chunk_type': 'section', 'parent_id': 'page_1', 'section_id': 'page_1_section_0', 'chunk_index': 0, 'chunk_level': 1, 'source': '../harrypotter.pdf', 'page_number': 1}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: hp_chunk_1\n",
      "Text: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to ...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'chunk_index': 0, 'chunk_level': 2, 'page_number': 1, 'chunk_type': 'paragraph', 'section_id': 'page_1_section_0', 'paragraph_id': 'page_1_section_0_para_0', 'parent_id': 'page_1_section_0'}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: hp_chunk_2\n",
      "Text: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to ...\n",
      "Metadata: {'sentence_id': 'page_1_section_0_para_0_sent_0', 'chunk_index': 0, 'section_id': 'page_1_section_0', 'source': '../harrypotter.pdf', 'chunk_type': 'sentence', 'paragraph_id': 'page_1_section_0_para_0', 'chunk_level': 3, 'parent_id': 'page_1_section_0_para_0', 'page_number': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5820e",
   "metadata": {},
   "source": [
    "### Querying the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88654e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich table for displaying results (optional but nice)\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    console = Console()\n",
    "    use_rich = True\n",
    "except ImportError:\n",
    "    use_rich = False\n",
    "    print(\"Rich package not found. Using standard print.\")\n",
    "\n",
    "# Function to display query results\n",
    "def print_results(results, use_rich=use_rich):\n",
    "    if use_rich:\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Rank\", width=6)\n",
    "        table.add_column(\"Document ID\")\n",
    "        table.add_column(\"Document Text\", width=60)\n",
    "        table.add_column(\"Page\")\n",
    "        table.add_column(\"Distance\")\n",
    "        \n",
    "        docs = results['documents'][0]\n",
    "        ids = results['ids'][0]\n",
    "        metas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        for i, (doc, doc_id, meta, dist) in enumerate(zip(docs, ids, metas, distances)):\n",
    "            table.add_row(\n",
    "                str(i+1),\n",
    "                doc_id,\n",
    "                (doc[:100] + \"...\") if len(doc) > 100 else doc,\n",
    "                str(meta.get('page_number', 'N/A')),\n",
    "                f\"{dist:.4f}\"\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    else:\n",
    "        # Standard print version\n",
    "        for i, (doc, meta, dist) in enumerate(zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            print(f\"\\n--- Result {i+1} ---\")\n",
    "            print(f\"Text: {doc[:100]}...\")\n",
    "            print(f\"Metadata: {meta}\")\n",
    "            print(f\"Distance: {dist:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09ae3ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: 'Who was Dumbledore? When was he first introduced?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Rank   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document ID  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document Text                                                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Page </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Distance </span>┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│ 1      │ hp_chunk_148 │ . “Really, Dumbledore, you think you can explain all this in │ 11   │ 0.4705   │\n",
       "│        │              │ a letter?                                                    │      │          │\n",
       "│        │              │ These people will never under...                             │      │          │\n",
       "│ 2      │ hp_chunk_92  │ . This man’s                                                 │ 7    │ 0.4843   │\n",
       "│        │              │ name was Albus Dumbledore.                                   │      │          │\n",
       "│        │              │ Albus Dumbledore didn’t seem to realize that he had just     │      │          │\n",
       "│        │              │ arr...                                                       │      │          │\n",
       "│ 3      │ hp_chunk_120 │ ? All this ‘You-Know-Who’ nonsense — for eleven years I have │ 9    │ 0.4851   │\n",
       "│        │              │ been trying to persuade people to call ...                   │      │          │\n",
       "└────────┴──────────────┴──────────────────────────────────────────────────────────────┴──────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mRank  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument ID \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument Text                                               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mPage\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDistance\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│ 1      │ hp_chunk_148 │ . “Really, Dumbledore, you think you can explain all this in │ 11   │ 0.4705   │\n",
       "│        │              │ a letter?                                                    │      │          │\n",
       "│        │              │ These people will never under...                             │      │          │\n",
       "│ 2      │ hp_chunk_92  │ . This man’s                                                 │ 7    │ 0.4843   │\n",
       "│        │              │ name was Albus Dumbledore.                                   │      │          │\n",
       "│        │              │ Albus Dumbledore didn’t seem to realize that he had just     │      │          │\n",
       "│        │              │ arr...                                                       │      │          │\n",
       "│ 3      │ hp_chunk_120 │ ? All this ‘You-Know-Who’ nonsense — for eleven years I have │ 9    │ 0.4851   │\n",
       "│        │              │ been trying to persuade people to call ...                   │      │          │\n",
       "└────────┴──────────────┴──────────────────────────────────────────────────────────────┴──────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run a query\n",
    "query = \"Who was Dumbledore? When was he first introduced?\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nResults for query: '{query}'\")\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62da64",
   "metadata": {},
   "source": [
    "### Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "041e750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (2.0.10)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (2.181.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-google-genai) (0.3.75)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.4.23)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.30.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.24.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Requirement already satisfied: anyio in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "261b1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Initialize Gemini (fixed the model name - using a valid Gemini model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cbee757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Better prompt\n",
    "rag_prompt_template = \"\"\"\n",
    "You are an expert on Harry Potter books. Answer questions using ONLY the context below.\n",
    "If you can't find a complete answer in the context but see partial information, try to provide what you can find and acknowledge the limitations of the available information.\n",
    "If there is NO relevant information at all in the context, respond with \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context provided):\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "310d33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from rank_bm25) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6286aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def answer_with_hybrid_rag(query, n_results=5):\n",
    "    # 1. Semantic search with ChromaDB\n",
    "    semantic_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Perform keyword search with BM25\n",
    "    # First get all documents to search across\n",
    "    all_docs = collection.get(\n",
    "        limit=100,  # Adjust based on your collection size\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # Tokenize for BM25\n",
    "    tokenized_docs = [doc.split() for doc in all_docs[\"documents\"]]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top BM25 results\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[-n_results:][::-1]\n",
    "    \n",
    "    # 3. Combine results (simple union)\n",
    "    combined_docs = []\n",
    "    combined_meta = []\n",
    "    combined_ids = [] \n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Add semantic results\n",
    "    for doc, meta, doc_id in zip(\n",
    "        semantic_results[\"documents\"][0], \n",
    "        semantic_results[\"metadatas\"][0],\n",
    "        semantic_results[\"ids\"][0]\n",
    "    ):\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(doc)\n",
    "            combined_meta.append(meta)\n",
    "            combined_ids.append(doc_id)  # Store the id\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Add keyword results\n",
    "    for idx in top_bm25_indices:\n",
    "        doc_id = all_docs[\"ids\"][idx]\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(all_docs[\"documents\"][idx])\n",
    "            combined_meta.append(all_docs[\"metadatas\"][idx])\n",
    "            combined_ids.append(doc_id)  # Store the id\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Limit to n_results total\n",
    "    combined_docs = combined_docs[:n_results]\n",
    "    combined_meta = combined_meta[:n_results]\n",
    "    combined_ids = combined_ids[:n_results]\n",
    "    \n",
    "    # Format context and complete RAG as before\n",
    "    formatted_docs = []\n",
    "    for doc, meta in zip(combined_docs, combined_meta):\n",
    "        page_num = meta.get(\"page_number\", \"unknown\")\n",
    "        formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "    filled_prompt = prompt.format(context=context, query=query)\n",
    "    response = llm.invoke(filled_prompt)\n",
    "    \n",
    "    # Create a mock results object for print_results compatibility\n",
    "    mock_results = {\n",
    "        \"documents\": [combined_docs],\n",
    "        \"metadatas\": [combined_meta],\n",
    "        \"distances\": [[0.0] * len(combined_docs)],  # Placeholder distances\n",
    "        \"ids\": [combined_ids]  # Add this line\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content if hasattr(response, 'content') else str(response),\n",
    "        \"source_documents\": mock_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c0841a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:381643304294'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"asia-east1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"0\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/381643304294\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:381643304294'. [reason: \"RATE_LIMIT_EXCEEDED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"quota_unit\"\n  value: \"1/min/{project}/{region}\"\n}\nmetadata {\n  key: \"quota_metric\"\n  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n}\nmetadata {\n  key: \"quota_location\"\n  value: \"asia-east1\"\n}\nmetadata {\n  key: \"quota_limit\"\n  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n}\nmetadata {\n  key: \"quota_limit_value\"\n  value: \"0\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/381643304294\"\n}\n, links {\n  description: \"Request a higher quota limit.\"\n  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test our RAG pipeline with a question\u001b[39;00m\n\u001b[32m      2\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mWhat happened when Harry first met Hagrid?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43manswer_with_hybrid_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36manswer_with_hybrid_rag\u001b[39m\u001b[34m(query, n_results)\u001b[39m\n\u001b[32m     68\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(formatted_docs)\n\u001b[32m     69\u001b[39m filled_prompt = prompt.format(context=context, query=query)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Create a mock results object for print_results compatibility\u001b[39;00m\n\u001b[32m     73\u001b[39m mock_results = {\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: [combined_docs],\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m: [combined_meta],\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m: [[\u001b[32m0.0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(combined_docs)],  \u001b[38;5;66;03m# Placeholder distances\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m: [combined_ids]  \u001b[38;5;66;03m# Add this line\u001b[39;00m\n\u001b[32m     78\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:381643304294'. [reason: \"RATE_LIMIT_EXCEEDED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"quota_unit\"\n  value: \"1/min/{project}/{region}\"\n}\nmetadata {\n  key: \"quota_metric\"\n  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n}\nmetadata {\n  key: \"quota_location\"\n  value: \"asia-east1\"\n}\nmetadata {\n  key: \"quota_limit\"\n  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n}\nmetadata {\n  key: \"quota_limit_value\"\n  value: \"0\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/381643304294\"\n}\n, links {\n  description: \"Request a higher quota limit.\"\n  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n}\n]"
     ]
    }
   ],
   "source": [
    "# Test our RAG pipeline with a question\n",
    "test_query = \"What happened when Harry first met Hagrid?\"\n",
    "response = answer_with_hybrid_rag(test_query)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nAnswer: {response['answer']}\")\n",
    "print(\"\\nSources:\")\n",
    "print_results(response[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45035af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question: Who is Voldemort and why is he feared?\n",
      "\n",
      "Answer: Voldemort is a powerful wizard who started gaining followers about twenty years ago, causing \"Dark days\" where people didn't know who to trust or dare to get friendly with strange wizards or witches. He is referred to as \"Lord Voldemort\" by his follower Quirrell, who considers him a \"great wizard\" and his \"master.\" Voldemort believes \"there is no good and evil, there is only power, and those too weak to seek it.\" He had powers Dumbledore says he will never have, and Dumbledore is described as the \"only one Voldemort was frightened of.\" He is still \"out there somewhere, perhaps looking for another body to share.\"\n",
      "\n",
      "Voldemort is feared because of the \"dark days\" he caused. People are so afraid of him that they often avoid saying his name, referring to him as \"You-Know-Who,\" as Dumbledore explains that \"Fear of a name increases fear of the thing itself.\" Hagrid gulps and shudders at the mention of his name, and Professor McGonagall flinches. Harry also starts to feel \"a prickle of fear every time You-Know-Who was mentioned.\" People get scared \"in case You-Know-Who’s behind\" bad events. Hagrid describes him as \"bad,\" \"as bad as you could go,\" \"Worse. Worse than worse.\"\n",
      "\n",
      "Top source:\n",
      "[Page 46]:\n",
      ". bad. As bad as you could go. Worse. Worse than worse. His name was . . .”\n",
      "Hagrid gulped, but no words came out. “Could you write it down?” Harry suggested. “Nah — can’t spell it. All right — Voldemort.” Hagrid shuddered. “Don’\n",
      "make me say it again. Anyway, this — this wizard, about twenty years ago\n",
      "now, started lookin’ fer followers. Got ’em, too — some were afraid, some\n",
      "just wanted a bit o’ his power, ’cause he was gettin’ himself power, all right. Dark days, Harry. Didn’t know who ter trust, didn’t dare get friendly with\n",
      "strange wizards or witches . .\n",
      "\n",
      "==================================================\n",
      "Question: What are the four houses at Hogwarts?\n",
      "\n",
      "Answer: Voldemort is a powerful wizard who started gaining followers about twenty years ago, causing \"Dark days\" where people didn't know who to trust or dare to get friendly with strange wizards or witches. He is referred to as \"Lord Voldemort\" by his follower Quirrell, who considers him a \"great wizard\" and his \"master.\" Voldemort believes \"there is no good and evil, there is only power, and those too weak to seek it.\" He had powers Dumbledore says he will never have, and Dumbledore is described as the \"only one Voldemort was frightened of.\" He is still \"out there somewhere, perhaps looking for another body to share.\"\n",
      "\n",
      "Voldemort is feared because of the \"dark days\" he caused. People are so afraid of him that they often avoid saying his name, referring to him as \"You-Know-Who,\" as Dumbledore explains that \"Fear of a name increases fear of the thing itself.\" Hagrid gulps and shudders at the mention of his name, and Professor McGonagall flinches. Harry also starts to feel \"a prickle of fear every time You-Know-Who was mentioned.\" People get scared \"in case You-Know-Who’s behind\" bad events. Hagrid describes him as \"bad,\" \"as bad as you could go,\" \"Worse. Worse than worse.\"\n",
      "\n",
      "Top source:\n",
      "[Page 46]:\n",
      ". bad. As bad as you could go. Worse. Worse than worse. His name was . . .”\n",
      "Hagrid gulped, but no words came out. “Could you write it down?” Harry suggested. “Nah — can’t spell it. All right — Voldemort.” Hagrid shuddered. “Don’\n",
      "make me say it again. Anyway, this — this wizard, about twenty years ago\n",
      "now, started lookin’ fer followers. Got ’em, too — some were afraid, some\n",
      "just wanted a bit o’ his power, ’cause he was gettin’ himself power, all right. Dark days, Harry. Didn’t know who ter trust, didn’t dare get friendly with\n",
      "strange wizards or witches . .\n",
      "\n",
      "==================================================\n",
      "Question: What are the four houses at Hogwarts?\n",
      "\n",
      "Answer: The four Houses at Hogwarts are Gryffindor, Hufflepuff, Ravenclaw, and Slytherin.\n",
      "\n",
      "Top source:\n",
      "[Page 97]:\n",
      "ceremony because, while you are here, your House will be something like\n",
      "your family within Hogwarts. You will have classes with the rest of your\n",
      "House, sleep in your House dormitory, and spend free time in your House\n",
      "common room. “The four Houses are called Gryffindor, Hufflepuff, Ravenclaw, and\n",
      "Slytherin. Each House has its own noble history and each has produced\n",
      "outstanding witches and wizards. While you are at Hogwarts, your triumphs\n",
      "will earn your House points, while any rule-breaking will lose House points. At the end of the year, the House with the most points is awarded the House\n",
      "Cup, a great honor. I hope each of you will be a credit to whichever House\n",
      "becomes yours. “The Sorting Ceremony will take place in a few minutes in front of the rest\n",
      "of the school. I suggest you all smarten yourselves up as much as you can\n",
      "while you are waiting.”\n",
      "Her eyes lingered for a moment on Neville’s cloak, which was fastened\n",
      "under his left ear, and on Ron’s smudged nose. Harry nervously tried to\n",
      "flatten his hair. “I shall return when we are ready for you,” said Professor McGonagall. “Please wait quietly.”\n",
      "She left the chamber.\n",
      "\n",
      "==================================================\n",
      "Question: How did Harry survive the killing curse as a baby?\n",
      "\n",
      "Answer: The four Houses at Hogwarts are Gryffindor, Hufflepuff, Ravenclaw, and Slytherin.\n",
      "\n",
      "Top source:\n",
      "[Page 97]:\n",
      "ceremony because, while you are here, your House will be something like\n",
      "your family within Hogwarts. You will have classes with the rest of your\n",
      "House, sleep in your House dormitory, and spend free time in your House\n",
      "common room. “The four Houses are called Gryffindor, Hufflepuff, Ravenclaw, and\n",
      "Slytherin. Each House has its own noble history and each has produced\n",
      "outstanding witches and wizards. While you are at Hogwarts, your triumphs\n",
      "will earn your House points, while any rule-breaking will lose House points. At the end of the year, the House with the most points is awarded the House\n",
      "Cup, a great honor. I hope each of you will be a credit to whichever House\n",
      "becomes yours. “The Sorting Ceremony will take place in a few minutes in front of the rest\n",
      "of the school. I suggest you all smarten yourselves up as much as you can\n",
      "while you are waiting.”\n",
      "Her eyes lingered for a moment on Neville’s cloak, which was fastened\n",
      "under his left ear, and on Ron’s smudged nose. Harry nervously tried to\n",
      "flatten his hair. “I shall return when we are ready for you,” said Professor McGonagall. “Please wait quietly.”\n",
      "She left the chamber.\n",
      "\n",
      "==================================================\n",
      "Question: How did Harry survive the killing curse as a baby?\n",
      "\n",
      "Answer: The context states that \"You-Know-Who\" (Voldemort) tried to kill Harry as a baby with a powerful, evil curse, but \"he couldn’t do it.\" The curse \"took care of yer mum an’ dad an’ yer house, even — but it didn’t work on you, an’ that’s why yer famous, Harry. No one ever lived after he decided ter kill ’em, no one except you, an’ he’d killed some o’ the best witches an’ wizards of the age — the McKinnons, the Bones, the Prewetts — an’ you was only a baby, an’ you lived.” The mark on Harry's forehead is what he got when this curse touched him.\n",
      "\n",
      "However, the provided context does not explain the specific reason or mechanism by which Harry survived the killing curse. It only states that the curse \"didn't work on him.\"\n",
      "\n",
      "Top source:\n",
      "[Page 47]:\n",
      ". “You-Know-Who killed ’em. An’ then — an’ this is the real myst’ry of the\n",
      "thing — he tried to kill you, too. Wanted ter make a clean job of it, I suppose,\n",
      "or maybe he just liked killin’ by then. But he couldn’t do it. Never wondered\n",
      "how you got that mark on yer forehead? That was no ordinary cut. That’s\n",
      "what yeh get when a powerful, evil curse touches yeh — took care of yer\n",
      "mum an’ dad an’ yer house, even — but it didn’t work on you, an’ that’s why\n",
      "yer famous, Harry. No one ever lived after he decided ter kill ’em, no one\n",
      "except you, an’ he’d killed some o’ the best witches an’ wizards of the age —\n",
      "the McKinnons, the Bones, the Prewetts — an’ you was only a baby, an’ you\n",
      "lived.”\n",
      "Something very painful was going on in Harry’s mind. As Hagrid’s story\n",
      "came to a close, he saw again the blinding flash of green light, more clearly\n",
      "than he had ever remembered it before — and he remembered something else,\n",
      "for the first time in his life: a high, cold, cruel laugh. Hagrid was watching him sadly. “Took yeh from the ruined house myself, on Dumbledore’s orders. Brought\n",
      "yeh ter this lot . . .”\n",
      "“Load of old tosh,” said Uncle Vernon. Harry jumped; he had almost\n",
      "forgotten that the Dursleys were there. Uncle Vernon certainly seemed to have\n",
      "got back his courage. He was glaring at Hagrid and his fists were clenched. “Now, you listen here, boy,” he snarled, “I accept there’s something strange\n",
      "\n",
      "\n",
      "Results exported to multiquery_rag_results.txt\n",
      "\n",
      "Answer: The context states that \"You-Know-Who\" (Voldemort) tried to kill Harry as a baby with a powerful, evil curse, but \"he couldn’t do it.\" The curse \"took care of yer mum an’ dad an’ yer house, even — but it didn’t work on you, an’ that’s why yer famous, Harry. No one ever lived after he decided ter kill ’em, no one except you, an’ he’d killed some o’ the best witches an’ wizards of the age — the McKinnons, the Bones, the Prewetts — an’ you was only a baby, an’ you lived.” The mark on Harry's forehead is what he got when this curse touched him.\n",
      "\n",
      "However, the provided context does not explain the specific reason or mechanism by which Harry survived the killing curse. It only states that the curse \"didn't work on him.\"\n",
      "\n",
      "Top source:\n",
      "[Page 47]:\n",
      ". “You-Know-Who killed ’em. An’ then — an’ this is the real myst’ry of the\n",
      "thing — he tried to kill you, too. Wanted ter make a clean job of it, I suppose,\n",
      "or maybe he just liked killin’ by then. But he couldn’t do it. Never wondered\n",
      "how you got that mark on yer forehead? That was no ordinary cut. That’s\n",
      "what yeh get when a powerful, evil curse touches yeh — took care of yer\n",
      "mum an’ dad an’ yer house, even — but it didn’t work on you, an’ that’s why\n",
      "yer famous, Harry. No one ever lived after he decided ter kill ’em, no one\n",
      "except you, an’ he’d killed some o’ the best witches an’ wizards of the age —\n",
      "the McKinnons, the Bones, the Prewetts — an’ you was only a baby, an’ you\n",
      "lived.”\n",
      "Something very painful was going on in Harry’s mind. As Hagrid’s story\n",
      "came to a close, he saw again the blinding flash of green light, more clearly\n",
      "than he had ever remembered it before — and he remembered something else,\n",
      "for the first time in his life: a high, cold, cruel laugh. Hagrid was watching him sadly. “Took yeh from the ruined house myself, on Dumbledore’s orders. Brought\n",
      "yeh ter this lot . . .”\n",
      "“Load of old tosh,” said Uncle Vernon. Harry jumped; he had almost\n",
      "forgotten that the Dursleys were there. Uncle Vernon certainly seemed to have\n",
      "got back his courage. He was glaring at Hagrid and his fists were clenched. “Now, you listen here, boy,” he snarled, “I accept there’s something strange\n",
      "\n",
      "\n",
      "Results exported to multiquery_rag_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions to evaluate system\n",
    "results_for_export = []\n",
    "\n",
    "test_questions = [\n",
    "    \"Who is Voldemort and why is he feared?\",\n",
    "    \"What are the four houses at Hogwarts?\",\n",
    "    \"How did Harry survive the killing curse as a baby?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Question: {question}\")\n",
    "    response = answer_with_hybrid_rag(question)\n",
    "    print(f\"\\nAnswer: {response['answer']}\")\n",
    "    print(\"\\nTop source:\")\n",
    "    if len(response[\"source_documents\"][\"documents\"][0]) > 0:\n",
    "        top_doc = response[\"source_documents\"][\"documents\"][0][0]\n",
    "        top_meta = response[\"source_documents\"][\"metadatas\"][0][0]\n",
    "        page = top_meta.get(\"page_number\", \"N/A\")\n",
    "        print(f\"[Page {page}]:\\n{top_doc}\")  # Print full chunk\n",
    "        # Save for export\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": page,\n",
    "            \"chunk\": top_doc\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sources found.\")\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": None,\n",
    "            \"chunk\": None\n",
    "        })\n",
    "\n",
    "# Export results to a well-formatted text file\n",
    "with open(multiquery_rag_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"RAG Multi-Query Evaluation Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    for idx, res in enumerate(results_for_export, 1):\n",
    "        f.write(f\"Question {idx}: {res['question']}\\n\")\n",
    "        f.write(f\"Answer:\\n{res['answer']}\\n\\n\")\n",
    "        if res[\"chunk\"]:\n",
    "            f.write(f\"Top Source Chunk (Page {res['page']}):\\n{res['chunk']}\\n\")\n",
    "        else:\n",
    "            f.write(\"Top Source Chunk: No sources found.\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\\n\")\n",
    "print(f\"\\nResults exported to {multiquery_rag_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ef32c",
   "metadata": {},
   "source": [
    "### Creating a User Interface with Gradio\n",
    "This will create a simple web interface for users to query the Harry Potter RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio for the web interface\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d82f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (5.44.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.12.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (1.12.1)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.34.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.12.12)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.47.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.17.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\micro\\anaconda3\\envs\\nlp_2\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Could not load custom encoder model: [Errno 2] No such file or directory: '../Encoder/model.pkl'\n",
      "Will use ChromaDB's default embedding function if custom model is selected.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load the custom encoder model\n",
    "custom_model_loaded = False\n",
    "custom_encoder = None\n",
    "try:\n",
    "    # Path to the encoder model\n",
    "    model_path = \"../Encoder/model.pkl\"  # Adjust path as needed\n",
    "\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        custom_encoder = pickle.load(f)\n",
    "    custom_model_loaded = True\n",
    "    print(f\"Successfully loaded custom encoder model from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load custom encoder model: {e}\")\n",
    "    print(\"Will use ChromaDB's default embedding function if custom model is selected.\")\n",
    "\n",
    "# Helper: get embedding using the custom encoder (robust)\n",
    "def get_embedding_with_custom_model(text):\n",
    "    \"\"\"Generate embedding using the custom encoder model\"\"\"\n",
    "    if custom_model_loaded and custom_encoder is not None:\n",
    "        try:\n",
    "            # Try common APIs: custom_encoder.encode([...]) or custom_encoder.transform([...])\n",
    "            if hasattr(custom_encoder, \"encode\"):\n",
    "                emb = custom_encoder.encode([text])\n",
    "            elif hasattr(custom_encoder, \"transform\"):\n",
    "                emb = custom_encoder.transform([text])\n",
    "            else:\n",
    "                # Try calling directly\n",
    "                emb = custom_encoder([text])\n",
    "            # emb might be shape (1, D) numpy or list\n",
    "            emb0 = emb[0]\n",
    "            # convert to plain python list of floats\n",
    "            if isinstance(emb0, np.ndarray):\n",
    "                return emb0.tolist()\n",
    "            else:\n",
    "                # convert nested types to floats\n",
    "                return [float(x) for x in emb0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error using custom encoder: {e}\")\n",
    "            # fallback to Chroma's embedding if available\n",
    "            try:\n",
    "                return embedding_function([text])[0]\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"Custom encoder failed and no embedding_function fallback available.\")\n",
    "    else:\n",
    "        # Fallback to ChromaDB's embedding function if available\n",
    "        try:\n",
    "            return embedding_function([text])[0]\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"No custom encoder loaded and embedding_function is not available.\")\n",
    "\n",
    "# Primary RAG pipeline function\n",
    "def rag_query(query, top_k=3, use_hybrid_search=False, use_custom_encoder=False):\n",
    "    \"\"\"\n",
    "    Process a query through the RAG pipeline and return:\n",
    "      - answer (str)\n",
    "      - source chunks (str)\n",
    "      - elapsed time in seconds (str)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # If using hybrid search, prefer your existing answer_with_hybrid_rag function\n",
    "        if use_hybrid_search:\n",
    "            # If hybrid uses the collection internally, assume it handles embedder choice.\n",
    "            # If you want hybrid to use the custom encoder, modify answer_with_hybrid_rag accordingly.\n",
    "            response = answer_with_hybrid_rag(query, n_results=int(top_k))\n",
    "            answer = response.get('answer', \"\")\n",
    "            source_docs = response.get('source_documents', {\"documents\": [[]], \"metadatas\": [[]]})\n",
    "            # For backward compatibility in formatting below, wrap appropriately if necessary\n",
    "            results_obj = None\n",
    "        else:\n",
    "            # vector search branch: if custom encoder chosen, compute query embedding locally\n",
    "            if use_custom_encoder and custom_model_loaded:\n",
    "                try:\n",
    "                    query_embedding = get_embedding_with_custom_model(query)\n",
    "                except Exception as e:\n",
    "                    # fallback: if embedding fails, fall back to text-based query\n",
    "                    print(f\"Custom encoder embedding failed: {e}. Falling back to Chroma text embedding.\")\n",
    "                    query_embedding = None\n",
    "\n",
    "                if query_embedding is not None:\n",
    "                    results = collection.query(\n",
    "                        query_embeddings=[query_embedding],\n",
    "                        n_results=int(top_k),\n",
    "                        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                    )\n",
    "                else:\n",
    "                    # fallback to server-side embedding\n",
    "                    results = collection.query(\n",
    "                        query_texts=[query],\n",
    "                        n_results=int(top_k),\n",
    "                        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                    )\n",
    "            else:\n",
    "                # Use ChromaDB's default embedding function (server-side)\n",
    "                results = collection.query(\n",
    "                    query_texts=[query],\n",
    "                    n_results=int(top_k),\n",
    "                    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                )\n",
    "\n",
    "            # Format context from retrieved chunks\n",
    "            documents = results[\"documents\"][0]\n",
    "            metadatas = results[\"metadatas\"][0]\n",
    "            distances = results.get(\"distances\", [[]])[0] if \"distances\" in results else [None] * len(documents)\n",
    "\n",
    "            # Format each document with its page number for better context\n",
    "            formatted_docs = []\n",
    "            for doc, meta, dist in zip(documents, metadatas, distances):\n",
    "                page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                if dist is not None:\n",
    "                    formatted_docs.append(f\"[Page {page_num}] (dist={dist:.4f}): {doc}\")\n",
    "                else:\n",
    "                    formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "\n",
    "            # join into context\n",
    "            context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "\n",
    "            # Fill the prompt template\n",
    "            try:\n",
    "                filled_prompt = prompt.format(context=context, query=query)\n",
    "            except Exception:\n",
    "                # fallback if prompt doesn't use named placeholders\n",
    "                filled_prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "            # Call the LLM\n",
    "            try:\n",
    "                response_obj = llm.invoke(filled_prompt)\n",
    "                answer = response_obj.content if hasattr(response_obj, 'content') else str(response_obj)\n",
    "            except Exception as e:\n",
    "                # If llm.invoke fails, surface the error\n",
    "                answer = f\"LLM invocation failed: {e}\"\n",
    "\n",
    "            source_docs = results\n",
    "\n",
    "        # Format source documents for display\n",
    "        sources_text = \"\"\n",
    "        # if source_docs is a dict produced by collection.query\n",
    "        if isinstance(source_docs, dict) and \"documents\" in source_docs and \"metadatas\" in source_docs:\n",
    "            docs_list = source_docs[\"documents\"][0]\n",
    "            meta_list = source_docs[\"metadatas\"][0]\n",
    "            for i, (doc, meta) in enumerate(zip(docs_list, meta_list)):\n",
    "                page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                sources_text += f\"\\n\\nSource {i+1} [Page {page_num}]:\\n{doc}\"\n",
    "        else:\n",
    "            # Try to format the results object if available\n",
    "            try:\n",
    "                docs_list = source_docs[\"documents\"][0]\n",
    "                meta_list = source_docs[\"metadatas\"][0]\n",
    "                for i, (doc, meta) in enumerate(zip(docs_list, meta_list)):\n",
    "                    page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                    sources_text += f\"\\n\\nSource {i+1} [Page {page_num}]:\\n{doc}\"\n",
    "            except Exception:\n",
    "                # fallback textual representation\n",
    "                sources_text = str(source_docs)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        elapsed_str = f\"{elapsed:.3f} seconds\"\n",
    "\n",
    "        return answer, sources_text, elapsed_str\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        elapsed_str = f\"{elapsed:.3f} seconds\"\n",
    "        return f\"Error during processing: {e}\", \"\", elapsed_str\n",
    "\n",
    "\n",
    "# Create Gradio Interface\n",
    "with gr.Blocks(title=\"Harry Potter RAG System\") as demo:\n",
    "    gr.Markdown(\"# 🧙‍♂️ Harry Potter RAG System\")\n",
    "    gr.Markdown(\"Ask questions about the Harry Potter books and get answers based on the text.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"E.g., Who is Harry Potter and what happened to his parents?\",\n",
    "                lines=2\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            top_k = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                value=3,\n",
    "                step=1,\n",
    "                label=\"Number of chunks to retrieve\"\n",
    "            )\n",
    "            hybrid_search = gr.Checkbox(\n",
    "                label=\"Use hybrid search\",\n",
    "                value=True,\n",
    "                info=\"If enabled, uses your hybrid function (answer_with_hybrid_rag).\"\n",
    "            )\n",
    "            custom_encoder_cb = gr.Checkbox(\n",
    "                label=\"Use custom encoder (model.pkl)\",\n",
    "                value=False,\n",
    "                interactive=custom_model_loaded,\n",
    "                info=\"Use custom encoder model instead of ChromaDB's default\" if custom_model_loaded else \"Custom model not found\"\n",
    "            )\n",
    "\n",
    "    submit_btn = gr.Button(\"Submit Question\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
    "        with gr.Column():\n",
    "            sources_output = gr.Textbox(label=\"Source Chunks\", lines=10, max_lines=30)\n",
    "            elapsed_output = gr.Textbox(label=\"Elapsed Time (s)\", lines=1)\n",
    "\n",
    "    # Examples (last field corresponds to custom_encoder checkbox)\n",
    "    gr.Examples([\n",
    "        [\"Who is Dumbledore?\", 3, True, False],\n",
    "        [\"What happened when Harry met Hagrid?\", 5, True, False],\n",
    "        [\"What are the four houses at Hogwarts?\", 3, True, False],\n",
    "        [\"Why did Harry survive Voldemort's killing curse?\", 5, True, False],\n",
    "    ], inputs=[query_input, top_k, hybrid_search, custom_encoder_cb])\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=rag_query,\n",
    "        inputs=[query_input, top_k, hybrid_search, custom_encoder_cb],\n",
    "        outputs=[answer_output, sources_output, elapsed_output]\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
