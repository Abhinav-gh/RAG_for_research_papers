{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder_from_scratch_with_pooling.py\n",
    "# Full PyTorch implementation of BERT-base style encoder-only Transformer with:\n",
    "# - Word2Vec token embeddings\n",
    "# - Learned positional & segment embeddings\n",
    "# - MLM (15% mask; 80/10/10 replacement rule) -- PAD excluded from random replacements\n",
    "# - NSP head\n",
    "# - Mask-aware mean pooling for retrieval (added)\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install torch torchvision torchaudio\n",
    "#   pip install gensim\n",
    "#\n",
    "# Run: python bert_encoder_from_scratch_with_pooling.py\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# If you need to install gensim: uncomment below\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -------------------------\n",
    "# Config (change for experiments)\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_SIZE = 768       # BERT-base hidden size\n",
    "NUM_LAYERS = 12         # BERT-base number of encoder layers\n",
    "NUM_HEADS = 12          # BERT-base attention heads\n",
    "FFN_DIM = 3072          # BERT-base intermediate size\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE  # Use same dimension for direct weight tie\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# Simple whitespace tokenizer & vocab build (replace if desired)\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    # build vocab\n",
    "    stoi = {}\n",
    "    itos = []\n",
    "    # add special tokens first\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                # project or pad/truncate (here we project via linear mapping if sizes differ)\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "        else:\n",
    "            # keep random init for unknown tokens (special tokens will remain random except we can set PAD to zeros)\n",
    "            pass\n",
    "    # set PAD embedding to zeros\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset for NSP + MLM\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, paragraphs: List[str], stoi: dict, max_seq_len=MAX_SEQ_LEN, short_seq_prob=0.1):\n",
    "        \"\"\"\n",
    "        paragraphs: list of paragraphs (strings). We'll split paragraphs into sentences by '.' naive split for demo.\n",
    "        Real pipelines use sentence-splitting more robustly.\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.itos = {v:k for k,v in stoi.items()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.short_seq_prob = short_seq_prob\n",
    "\n",
    "        # build sentences list by naive splitting on periods (improve for real corpora)\n",
    "        sents = []\n",
    "        for p in paragraphs:\n",
    "            pieces = [s.strip() for s in p.strip().split('.') if s.strip()]\n",
    "            sents.extend(pieces)\n",
    "        # Now we have a list of sentences\n",
    "        self.sentences = sents\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.sentences) - 1)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        ids = [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "        return ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # create a training example for NSP:\n",
    "        # 50% of time next sentence is actual next sentence (label 1), else random sentence (label 0)\n",
    "        is_next = random.random() < 0.5\n",
    "        sent_a = self.sentences[idx]\n",
    "        if is_next:\n",
    "            sent_b = self.sentences[idx+1]\n",
    "            label_nsp = 1\n",
    "        else:\n",
    "            # random sentence\n",
    "            rand_idx = random.randint(0, len(self.sentences)-1)\n",
    "            # avoid accidentally picking the actual next\n",
    "            if rand_idx == idx+1 and len(self.sentences) > 2:\n",
    "                rand_idx = (rand_idx + 2) % len(self.sentences)\n",
    "            sent_b = self.sentences[rand_idx]\n",
    "            label_nsp = 0\n",
    "        ids_a = self._tokenize_to_ids(sent_a)\n",
    "        ids_b = self._tokenize_to_ids(sent_b)\n",
    "\n",
    "        # Truncate if too long; simple truncation here (not optimal)\n",
    "        # Reserve 3 tokens for [CLS], [SEP], [SEP]\n",
    "        max_len_for_pair = self.max_seq_len - 3\n",
    "        # simple truncation from the end\n",
    "        while len(ids_a) + len(ids_b) > max_len_for_pair:\n",
    "            if len(ids_a) > len(ids_b):\n",
    "                ids_a.pop()\n",
    "            else:\n",
    "                ids_b.pop()\n",
    "\n",
    "        # Build input: [CLS] A [SEP] B [SEP]\n",
    "        input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "        # token type ids: 0 for A (including CLS), 1 for B\n",
    "        token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"nsp_label\": torch.tensor(label_nsp, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    # Pads sequences to max length in batch\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids = []\n",
    "    padded_token_types = []\n",
    "    attention_masks = []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "    padded_input_ids = torch.stack(padded_input_ids)\n",
    "    padded_token_types = torch.stack(padded_token_types)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"token_type_ids\": padded_token_types,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"nsp_labels\": nsp_labels\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM masking function (15% mask with 80/10/10)\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids: torch.Tensor, pad_id: int, mask_token_id: int, vocab_size: int, mask_prob=MLM_MASK_PROB):\n",
    "    \"\"\"\n",
    "    input_ids: (batch, seq_len)\n",
    "    Returns:\n",
    "        input_ids_masked: masked inputs with replacements applied\n",
    "        mlm_labels: same shape, original token ids where masked else -100 (for ignore_index)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, fill_value=-100)\n",
    "\n",
    "    # don't consider special tokens for masking: [CLS], [SEP], [PAD], token_type maybe?\n",
    "    # We'll consider tokens where input_ids != pad_id and not CLS/SEP/MASK (we shouldn't mask [CLS]/[SEP])\n",
    "    special_ids = set([pad_id, mask_token_id])  # pad and mask token are special; also exclude CLS/SEP\n",
    "    # We'll expect caller to pass special token ids; for now assume CLS and SEP indices are small (we can pass them in if needed)\n",
    "    # To be safer: we'll treat indices 0..len(SPECIAL_TOKENS)-1 as special because we added them first in vocab\n",
    "    # BUT we need pad_id and mask_token_id already.\n",
    "\n",
    "    # Create mask positions\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob)\n",
    "    # do not mask PAD\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    # do not mask CLS or SEP (we assume they are among small indices; instead pass a mask)\n",
    "    # Let's mask out positions where token id is CLS (index of CLS_TOKEN) or SEP\n",
    "    # To be safe, caller should ensure CLS/SEP not masked; but let's set zeros for tokens with ids 0..4 (special tokens)\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "\n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()  # shape batch x seq_len\n",
    "\n",
    "    # For masked positions, create labels and replace according to 80/10/10\n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "\n",
    "    input_ids_masked = input_ids.clone()\n",
    "\n",
    "    # For each masked position, decide replacement\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    # 80% -> [MASK]\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    # 10% -> random token\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    # 10% -> keep original (do nothing) -> masked_positions & (rand >= 0.9)\n",
    "\n",
    "    # Apply [MASK]\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "\n",
    "    # Apply random tokens (excluding PAD token by sampling from non-special region if possible)\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        special_upper = len(SPECIAL_TOKENS)  # exclude special tokens including PAD\n",
    "        if special_upper < vocab_size:\n",
    "            # sample from [special_upper, vocab_size) to avoid sampling special tokens (including PAD)\n",
    "            rand_tokens = torch.randint(low=special_upper, high=vocab_size, size=(count,), dtype=torch.long, device=input_ids.device)\n",
    "        else:\n",
    "            # fallback: if there are no non-special tokens (very small vocab), sample across vocab but avoid PAD if possible\n",
    "            rand_tokens = torch.randint(low=0, high=vocab_size, size=(count,), dtype=torch.long, device=input_ids.device)\n",
    "            if vocab_size > 1:\n",
    "                # replace any accidental PAD selections with another token (pad_id+1 mod vocab_size)\n",
    "                rand_tokens[rand_tokens == pad_id] = (pad_id + 1) % vocab_size\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Model components\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn_layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ffn_dim, hidden_size),\n",
    "        )\n",
    "        self.ffn_layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # x: (batch, seq_len, hidden)\n",
    "        # attention_mask: (batch, seq_len) with 1 for tokens to keep, 0 for pad\n",
    "        # MultiheadAttention with batch_first=True expects src_key_padding_mask shape (batch, seq_len) with True for tokens to be masked\n",
    "        key_padding_mask = (attention_mask == 0)  # True where pad\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.attn_layernorm(x)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        x = self.ffn_layernorm(x)\n",
    "        return x\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        # Token embeddings (initialized from Word2Vec weights if provided)\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            with torch.no_grad():\n",
    "                self.token_embeddings.weight.copy_(embedding_weights)\n",
    "\n",
    "        # Position & segment embeddings\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder stack\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # NSP head: take hidden state of first token ([CLS]) and classify\n",
    "        self.nsp_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        # MLM head (language modeling) â€” weight tied to token_embeddings\n",
    "        # We'll implement output logits via F.linear using token_embeddings.weight and a bias\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def encode(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Run embeddings + encoder stack and return sequence_output (batch, seq_len, hidden).\n",
    "        This method is useful when you only need representations (e.g., for retrieval).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "\n",
    "        # Embeddings\n",
    "        token_emb = self.token_embeddings(input_ids)  # (batch, seq_len, hidden)\n",
    "        # positions\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.position_embeddings(position_ids)\n",
    "        seg_emb = self.segment_embeddings(token_type_ids)\n",
    "        x = token_emb + pos_emb + seg_emb\n",
    "        x = self.emb_layernorm(x)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        # Encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        sequence_output = x  # (batch, seq_len, hidden)\n",
    "        return sequence_output\n",
    "\n",
    "    def get_pooled_embeddings(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor, token_type_ids: torch.LongTensor = None, exclude_special: bool = True, normalize: bool = True):\n",
    "        \"\"\"\n",
    "        Compute mask-aware mean pooling over encoder outputs.\n",
    "\n",
    "        - attention_mask: (batch, seq_len) with 1 for real tokens and 0 for padding.\n",
    "        - exclude_special: if True, positions whose token id is in the first `len(SPECIAL_TOKENS)` indices are excluded\n",
    "                           from the mean (commonly excludes [CLS],[SEP],[PAD],[MASK],[UNK]).\n",
    "        - normalize: if True, L2-normalize the resulting pooled vectors (good for cosine-similarity retrieval).\n",
    "\n",
    "        Returns:\n",
    "            pooled: (batch, hidden)\n",
    "        \"\"\"\n",
    "        # Run encoder to get token-level representations\n",
    "        sequence_output = self.encode(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)  # [B, L, H]\n",
    "\n",
    "        # Build mask float (1.0 for tokens to include, 0.0 for pad/special if excluded)\n",
    "        mask = attention_mask.to(sequence_output.dtype)  # [B, L], 1.0 for valid tokens\n",
    "\n",
    "        if exclude_special:\n",
    "            # Exclude tokens whose ids are in the first len(SPECIAL_TOKENS) (we added these at the front of vocab)\n",
    "            # This removes [CLS], [SEP], etc. from the pooling by setting mask=0 at those positions.\n",
    "            special_upper = len(SPECIAL_TOKENS)\n",
    "            special_flags = (input_ids < special_upper).to(sequence_output.dtype)  # 1.0 at special positions\n",
    "            mask = mask * (1.0 - special_flags)  # zero out special positions\n",
    "\n",
    "        # Avoid division by zero: at least keep denom >= eps\n",
    "        denom = mask.sum(dim=1, keepdim=True).clamp(min=1e-9)  # [B, 1]\n",
    "\n",
    "        # Weighted sum and mean\n",
    "        masked_sum = torch.einsum('bld,bl->bd', sequence_output, mask)  # (B, H)\n",
    "        pooled = masked_sum / denom  # (B, H)\n",
    "\n",
    "        if normalize:\n",
    "            pooled = F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        input_ids: (batch, seq_len)\n",
    "        token_type_ids: (batch, seq_len)\n",
    "        attention_mask: (batch, seq_len) 1 for tokens, 0 for pad\n",
    "        Returns:\n",
    "            mlm_logits: (batch, seq_len, vocab)\n",
    "            nsp_logits: (batch, 2)\n",
    "            sequence_output: (batch, seq_len, hidden)\n",
    "        Note: For retrieval embeddings, call model.get_pooled_embeddings(...)\n",
    "        \"\"\"\n",
    "        # reuse encode for sequence output\n",
    "        sequence_output = self.encode(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        # pooled output = representation of [CLS] (first token)\n",
    "        pooled = sequence_output[:, 0]  # (batch, hidden)\n",
    "        nsp_logits = self.nsp_classifier(pooled)  # (batch, 2)\n",
    "\n",
    "        # MLM logits via weight tying: use F.linear\n",
    "        mlm_logits = F.linear(sequence_output, self.token_embeddings.weight, self.mlm_bias)  # (batch, seq_len, vocab)\n",
    "\n",
    "        return mlm_logits, nsp_logits, sequence_output\n",
    "\n",
    "# -------------------------\n",
    "# Example usage: Putting it all together\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Sample (toy) corpus; replace with your real corpus list of paragraphs.\n",
    "    sample_corpus = [\n",
    "        \"the quick brown fox jumps over the lazy dog. the dog did not seem to mind.\",\n",
    "        \"i love machine learning and natural language processing. transformers are powerful models.\",\n",
    "        \"this is an example sentence for training word2vec. another example sentence is here.\",\n",
    "        \"today is a sunny day. we will go to the park and enjoy the weather.\",\n",
    "        \"deep learning enables many tasks such as translation, summarization and question answering.\"\n",
    "    ]\n",
    "    # Build vocabulary from corpus\n",
    "    stoi, itos = build_vocab([s for p in sample_corpus for s in p.strip().split('.') if s.strip()])\n",
    "    vocab_size = len(itos)\n",
    "    pad_id = stoi[PAD_TOKEN]\n",
    "    mask_id = stoi[MASK_TOKEN]\n",
    "    cls_id = stoi[CLS_TOKEN]\n",
    "    sep_id = stoi[SEP_TOKEN]\n",
    "    unk_id = stoi[UNK_TOKEN]\n",
    "\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "    # Train Word2Vec on sentences (tokenized)\n",
    "    # WARNING: training vector_size=768 is heavy; for quick test, you can set smaller WORD2VEC_SIZE in config.\n",
    "    print(\"Training Word2Vec (this may take a while for large vector sizes)...\")\n",
    "    # Prepare sentences list for word2vec (list of token lists)\n",
    "    tokenized_sents = []\n",
    "    for p in sample_corpus:\n",
    "        pieces = [s.strip() for s in p.strip().split('.') if s.strip()]\n",
    "        for sent in pieces:\n",
    "            tokenized_sents.append(sent.split())\n",
    "    w2v = Word2Vec(sentences=tokenized_sents, vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=10)\n",
    "\n",
    "    # Build embedding matrix aligned with itos\n",
    "    embedding_weights = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = BertPretrainingDataset(paragraphs=sample_corpus, stoi=stoi, max_seq_len=MAX_SEQ_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "    # Initialize model\n",
    "    model = BertEncoderModel(vocab_size=vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=MAX_SEQ_LEN, pad_token_id=pad_id, embedding_weights=embedding_weights, dropout=DROPOUT)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Loss functions\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 1  # change as needed\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(DEVICE)\n",
    "\n",
    "            # Create MLM masked inputs & labels according to scheme\n",
    "            input_ids_masked, mlm_labels = create_mlm_labels_and_masked_input(input_ids, pad_id, mask_id, vocab_size, mask_prob=MLM_MASK_PROB)\n",
    "            input_ids_masked = input_ids_masked.to(DEVICE)\n",
    "            mlm_labels = mlm_labels.to(DEVICE)\n",
    "\n",
    "            # Forward\n",
    "            mlm_logits, nsp_logits, _ = model(input_ids_masked, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute MLM loss: reshape to (batch * seq_len, vocab)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
    "            nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            loss = mlm_loss + nsp_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0:\n",
    "                print(f\"Epoch {epoch} step {global_step}: loss {loss.item():.4f} (mlm {mlm_loss.item():.4f} nsp {nsp_loss.item():.4f})\")\n",
    "\n",
    "    print(\"Training loop finished (toy example). Save model if desired.\")\n",
    "    # Example saving\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"vocab\": itos,\n",
    "        \"stoi\": stoi\n",
    "    }, \"bert_encoder_toy_with_pooling.pth\")\n",
    "    print(\"Saved to bert_encoder_toy_with_pooling.pth\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Example: get pooled retrieval embeddings for some sentences\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Take a small batch from dataset and produce embeddings\n",
    "        sample_batch = next(iter(DataLoader(dataset, batch_size=2, collate_fn=lambda b: collate_fn(b, pad_id))))\n",
    "        ids = sample_batch[\"input_ids\"].to(DEVICE)\n",
    "        masks = sample_batch[\"attention_mask\"].to(DEVICE)\n",
    "        types = sample_batch[\"token_type_ids\"].to(DEVICE)\n",
    "        embeddings = model.get_pooled_embeddings(ids, masks, token_type_ids=types, exclude_special=True, normalize=True)\n",
    "        print(\"Pooled embeddings shape:\", embeddings.shape)  # (batch, hidden)\n",
    "        # First 2 dims\n",
    "        print(\"First pooled vector (first 8 values):\", embeddings[0, :8].cpu().numpy())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
