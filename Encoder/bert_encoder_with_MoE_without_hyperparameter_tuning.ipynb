{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25491941",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f917bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder_from_scratch_with_pooling_multitype_allpairs.py\n",
    "# Modified version that supports:\n",
    "# - 'C' = Chunk (uses MLM + NSP) → uses ALL possible positive & negative pairs\n",
    "# - 'Q' = Query (uses MLM only)\n",
    "# - Added model saving and evaluation on test subset\n",
    "# - Added Mixture-of-Experts (MoE) in feedforward with Top-K=2 routing and 5 experts\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 1024\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# Utility: Vocab builder\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    stoi, itos = {}, []\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (supports queries and chunks)\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], stoi: dict, max_seq_len=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: list of tuples [(text, discriminator)], where discriminator ∈ {'Q', 'C'}\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        for text, dtype in data:\n",
    "            if dtype == \"Q\":\n",
    "                # Single-sentence query (MLM only)\n",
    "                self.samples.append((text, dtype, None, None))\n",
    "            elif dtype == \"C\":\n",
    "                # Split chunk into sentences\n",
    "                sents = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "                if len(sents) < 2:\n",
    "                    sents = sents + sents  # duplicate if only one sentence\n",
    "                # Positive pairs: consecutive sentences\n",
    "                for i in range(len(sents) - 1):\n",
    "                    self.samples.append((sents[i], \"C\", sents[i + 1], 1))\n",
    "                # Negative pairs: non-consecutive\n",
    "                for i in range(len(sents)):\n",
    "                    for j in range(len(sents)):\n",
    "                        if abs(i - j) > 1:  # skip consecutive\n",
    "                            self.samples.append((sents[i], \"C\", sents[j], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        return [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_a, dtype, sent_b, nsp_label = self.samples[idx]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 1: Query (MLM only)\n",
    "        # -------------------------------\n",
    "        if dtype == 'Q':\n",
    "            ids = self._tokenize_to_ids(sent_a)\n",
    "            ids = ids[:self.max_seq_len - 2]\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * len(input_ids)\n",
    "            nsp_label = -100  # dummy\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"Q\"\n",
    "            }\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 2: Chunk (MLM + NSP)\n",
    "        # -------------------------------\n",
    "        elif dtype == 'C':\n",
    "            ids_a = self._tokenize_to_ids(sent_a)\n",
    "            ids_b = self._tokenize_to_ids(sent_b)\n",
    "            while len(ids_a) + len(ids_b) > self.max_seq_len - 3:\n",
    "                if len(ids_a) > len(ids_b):\n",
    "                    ids_a.pop()\n",
    "                else:\n",
    "                    ids_b.pop()\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"C\"\n",
    "            }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "    batch_types = [b[\"batch_type\"] for b in batch]\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids, padded_token_types, attention_masks = [], [], []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"token_type_ids\": torch.stack(padded_token_types),\n",
    "        \"attention_mask\": torch.stack(attention_masks),\n",
    "        \"nsp_labels\": nsp_labels,\n",
    "        \"batch_type\": batch_types\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM Masking\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids, pad_id, mask_token_id, vocab_size, mask_prob=MLM_MASK_PROB):\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, -100)\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob)\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()\n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        rand_tokens = torch.randint(len(SPECIAL_TOKENS), vocab_size, (count,), device=input_ids.device)\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Mixture-of-Experts Module\n",
    "# -------------------------\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # experts: each expert is a small Feed-Forward Network (H -> ffn_dim -> H)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # router: maps hidden vector to expert logits\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, S, H)\n",
    "        returns: out (B, S, H), aux_loss (scalar)\n",
    "        \"\"\"\n",
    "        B, S, H = x.size()\n",
    "        # ---- router logits (noiseless, for load-balancing) ----\n",
    "        logits = self.router(x)  # (B, S, E)\n",
    "        # soft probabilities for load balancing (use non-noisy softmax)\n",
    "        probs_all = F.softmax(logits, dim=-1)  # (B, S, E)\n",
    "        # importance per expert:\n",
    "        importance = probs_all.sum(dim=(0, 1))  # (E,)\n",
    "        total_tokens = float(B * S)\n",
    "        # aux_loss encourages balanced importance across experts\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "\n",
    "        # ---- noisy logits for selection (only add noise during training) ----\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "\n",
    "        # top-k selection on noisy logits\n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)  # shapes (B,S,k)\n",
    "        # convert topk vals to normalized weights via softmax over k\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)  # (B,S,k)\n",
    "\n",
    "        # Compute each expert's output on the full x (inefficient but simple)\n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))  # (B,S,H)\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)  # (B,S,E,H)\n",
    "\n",
    "        # Build a gating tensor of shape (B,S,E) with nonzero entries only at topk indices\n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)  # float\n",
    "        # scatter the topk_weights into gating at positions topk_idx\n",
    "        # topk_idx: (B,S,k), topk_weights: (B,S,k)\n",
    "        # We can flatten and scatter\n",
    "        flat_idx = topk_idx.view(-1, self.k)  # (B*S, k)\n",
    "        flat_w = topk_weights.view(-1, self.k)  # (B*S, k)\n",
    "        # For each row r in [0..B*S-1], scatter into gating_flat[r, idx] = weight\n",
    "        gating_flat = gating.view(-1, self.num_experts)  # (B*S, E)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)  # (B*S, k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)  # (B,S,E)\n",
    "\n",
    "        # Combine experts: out[b,s,:] = sum_e gating[b,s,e] * expert_stack[b,s,e,:]\n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)  # (B,S,H)\n",
    "\n",
    "        return out, aux_loss\n",
    "\n",
    "# -------------------------\n",
    "# Transformer encoder\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        # Replace ffn with MoE module\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        # MoE FFN\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=DROPOUT, moe_experts=moe_experts, moe_k=moe_k) for _ in range(num_layers)])\n",
    "        self.nsp_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 2))\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux = layer(x, mask)\n",
    "            total_aux = total_aux + aux\n",
    "        return x, total_aux\n",
    "    def forward(self, ids, tt=None, mask=None):\n",
    "        seq_out, total_aux = self.encode(ids, tt, mask)\n",
    "        pooled = seq_out[:, 0]\n",
    "        nsp_logits = self.nsp_classifier(pooled)\n",
    "        mlm_logits = F.linear(seq_out, self.token_embeddings.weight, self.mlm_bias)\n",
    "        return mlm_logits, nsp_logits, total_aux\n",
    "\n",
    "# -------------------------\n",
    "# Training and Evaluation\n",
    "# -------------------------\n",
    "def main():\n",
    "    corpus = [\n",
    "        (\"the quick brown fox jumps over the lazy dog. the dog did not mind.\", \"C\"),\n",
    "        (\"i love machine learning and transformers.\", \"Q\"),\n",
    "        (\"deep learning enables summarization and translation. it is powerful.\", \"C\"),\n",
    "        (\"best restaurants near me\", \"Q\")\n",
    "    ]\n",
    "    stoi, itos = build_vocab([x[0] for x in corpus])\n",
    "    vocab_size = len(itos)\n",
    "    w2v = train_word2vec([x[0] for x in corpus])\n",
    "    emb = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "    pad_id = stoi[PAD_TOKEN]; mask_id = stoi[MASK_TOKEN]\n",
    "    ds = BertPretrainingDataset(corpus, stoi)\n",
    "\n",
    "    # Split train/test\n",
    "    total_len = len(ds)\n",
    "    test_len = max(1, total_len // 5)\n",
    "    train_len = total_len - test_len\n",
    "    train_ds, test_ds = random_split(ds, [train_len, test_len])\n",
    "    dl_train = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    dl_test = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "    # instantiate model with MoE: 5 experts, top-k=2\n",
    "    model = BertEncoderModel(vocab_size, embedding_weights=emb, moe_experts=5, moe_k=2).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        for batch in dl_train:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            tts = batch[\"token_type_ids\"].to(DEVICE)\n",
    "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(DEVICE)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "            ids_masked, mlm_labels = ids_masked.to(DEVICE), mlm_labels.to(DEVICE)\n",
    "            mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=DEVICE)\n",
    "            # auxiliary load-balancing loss scaled down\n",
    "            aux_coeff = 0.01\n",
    "            loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(f\"Loss {loss.item():.4f} (MLM {mlm_loss.item():.4f}, NSP {nsp_loss.item():.4f}, AUX {aux_coeff * aux_loss.item():.6f})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Save model and vocab\n",
    "    # -------------------------\n",
    "    save_dir = \"saved_bert_encoder\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"bert_encoder.pt\"))\n",
    "    import json\n",
    "    with open(os.path.join(save_dir, \"vocab.json\"), \"w\") as f:\n",
    "        json.dump({\"stoi\": stoi, \"itos\": itos}, f)\n",
    "    print(f\"Model and vocab saved to {save_dir}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Evaluation\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    total_mlm_correct = 0\n",
    "    total_mlm_count = 0\n",
    "    total_nsp_correct = 0\n",
    "    total_nsp_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dl_test:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            tts = batch[\"token_type_ids\"].to(DEVICE)\n",
    "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(DEVICE)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "            ids_masked, mlm_labels = ids_masked.to(DEVICE), mlm_labels.to(DEVICE)\n",
    "            mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "            # MLM accuracy\n",
    "            mlm_preds = mlm_logits.argmax(-1)\n",
    "            mask_positions = mlm_labels != -100\n",
    "            total_mlm_correct += (mlm_preds[mask_positions] == mlm_labels[mask_positions]).sum().item()\n",
    "            total_mlm_count += mask_positions.sum().item()\n",
    "            # NSP accuracy\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_preds = nsp_logits.argmax(-1)\n",
    "                total_nsp_correct += (nsp_preds == nsp_labels).sum().item()\n",
    "                total_nsp_count += nsp_labels.numel()\n",
    "\n",
    "    mlm_acc = total_mlm_correct / max(1, total_mlm_count)\n",
    "    nsp_acc = total_nsp_correct / max(1, total_nsp_count)\n",
    "    print(f\"MLM Accuracy: {mlm_acc:.4f}, NSP Accuracy: {nsp_acc:.4f}\")\n",
    "    print(\"Training and evaluation done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
