{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30feaa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder_from_scratch_with_pooling_multitype.py\n",
    "# Modified version that supports two input types:\n",
    "# - 'C' = Chunk (uses MLM + NSP)\n",
    "# - 'Q' = Query (uses MLM only)\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 1024\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# Utility: Vocab builder\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    stoi, itos = {}, []\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (supports queries and chunks)\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], stoi: dict, max_seq_len=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: list of tuples [(text, discriminator)], where discriminator âˆˆ {'Q', 'C'}\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        return [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, dtype = self.data[idx]\n",
    "        # -------------------------------\n",
    "        # Case 1: Query (MLM only)\n",
    "        # -------------------------------\n",
    "        if dtype == 'Q':\n",
    "            ids = self._tokenize_to_ids(text)\n",
    "            ids = ids[:self.max_seq_len - 2]\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * len(input_ids)\n",
    "            nsp_label = -100  # dummy\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"Q\"\n",
    "            }\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 2: Chunk (MLM + NSP)\n",
    "        # -------------------------------\n",
    "        elif dtype == 'C':\n",
    "            # simple sentence split\n",
    "            sents = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "            if len(sents) < 2:\n",
    "                sents = sents + sents  # duplicate if only one sentence\n",
    "            idx_s = random.randint(0, len(sents) - 2)\n",
    "            sent_a = sents[idx_s]\n",
    "            is_next = random.random() < 0.5\n",
    "            if is_next:\n",
    "                sent_b = sents[idx_s + 1]\n",
    "                nsp_label = 1\n",
    "            else:\n",
    "                rand_idx = random.randint(0, len(sents) - 1)\n",
    "                while rand_idx == idx_s + 1:\n",
    "                    rand_idx = random.randint(0, len(sents) - 1)\n",
    "                sent_b = sents[rand_idx]\n",
    "                nsp_label = 0\n",
    "            ids_a = self._tokenize_to_ids(sent_a)\n",
    "            ids_b = self._tokenize_to_ids(sent_b)\n",
    "            while len(ids_a) + len(ids_b) > self.max_seq_len - 3:\n",
    "                if len(ids_a) > len(ids_b):\n",
    "                    ids_a.pop()\n",
    "                else:\n",
    "                    ids_b.pop()\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"C\"\n",
    "            }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "    batch_types = [b[\"batch_type\"] for b in batch]\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids, padded_token_types, attention_masks = [], [], []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"token_type_ids\": torch.stack(padded_token_types),\n",
    "        \"attention_mask\": torch.stack(attention_masks),\n",
    "        \"nsp_labels\": nsp_labels,\n",
    "        \"batch_type\": batch_types\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM Masking\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids, pad_id, mask_token_id, vocab_size, mask_prob=MLM_MASK_PROB):\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, -100)\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob)\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()\n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        rand_tokens = torch.randint(len(SPECIAL_TOKENS), vocab_size, (count,), device=input_ids.device)\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Transformer encoder\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = nn.Sequential(nn.Linear(hidden_size, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, ffn_dim) for _ in range(num_layers)])\n",
    "        self.nsp_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 2))\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    def forward(self, ids, tt=None, mask=None):\n",
    "        seq_out = self.encode(ids, tt, mask)\n",
    "        pooled = seq_out[:, 0]\n",
    "        nsp_logits = self.nsp_classifier(pooled)\n",
    "        mlm_logits = F.linear(seq_out, self.token_embeddings.weight, self.mlm_bias)\n",
    "        return mlm_logits, nsp_logits\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Dummy sample with types\n",
    "    corpus = [\n",
    "        (\"the quick brown fox jumps over the lazy dog. the dog did not mind.\", \"C\"),\n",
    "        (\"i love machine learning and transformers.\", \"Q\"),\n",
    "        (\"deep learning enables summarization and translation. it is powerful.\", \"C\"),\n",
    "        (\"best restaurants near me\", \"Q\")\n",
    "    ]\n",
    "    stoi, itos = build_vocab([x[0] for x in corpus])\n",
    "    vocab_size = len(itos)\n",
    "    w2v = train_word2vec([x[0] for x in corpus])\n",
    "    emb = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "    pad_id = stoi[PAD_TOKEN]; mask_id = stoi[MASK_TOKEN]\n",
    "    ds = BertPretrainingDataset(corpus, stoi)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    model = BertEncoderModel(vocab_size, embedding_weights=emb).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        for batch in dl:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            tts = batch[\"token_type_ids\"].to(DEVICE)\n",
    "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(DEVICE)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "            ids_masked, mlm_labels = ids_masked.to(DEVICE), mlm_labels.to(DEVICE)\n",
    "            mlm_logits, nsp_logits = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
    "            # Only compute NSP for chunk batches\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=DEVICE)\n",
    "            loss = mlm_loss + nsp_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(f\"Loss {loss.item():.4f} (MLM {mlm_loss.item():.4f}, NSP {nsp_loss.item():.4f})\")\n",
    "    print(\"Training done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11511736",
   "metadata": {},
   "source": [
    "The code above does not consider all positive and negative sentence pairs in a chunk for NSP. The code below uses all possible positive and negative pairs for every chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81839de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder_from_scratch_with_pooling_multitype_allpairs.py\n",
    "# Modified version that supports:\n",
    "# - 'C' = Chunk (uses MLM + NSP) â†’ uses ALL possible positive & negative pairs\n",
    "# - 'Q' = Query (uses MLM only)\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 1024\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# Utility: Vocab builder\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    stoi, itos = {}, []\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (supports queries and chunks)\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], stoi: dict, max_seq_len=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: list of tuples [(text, discriminator)], where discriminator âˆˆ {'Q', 'C'}\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        for text, dtype in data:\n",
    "            if dtype == \"Q\":\n",
    "                # Single-sentence query (MLM only)\n",
    "                self.samples.append((text, dtype, None, None))\n",
    "            elif dtype == \"C\":\n",
    "                # Split chunk into sentences\n",
    "                sents = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "                if len(sents) < 2:\n",
    "                    sents = sents + sents  # duplicate if only one sentence\n",
    "                # Positive pairs: consecutive sentences\n",
    "                for i in range(len(sents) - 1):\n",
    "                    self.samples.append((sents[i], \"C\", sents[i + 1], 1))\n",
    "                # Negative pairs: non-consecutive\n",
    "                for i in range(len(sents)):\n",
    "                    for j in range(len(sents)):\n",
    "                        if abs(i - j) > 1:  # skip consecutive\n",
    "                            self.samples.append((sents[i], \"C\", sents[j], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        return [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_a, dtype, sent_b, nsp_label = self.samples[idx]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 1: Query (MLM only)\n",
    "        # -------------------------------\n",
    "        if dtype == 'Q':\n",
    "            ids = self._tokenize_to_ids(sent_a)\n",
    "            ids = ids[:self.max_seq_len - 2]\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * len(input_ids)\n",
    "            nsp_label = -100  # dummy\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"Q\"\n",
    "            }\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 2: Chunk (MLM + NSP) â€” uses precomputed pairs\n",
    "        # -------------------------------\n",
    "        elif dtype == 'C':\n",
    "            ids_a = self._tokenize_to_ids(sent_a)\n",
    "            ids_b = self._tokenize_to_ids(sent_b)\n",
    "            while len(ids_a) + len(ids_b) > self.max_seq_len - 3:\n",
    "                if len(ids_a) > len(ids_b):\n",
    "                    ids_a.pop()\n",
    "                else:\n",
    "                    ids_b.pop()\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"C\"\n",
    "            }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "    batch_types = [b[\"batch_type\"] for b in batch]\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids, padded_token_types, attention_masks = [], [], []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"token_type_ids\": torch.stack(padded_token_types),\n",
    "        \"attention_mask\": torch.stack(attention_masks),\n",
    "        \"nsp_labels\": nsp_labels,\n",
    "        \"batch_type\": batch_types\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM Masking\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids, pad_id, mask_token_id, vocab_size, mask_prob=MLM_MASK_PROB):\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, -100)\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob)\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()\n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        rand_tokens = torch.randint(len(SPECIAL_TOKENS), vocab_size, (count,), device=input_ids.device)\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Transformer encoder\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = nn.Sequential(nn.Linear(hidden_size, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, ffn_dim) for _ in range(num_layers)])\n",
    "        self.nsp_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 2))\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    def forward(self, ids, tt=None, mask=None):\n",
    "        seq_out = self.encode(ids, tt, mask)\n",
    "        pooled = seq_out[:, 0]\n",
    "        nsp_logits = self.nsp_classifier(pooled)\n",
    "        mlm_logits = F.linear(seq_out, self.token_embeddings.weight, self.mlm_bias)\n",
    "        return mlm_logits, nsp_logits\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "def main():\n",
    "    corpus = [\n",
    "        (\"the quick brown fox jumps over the lazy dog. the dog did not mind.\", \"C\"),\n",
    "        (\"i love machine learning and transformers.\", \"Q\"),\n",
    "        (\"deep learning enables summarization and translation. it is powerful.\", \"C\"),\n",
    "        (\"best restaurants near me\", \"Q\")\n",
    "    ]\n",
    "    stoi, itos = build_vocab([x[0] for x in corpus])\n",
    "    vocab_size = len(itos)\n",
    "    w2v = train_word2vec([x[0] for x in corpus])\n",
    "    emb = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "    pad_id = stoi[PAD_TOKEN]; mask_id = stoi[MASK_TOKEN]\n",
    "    ds = BertPretrainingDataset(corpus, stoi)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    model = BertEncoderModel(vocab_size, embedding_weights=emb).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        for batch in dl:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            tts = batch[\"token_type_ids\"].to(DEVICE)\n",
    "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(DEVICE)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "            ids_masked, mlm_labels = ids_masked.to(DEVICE), mlm_labels.to(DEVICE)\n",
    "            mlm_logits, nsp_logits = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=DEVICE)\n",
    "            loss = mlm_loss + nsp_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(f\"Loss {loss.item():.4f} (MLM {mlm_loss.item():.4f}, NSP {nsp_loss.item():.4f})\")\n",
    "    print(\"Training done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
