{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder_from_scratch_with_pooling_multitype_allpairs_bo.py\n",
    "# Your original code (MLM + NSP + MoE) refactored + Bayesian Optimization (Gaussian Process + EI)\n",
    "#  - 3 epochs per BO trial\n",
    "#  - 10 BO iterations\n",
    "#  - Logs saved to bo_logs/\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime\n",
    "\n",
    "# for Gaussian Process BO\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "from scipy.stats import norm\n",
    "\n",
    "# -------------------------\n",
    "# Config (DEFAULTS)\n",
    "# -------------------------\n",
    "VOCAB_MIN_FREQ = 1\n",
    "MAX_SEQ_LEN = 1024\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "FFN_DIM = 3072\n",
    "DROPOUT = 0.1\n",
    "WORD2VEC_SIZE = HIDDEN_SIZE\n",
    "WORD2VEC_WINDOW = 5\n",
    "WORD2VEC_MIN_COUNT = 1\n",
    "MLM_MASK_PROB = 0.15\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# -------------------------\n",
    "# Special tokens\n",
    "# -------------------------\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# -------------------------\n",
    "# BO Settings\n",
    "# -------------------------\n",
    "BO_ITERATIONS = 10\n",
    "BO_INIT_POINTS = 3   # random initial points\n",
    "TRIAL_EPOCHS = 3     # you asked for 3 epochs per evaluation\n",
    "BO_LOG_DIR = \"bo_logs\"\n",
    "\n",
    "os.makedirs(BO_LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BO_LOG_DIR, \"best_model\"), exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utility: Vocab builder\n",
    "# -------------------------\n",
    "def build_vocab(sentences: List[str], min_freq: int = VOCAB_MIN_FREQ):\n",
    "    from collections import Counter\n",
    "    token_counts = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        token_counts.update(tokens)\n",
    "    stoi, itos = {}, []\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        stoi[t] = len(itos)\n",
    "        itos.append(t)\n",
    "    for token, cnt in token_counts.items():\n",
    "        if cnt >= min_freq and token not in stoi:\n",
    "            stoi[token] = len(itos)\n",
    "            itos.append(token)\n",
    "    return stoi, itos\n",
    "\n",
    "# -------------------------\n",
    "# Train or load Word2Vec\n",
    "# -------------------------\n",
    "def train_word2vec(sentences: List[str], vector_size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW, min_count=WORD2VEC_MIN_COUNT, epochs=5):\n",
    "    tokenized = [s.strip().split() for s in sentences]\n",
    "    w2v = Word2Vec(sentences=tokenized, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, sg=0)\n",
    "    return w2v\n",
    "\n",
    "def build_embedding_matrix(w2v: Word2Vec, itos: List[str], hidden_size: int):\n",
    "    vocab_size = len(itos)\n",
    "    embeddings = np.random.normal(scale=0.02, size=(vocab_size, hidden_size)).astype(np.float32)\n",
    "    for idx, tok in enumerate(itos):\n",
    "        if tok in w2v.wv:\n",
    "            vec = w2v.wv[tok]\n",
    "            if vec.shape[0] != hidden_size:\n",
    "                vec = vec[:hidden_size] if vec.shape[0] >= hidden_size else np.pad(vec, (0, hidden_size - vec.shape[0]))\n",
    "            embeddings[idx] = vec\n",
    "    pad_idx = itos.index(PAD_TOKEN)\n",
    "    embeddings[pad_idx] = np.zeros(hidden_size, dtype=np.float32)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (supports queries and chunks)\n",
    "# -------------------------\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], stoi: dict, max_seq_len=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: list of tuples [(text, discriminator)], where discriminator ∈ {'Q', 'C'}\n",
    "        \"\"\"\n",
    "        self.stoi = stoi\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        for text, dtype in data:\n",
    "            if dtype == \"Q\":\n",
    "                # Single-sentence query (MLM only)\n",
    "                self.samples.append((text, dtype, None, None))\n",
    "            elif dtype == \"C\":\n",
    "                # Split chunk into sentences\n",
    "                sents = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "                if len(sents) < 2:\n",
    "                    sents = sents + sents  # duplicate if only one sentence\n",
    "                # Positive pairs: consecutive sentences\n",
    "                for i in range(len(sents) - 1):\n",
    "                    self.samples.append((sents[i], \"C\", sents[i + 1], 1))\n",
    "                # Negative pairs: non-consecutive\n",
    "                for i in range(len(sents)):\n",
    "                    for j in range(len(sents)):\n",
    "                        if abs(i - j) > 1:  # skip consecutive\n",
    "                            self.samples.append((sents[i], \"C\", sents[j], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _tokenize_to_ids(self, text: str) -> List[int]:\n",
    "        toks = text.strip().split()\n",
    "        return [self.stoi.get(t, self.stoi[UNK_TOKEN]) for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_a, dtype, sent_b, nsp_label = self.samples[idx]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 1: Query (MLM only)\n",
    "        # -------------------------------\n",
    "        if dtype == 'Q':\n",
    "            ids = self._tokenize_to_ids(sent_a)\n",
    "            ids = ids[:self.max_seq_len - 2]\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * len(input_ids)\n",
    "            nsp_label = -100  # dummy\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"Q\"\n",
    "            }\n",
    "\n",
    "        # -------------------------------\n",
    "        # Case 2: Chunk (MLM + NSP)\n",
    "        # -------------------------------\n",
    "        elif dtype == 'C':\n",
    "            ids_a = self._tokenize_to_ids(sent_a)\n",
    "            ids_b = self._tokenize_to_ids(sent_b)\n",
    "            while len(ids_a) + len(ids_b) > self.max_seq_len - 3:\n",
    "                if len(ids_a) > len(ids_b):\n",
    "                    ids_a.pop()\n",
    "                else:\n",
    "                    ids_b.pop()\n",
    "            input_ids = [self.stoi[CLS_TOKEN]] + ids_a + [self.stoi[SEP_TOKEN]] + ids_b + [self.stoi[SEP_TOKEN]]\n",
    "            token_type_ids = [0] * (len(ids_a) + 2) + [1] * (len(ids_b) + 1)\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"nsp_label\": torch.tensor(nsp_label, dtype=torch.long),\n",
    "                \"batch_type\": \"C\"\n",
    "            }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    input_ids_list = [b[\"input_ids\"] for b in batch]\n",
    "    token_type_list = [b[\"token_type_ids\"] for b in batch]\n",
    "    nsp_labels = torch.stack([b[\"nsp_label\"] for b in batch]).long()\n",
    "    batch_types = [b[\"batch_type\"] for b in batch]\n",
    "\n",
    "    max_len = max([x.size(0) for x in input_ids_list])\n",
    "    padded_input_ids, padded_token_types, attention_masks = [], [], []\n",
    "    for ids, tt in zip(input_ids_list, token_type_list):\n",
    "        pad_len = max_len - ids.size(0)\n",
    "        padded_input_ids.append(F.pad(ids, (0, pad_len), value=pad_id))\n",
    "        padded_token_types.append(F.pad(tt, (0, pad_len), value=0))\n",
    "        attention_masks.append((F.pad(ids, (0, pad_len), value=pad_id) != pad_id).long())\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"token_type_ids\": torch.stack(padded_token_types),\n",
    "        \"attention_mask\": torch.stack(attention_masks),\n",
    "        \"nsp_labels\": nsp_labels,\n",
    "        \"batch_type\": batch_types\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# MLM Masking\n",
    "# -------------------------\n",
    "def create_mlm_labels_and_masked_input(input_ids, pad_id, mask_token_id, vocab_size, mask_prob=MLM_MASK_PROB):\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mlm_labels = torch.full_like(input_ids, -100)\n",
    "    prob_matrix = torch.full((batch_size, seq_len), mask_prob)\n",
    "    prob_matrix[input_ids == pad_id] = 0.0\n",
    "    special_upper = len(SPECIAL_TOKENS)\n",
    "    prob_matrix[input_ids < special_upper] = 0.0\n",
    "    masked_positions = torch.bernoulli(prob_matrix).bool()\n",
    "    mlm_labels[masked_positions] = input_ids[masked_positions]\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    rand_for_replace = torch.rand_like(input_ids, dtype=torch.float)\n",
    "    mask_replace = masked_positions & (rand_for_replace < 0.8)\n",
    "    random_replace = masked_positions & (rand_for_replace >= 0.8) & (rand_for_replace < 0.9)\n",
    "    input_ids_masked[mask_replace] = mask_token_id\n",
    "    if random_replace.any():\n",
    "        count = int(random_replace.sum().item())\n",
    "        rand_tokens = torch.randint(len(SPECIAL_TOKENS), vocab_size, (count,), device=input_ids.device)\n",
    "        input_ids_masked[random_replace] = rand_tokens\n",
    "    return input_ids_masked, mlm_labels\n",
    "\n",
    "# -------------------------\n",
    "# Mixture-of-Experts Module (unchanged logic)\n",
    "# -------------------------\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_dim, num_experts=5, k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # experts: each expert is a small Feed-Forward Network (H -> ffn_dim -> H)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ffn_dim, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # router: maps hidden vector to expert logits\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, S, H)\n",
    "        returns: out (B, S, H), aux_loss (scalar)\n",
    "        \"\"\"\n",
    "        B, S, H = x.size()\n",
    "        # ---- router logits (noiseless, for load-balancing) ----\n",
    "        logits = self.router(x)  # (B, S, E)\n",
    "        # soft probabilities for load balancing (use non-noisy softmax)\n",
    "        probs_all = F.softmax(logits, dim=-1)  # (B, S, E)\n",
    "        # importance per expert:\n",
    "        importance = probs_all.sum(dim=(0, 1))  # (E,)\n",
    "        total_tokens = float(B * S)\n",
    "        # aux_loss encourages balanced importance across experts\n",
    "        aux_loss = (self.num_experts * (importance / total_tokens).pow(2).sum())\n",
    "\n",
    "        # ---- noisy logits for selection (only add noise during training) ----\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits_noisy = logits + noise\n",
    "        else:\n",
    "            logits_noisy = logits\n",
    "\n",
    "        # top-k selection on noisy logits\n",
    "        topk_vals, topk_idx = torch.topk(logits_noisy, self.k, dim=-1)  # shapes (B,S,k)\n",
    "        # convert topk vals to normalized weights via softmax over k\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)  # (B,S,k)\n",
    "\n",
    "        # Compute each expert's output on the full x (inefficient but simple)\n",
    "        expert_outs = []\n",
    "        for e in range(self.num_experts):\n",
    "            expert_outs.append(self.experts[e](x))  # (B,S,H)\n",
    "        expert_stack = torch.stack(expert_outs, dim=2)  # (B,S,E,H)\n",
    "\n",
    "        # Build a gating tensor of shape (B,S,E) with nonzero entries only at topk indices\n",
    "        device = x.device\n",
    "        gating = torch.zeros(B, S, self.num_experts, device=device, dtype=x.dtype)  # float\n",
    "        # scatter the topk_weights into gating at positions topk_idx\n",
    "        # topk_idx: (B,S,k), topk_weights: (B,S,k)\n",
    "        # We can flatten and scatter\n",
    "        flat_idx = topk_idx.view(-1, self.k)  # (B*S, k)\n",
    "        flat_w = topk_weights.view(-1, self.k)  # (B*S, k)\n",
    "        # For each row r in [0..B*S-1], scatter into gating_flat[r, idx] = weight\n",
    "        gating_flat = gating.view(-1, self.num_experts)  # (B*S, E)\n",
    "        rows = torch.arange(gating_flat.size(0), device=device).unsqueeze(1).expand(-1, self.k)  # (B*S, k)\n",
    "        gating_flat.scatter_(1, flat_idx, flat_w)\n",
    "        gating = gating_flat.view(B, S, self.num_experts)  # (B,S,E)\n",
    "\n",
    "        # Combine experts: out[b,s,:] = sum_e gating[b,s,e] * expert_stack[b,s,e,:]\n",
    "        out = torch.einsum('bse,bseh->bsh', gating, expert_stack)  # (B,S,H)\n",
    "\n",
    "        return out, aux_loss\n",
    "\n",
    "# -------------------------\n",
    "# Transformer encoder (unchanged logic)\n",
    "# -------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        # Replace ffn with MoE module\n",
    "        self.ffn_moe = MoE(hidden_size, ffn_dim, num_experts=moe_experts, k=moe_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        # MoE FFN\n",
    "        ffn_out, aux_loss = self.ffn_moe(x, mask)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x, aux_loss\n",
    "\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_position_embeddings=512, pad_token_id=0, embedding_weights=None, moe_experts=5, moe_k=2):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, ffn_dim, dropout=DROPOUT, moe_experts=moe_experts, moe_k=moe_k) for _ in range(num_layers)])\n",
    "        self.nsp_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 2))\n",
    "        self.mlm_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = self.token_embeddings(ids) + self.position_embeddings(pos) + self.segment_embeddings(tt)\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "        total_aux = 0.0\n",
    "        for layer in self.layers:\n",
    "            x, aux = layer(x, mask)\n",
    "            total_aux = total_aux + aux\n",
    "        return x, total_aux\n",
    "    def forward(self, ids, tt=None, mask=None):\n",
    "        seq_out, total_aux = self.encode(ids, tt, mask)\n",
    "        pooled = seq_out[:, 0]\n",
    "        nsp_logits = self.nsp_classifier(pooled)\n",
    "        mlm_logits = F.linear(seq_out, self.token_embeddings.weight, self.mlm_bias)\n",
    "        return mlm_logits, nsp_logits, total_aux\n",
    "\n",
    "# -------------------------\n",
    "# Training / evaluation helpers (refactors of your loop)\n",
    "# -------------------------\n",
    "def train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                    mlm_loss_fct, nsp_loss_fct, pad_id: int, mask_id: int, vocab_size: int, device: torch.device,\n",
    "                    aux_coeff: float):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    for batch in dataloader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        tts = batch[\"token_type_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        nsp_labels = batch[\"nsp_labels\"].to(device)\n",
    "        btypes = batch[\"batch_type\"]\n",
    "        ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "        ids_masked, mlm_labels = ids_masked.to(device), mlm_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "        mlm_loss = mlm_loss_fct(mlm_logits.view(-1, mlm_logits.size(-1)), mlm_labels.view(-1))\n",
    "        if all(bt == \"C\" for bt in btypes):\n",
    "            nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "        else:\n",
    "            nsp_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_steps += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_steps)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, pad_id: int, mask_id: int, vocab_size: int, device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    aux_coeff = 0.01\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            tts = batch[\"token_type_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            nsp_labels = batch[\"nsp_labels\"].to(device)\n",
    "            btypes = batch[\"batch_type\"]\n",
    "            ids_masked, mlm_labels = create_mlm_labels_and_masked_input(ids, pad_id, mask_id, vocab_size)\n",
    "            ids_masked, mlm_labels = ids_masked.to(device), mlm_labels.to(device)\n",
    "\n",
    "            mlm_logits, nsp_logits, aux_loss = model(ids_masked, tts, mask)\n",
    "            mlm_loss = mlm_loss_fct(mlm_logits.view(-1, mlm_logits.size(-1)), mlm_labels.view(-1))\n",
    "            if all(bt == \"C\" for bt in btypes):\n",
    "                nsp_loss = nsp_loss_fct(nsp_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            else:\n",
    "                nsp_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = mlm_loss + nsp_loss + aux_coeff * aux_loss\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_steps)\n",
    "    return avg_loss\n",
    "\n",
    "# -------------------------\n",
    "# Helper: build data, vocab, embeddings, dataloaders given hyperparams\n",
    "# -------------------------\n",
    "def prepare_data_and_model(corpus: List[Tuple[str, str]],\n",
    "                           stoi_override: Dict[str,int]=None,\n",
    "                           hyperparams: Dict[str, Any]=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      stoi, itos, vocab_size, emb (tensor), pad_id, mask_id, ds (dataset)\n",
    "    hyperparams may alter word2vec_window, WORD2VEC_SIZE, WORD2VEC_MIN_COUNT, MAX_SEQ_LEN, etc.\n",
    "    \"\"\"\n",
    "    # Unpack hyperparams or use defaults\n",
    "    hv = hyperparams or {}\n",
    "    word2vec_window = int(hv.get(\"word2vec_window\", WORD2VEC_WINDOW))\n",
    "    word2vec_size = int(hv.get(\"word2vec_size\", WORD2VEC_SIZE))\n",
    "    word2vec_min_count = int(hv.get(\"word2vec_min_count\", WORD2VEC_MIN_COUNT))\n",
    "    max_seq_len = int(hv.get(\"max_seq_len\", MAX_SEQ_LEN))\n",
    "\n",
    "    # Build vocab (we'll build from corpus text)\n",
    "    texts = [x[0] for x in corpus]\n",
    "    stoi, itos = build_vocab(texts, min_freq=VOCAB_MIN_FREQ)\n",
    "    vocab_size = len(itos)\n",
    "\n",
    "    # Word2Vec train\n",
    "    w2v = train_word2vec(texts, vector_size=word2vec_size, window=word2vec_window, min_count=word2vec_min_count, epochs=5)\n",
    "    emb = build_embedding_matrix(w2v, itos, HIDDEN_SIZE)\n",
    "    pad_id = stoi[PAD_TOKEN]; mask_id = stoi[MASK_TOKEN]\n",
    "    ds = BertPretrainingDataset(corpus, stoi, max_seq_len=max_seq_len)\n",
    "    return stoi, itos, vocab_size, emb, pad_id, mask_id, ds\n",
    "\n",
    "# -------------------------\n",
    "# BO helpers: search space sampling, GP + EI\n",
    "# -------------------------\n",
    "def sample_random_hyperparams():\n",
    "    \"\"\"\n",
    "    Sample hyperparameters in the agreed search space.\n",
    "    \"\"\"\n",
    "    sample = {}\n",
    "    # learning_rate ∈ [1e-6, 5e-5] (log-uniform)\n",
    "    lr = 10 ** np.random.uniform(np.log10(1e-6), np.log10(5e-5))\n",
    "    sample[\"learning_rate\"] = float(lr)\n",
    "\n",
    "    # moe_experts ∈ {3,4,5,6}\n",
    "    sample[\"moe_experts\"] = int(np.random.choice([3,4,5,6]))\n",
    "\n",
    "    # moe_k ∈ {1,2}\n",
    "    sample[\"moe_k\"] = int(np.random.choice([1,2]))\n",
    "\n",
    "    # ffn_dim ∈ [1024,4096] (discrete set multiples of 256)\n",
    "    sample[\"ffn_dim\"] = int(int(np.random.choice(np.arange(1024, 4097, 256))))\n",
    "\n",
    "    # num_layers ∈ {6,8,12}\n",
    "    sample[\"num_layers\"] = int(np.random.choice([6,8,12]))\n",
    "\n",
    "    # num_heads ∈ {6,8,12}\n",
    "    sample[\"num_heads\"] = int(np.random.choice([6,8,12]))\n",
    "\n",
    "    # word2vec_window ∈ {3,5,7}\n",
    "    sample[\"word2vec_window\"] = int(np.random.choice([3,5,7]))\n",
    "\n",
    "    # mlm_mask_prob ∈ [0.10,0.20]\n",
    "    sample[\"mlm_mask_prob\"] = float(np.random.uniform(0.10, 0.20))\n",
    "\n",
    "    return sample\n",
    "\n",
    "def hyperparams_to_vector(hp: Dict[str,Any]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert hyperparameter dict to numeric vector for GP.\n",
    "    We'll use a consistent ordering:\n",
    "    [log_lr, moe_experts, moe_k, ffn_dim/256, num_layers, num_heads, word2vec_window, mlm_mask_prob]\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    vec.append(np.log10(hp[\"learning_rate\"]))\n",
    "    vec.append(float(hp[\"moe_experts\"]))\n",
    "    vec.append(float(hp[\"moe_k\"]))\n",
    "    vec.append(float(hp[\"ffn_dim\"]) / 256.0)\n",
    "    vec.append(float(hp[\"num_layers\"]))\n",
    "    vec.append(float(hp[\"num_heads\"]))\n",
    "    vec.append(float(hp[\"word2vec_window\"]))\n",
    "    vec.append(float(hp[\"mlm_mask_prob\"]))\n",
    "    return np.array(vec, dtype=float)\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, sigma: np.ndarray, best: float, xi: float = 0.01):\n",
    "    \"\"\"\n",
    "    Expected Improvement (for minimization: improvement = best - f).\n",
    "    mu, sigma: arrays of shape (n_candidates,)\n",
    "    best: current best (lower is better)\n",
    "    We compute EI = E[max(best - f, 0)] where f ~ N(mu, sigma^2)\n",
    "    \"\"\"\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "    imp = best - mu - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma == 0.0] = 0.0\n",
    "    return ei\n",
    "\n",
    "# -------------------------\n",
    "# Objective function for BO\n",
    "# -------------------------\n",
    "def objective_function(hp: Dict[str,Any],\n",
    "                       corpus: List[Tuple[str,str]],\n",
    "                       run_id: int,\n",
    "                       verbose: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Given hyperparameters, train the model for TRIAL_EPOCHS and return the validation total loss.\n",
    "    This function:\n",
    "      - Prepares data + embeddings (word2vec) depending on hp\n",
    "      - Builds model with hp architecture\n",
    "      - Trains for TRIAL_EPOCHS\n",
    "      - Evaluates on validation split and returns avg total loss (MLM + NSP + aux)\n",
    "    \"\"\"\n",
    "    # Prepare data and model\n",
    "    stoi, itos, vocab_size, emb, pad_id, mask_id, ds = prepare_data_and_model(corpus, hyperparams=hp)\n",
    "\n",
    "    # split dataset\n",
    "    total_len = len(ds)\n",
    "    if total_len < 2:\n",
    "        # fallback: use full dataset as train and val same (rare with tiny test corpus)\n",
    "        train_ds = ds\n",
    "        val_ds = ds\n",
    "    else:\n",
    "        test_len = max(1, total_len // 5)\n",
    "        train_len = total_len - test_len\n",
    "        train_ds, val_ds = random_split(ds, [train_len, test_len])\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = int(hp.get(\"batch_size\", BATCH_SIZE))\n",
    "    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    dl_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "    # Build model with hyperparameters\n",
    "    hidden_size = HIDDEN_SIZE  # keep same hidden size to avoid dimension mismatch in embedding\n",
    "    model = BertEncoderModel(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=int(hp[\"num_layers\"]),\n",
    "        num_heads=int(hp[\"num_heads\"]),\n",
    "        ffn_dim=int(hp[\"ffn_dim\"]),\n",
    "        max_position_embeddings=MAX_SEQ_LEN,\n",
    "        pad_token_id=pad_id,\n",
    "        embedding_weights=emb,\n",
    "        moe_experts=int(hp[\"moe_experts\"]),\n",
    "        moe_k=int(hp[\"moe_k\"])\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Losses and optimizer\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    lr = float(hp[\"learning_rate\"])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # override global MLM mask prob for the create_mlm labels function usage\n",
    "    # We will pass the value in calls to create_mlm_labels_and_masked_input by monkey-patching default global variable used there.\n",
    "    global MLM_MASK_PROB\n",
    "    prev_mlm_mask_prob = MLM_MASK_PROB\n",
    "    MLM_MASK_PROB = float(hp.get(\"mlm_mask_prob\", MLM_MASK_PROB))\n",
    "\n",
    "    aux_coeff = 0.01  # keep same scaling as before\n",
    "\n",
    "    try:\n",
    "        # Train for TRIAL_EPOCHS\n",
    "        for epoch in range(TRIAL_EPOCHS):\n",
    "            train_loss = train_one_epoch(model, dl_train, optimizer, mlm_loss_fct, nsp_loss_fct,\n",
    "                                         pad_id, mask_id, vocab_size, DEVICE, aux_coeff)\n",
    "            if verbose:\n",
    "                print(f\"[Run {run_id}] Epoch {epoch+1}/{TRIAL_EPOCHS} - train_loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Evaluate on validation\n",
    "        val_loss = evaluate_model(model, dl_val, pad_id, mask_id, vocab_size, DEVICE)\n",
    "        if verbose:\n",
    "            print(f\"[Run {run_id}] Validation loss: {val_loss:.6f}\")\n",
    "\n",
    "    finally:\n",
    "        # restore MLM_MASK_PROB global\n",
    "        MLM_MASK_PROB = prev_mlm_mask_prob\n",
    "\n",
    "    # free memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return float(val_loss)\n",
    "\n",
    "# -------------------------\n",
    "# Main: Bayesian Optimization loop\n",
    "# -------------------------\n",
    "def run_bayesian_optimization(corpus: List[Tuple[str,str]]):\n",
    "    # records\n",
    "    bo_records = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # initial random evaluations\n",
    "    print(\"[BO] Starting initial random evaluations...\")\n",
    "    for i in range(BO_INIT_POINTS):\n",
    "        hp = sample_random_hyperparams()\n",
    "        # also include batch_size and maybe word2vec_size for completeness\n",
    "        hp[\"batch_size\"] = BATCH_SIZE\n",
    "        hp[\"word2vec_size\"] = WORD2VEC_SIZE\n",
    "        hp[\"word2vec_min_count\"] = WORD2VEC_MIN_COUNT\n",
    "        print(f\"[BO] Init sample {i+1}/{BO_INIT_POINTS}: {hp}\")\n",
    "        loss_val = objective_function(hp, corpus, run_id=i+1, verbose=True)\n",
    "        vec = hyperparams_to_vector(hp)\n",
    "        X.append(vec)\n",
    "        y.append(loss_val)\n",
    "        rec = {\"iteration\": i+1, \"params\": hp, \"loss\": float(loss_val)}\n",
    "        bo_records.append(rec)\n",
    "        # save intermediate logs\n",
    "        with open(os.path.join(BO_LOG_DIR, \"bo_results.json\"), \"w\") as f:\n",
    "            json.dump(bo_records, f, indent=2)\n",
    "\n",
    "    # GP surrogate\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(X[0].shape[0]), nu=2.5) + WhiteKernel(noise_level=1e-6)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=3, random_state=42)\n",
    "\n",
    "    # main BO iterations\n",
    "    for it in range(BO_INIT_POINTS, BO_ITERATIONS):\n",
    "        print(f\"[BO] Iteration {it+1}/{BO_ITERATIONS}\")\n",
    "\n",
    "        # fit GP on current data\n",
    "        X_arr = np.vstack(X)\n",
    "        y_arr = np.array(y)\n",
    "        gp.fit(X_arr, y_arr)\n",
    "\n",
    "        # propose many random candidates and pick one that maximizes EI\n",
    "        n_candidates = 200\n",
    "        candidates = []\n",
    "        cand_vecs = []\n",
    "        for _ in range(n_candidates):\n",
    "            c = sample_random_hyperparams()\n",
    "            c[\"batch_size\"] = BATCH_SIZE\n",
    "            c[\"word2vec_size\"] = WORD2VEC_SIZE\n",
    "            c[\"word2vec_min_count\"] = WORD2VEC_MIN_COUNT\n",
    "            candidates.append(c)\n",
    "            cand_vecs.append(hyperparams_to_vector(c))\n",
    "        cand_vecs = np.vstack(cand_vecs)\n",
    "\n",
    "        mu, sigma = gp.predict(cand_vecs, return_std=True)\n",
    "        best_so_far = np.min(y_arr)\n",
    "        eis = expected_improvement(mu, sigma, best_so_far, xi=0.01)\n",
    "        best_idx = int(np.argmax(eis))\n",
    "        chosen_hp = candidates[best_idx]\n",
    "        print(f\"[BO] Chosen hyperparams (EI max): {chosen_hp}\")\n",
    "\n",
    "        # evaluate chosen configuration\n",
    "        loss_val = objective_function(chosen_hp, corpus, run_id=it+1, verbose=True)\n",
    "        vec = hyperparams_to_vector(chosen_hp)\n",
    "        X.append(vec)\n",
    "        y.append(loss_val)\n",
    "\n",
    "        rec = {\"iteration\": it+1, \"params\": chosen_hp, \"loss\": float(loss_val)}\n",
    "        bo_records.append(rec)\n",
    "        # save logs\n",
    "        with open(os.path.join(BO_LOG_DIR, \"bo_results.json\"), \"w\") as f:\n",
    "            json.dump(bo_records, f, indent=2)\n",
    "\n",
    "    # compute best config\n",
    "    losses = [r[\"loss\"] for r in bo_records]\n",
    "    best_idx = int(np.argmin(losses))\n",
    "    best_record = bo_records[best_idx]\n",
    "    best_hp = best_record[\"params\"]\n",
    "    best_loss = best_record[\"loss\"]\n",
    "    print(f\"[BO] Best config found: {best_hp} with loss {best_loss:.6f}\")\n",
    "\n",
    "    # save best config to file\n",
    "    with open(os.path.join(BO_LOG_DIR, \"best_config.json\"), \"w\") as f:\n",
    "        json.dump({\"best_params\": best_hp, \"best_loss\": best_loss}, f, indent=2)\n",
    "\n",
    "    # Retrain final model on full data with best hyperparams (optional)\n",
    "    print(\"[BO] Training final model on full dataset with best hyperparameters...\")\n",
    "    # Prepare final data and model\n",
    "    stoi, itos, vocab_size, emb, pad_id, mask_id, ds = prepare_data_and_model(corpus, hyperparams=best_hp)\n",
    "    dl_full = DataLoader(ds, batch_size=int(best_hp.get(\"batch_size\", BATCH_SIZE)), shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "    final_model = BertEncoderModel(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=int(best_hp[\"num_layers\"]),\n",
    "        num_heads=int(best_hp[\"num_heads\"]),\n",
    "        ffn_dim=int(best_hp[\"ffn_dim\"]),\n",
    "        max_position_embeddings=MAX_SEQ_LEN,\n",
    "        pad_token_id=pad_id,\n",
    "        embedding_weights=emb,\n",
    "        moe_experts=int(best_hp[\"moe_experts\"]),\n",
    "        moe_k=int(best_hp[\"moe_k\"])\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # optimizer\n",
    "    lr = float(best_hp[\"learning_rate\"])\n",
    "    optimizer = torch.optim.AdamW(final_model.parameters(), lr=lr)\n",
    "    mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    nsp_loss_fct = nn.CrossEntropyLoss()\n",
    "    # set global MLM mask prob to chosen\n",
    "    global MLM_MASK_PROB\n",
    "    prev_mlm_mask_prob = MLM_MASK_PROB\n",
    "    MLM_MASK_PROB = float(best_hp.get(\"mlm_mask_prob\", MLM_MASK_PROB))\n",
    "    aux_coeff = 0.01\n",
    "\n",
    "    try:\n",
    "        # train final for TRIAL_EPOCHS (you can increase if desired)\n",
    "        for epoch in range(TRIAL_EPOCHS):\n",
    "            t_loss = train_one_epoch(final_model, dl_full, optimizer, mlm_loss_fct, nsp_loss_fct, pad_id, mask_id, vocab_size, DEVICE, aux_coeff)\n",
    "            print(f\"[Final Train] Epoch {epoch+1}/{TRIAL_EPOCHS} - loss {t_loss:.6f}\")\n",
    "    finally:\n",
    "        MLM_MASK_PROB = prev_mlm_mask_prob\n",
    "\n",
    "    # save final model and vocab into bo_logs/best_model/\n",
    "    save_dir = os.path.join(BO_LOG_DIR, \"best_model\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(final_model.state_dict(), os.path.join(save_dir, \"bert_encoder.pt\"))\n",
    "    with open(os.path.join(save_dir, \"vocab.json\"), \"w\") as f:\n",
    "        json.dump({\"stoi\": stoi, \"itos\": itos}, f)\n",
    "    print(f\"[BO] Best model and vocab saved to {save_dir}\")\n",
    "\n",
    "    return best_record\n",
    "\n",
    "# -------------------------\n",
    "# Example corpus (keeps your original small corpus)\n",
    "# -------------------------\n",
    "def get_default_corpus():\n",
    "    return [\n",
    "        (\"the quick brown fox jumps over the lazy dog. the dog did not mind.\", \"C\"),\n",
    "        (\"i love machine learning and transformers.\", \"Q\"),\n",
    "        (\"deep learning enables summarization and translation. it is powerful.\", \"C\"),\n",
    "        (\"best restaurants near me\", \"Q\")\n",
    "    ]\n",
    "\n",
    "# -------------------------\n",
    "# Entrypoint\n",
    "# -------------------------\n",
    "def main():\n",
    "    corpus = get_default_corpus()\n",
    "    print(\"[MAIN] Starting Bayesian Optimization over hyperparameters...\")\n",
    "    best = run_bayesian_optimization(corpus)\n",
    "    print(\"[MAIN] BO Completed. Best record:\")\n",
    "    print(json.dumps(best, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
