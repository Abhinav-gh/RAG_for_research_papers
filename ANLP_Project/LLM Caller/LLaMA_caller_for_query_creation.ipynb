{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== INSTALL LLaMA + TRANSFORMERS + ACCELERATE ==================\n",
    "# Run this in a separate cell before running the main code.\n",
    "\n",
    "# ================== DOWNLOAD / LOAD LLaMA MODEL LOCALLY ==================\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Example: LLaMA-3.1 8B Instruct (open-source)\n",
    "# REQUIREMENTS: ~16GB GPU or use CPU (very slow)\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(\"[INFO] Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"[INFO] Loading model (this may take a while)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"     # automatically uses GPU if available\n",
    ")\n",
    "\n",
    "print(\"[INFO] Local LLaMA model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58142ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Metadatas\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "ABSOLUTE_DB_PATH = \"../VectorDB/chroma_Data\"\n",
    "COLLECTION_NAME = \"harry_potter_collection\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_QUERIES_PER_CHUNK = 5\n",
    "\n",
    "# Local model reference (must match the model loaded in the setup cell)\n",
    "LOCAL_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "OUTPUT_CSV = \"generated_pairs.csv\"\n",
    "\n",
    "# Manual rate limits (kept unchanged)\n",
    "MAX_RPM = 15\n",
    "MAX_RPD = 200\n",
    "SECONDS_PER_REQUEST = 60 / MAX_RPM\n",
    "\n",
    "# -------------------- Load Local LLaMA Model --------------------\n",
    "print(\"[INFO] Loading tokenizer and model...\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ").to(device)\n",
    "\n",
    "# -------------------- Token-limit enforcement --------------------\n",
    "def truncate_to_token_limit(text: str, max_tokens: int = 100000):\n",
    "    \"\"\"\n",
    "    Ensures we never exceed LLaMAâ€™s 128k context window.\n",
    "    100k is chosen to allow room for instructions/examples.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "# -------------------- Query generation --------------------\n",
    "def ask_llama_local(chunk_text: str, chunk_id: str, num_queries: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate queries using local LLaMA model.\n",
    "    Now uses safe token truncation.\n",
    "    \"\"\"\n",
    "\n",
    "    safe_chunk = truncate_to_token_limit(chunk_text)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI that generates realistic search queries a user might input to an LLM or search system.\n",
    "Each query should be short, relevant, and reflect what someone might actually ask.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Chunk: \"Harry receives his first letter from Hogwarts, but Uncle Vernon tries to stop him.\"\n",
    "Queries:\n",
    "- \"How did Harry get his Hogwarts letter?\"\n",
    "- \"Why did Uncle Vernon hide Harry's letter?\"\n",
    "- \"First Hogwarts letter incident\"\n",
    "\n",
    "Example 2:\n",
    "Chunk: \"Hagrid visits Harry to explain that he is a wizard.\"\n",
    "Queries:\n",
    "- \"Who is Hagrid and why did he visit Harry?\"\n",
    "- \"How did Harry find out he is a wizard?\"\n",
    "- \"Hagrid tells Harry he's a wizard\"\n",
    "\n",
    "Now, generate {num_queries} short user queries for the following chunk:\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk Text: \"{safe_chunk}\"\n",
    "Queries:\n",
    "-\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract lines after \"Queries:\"\n",
    "    if \"Queries:\" in text:\n",
    "        text = text.split(\"Queries:\")[1]\n",
    "\n",
    "    queries = [q.strip(\"- \").strip() for q in text.split(\"\\n\") if q.strip()]\n",
    "    return queries[:num_queries]\n",
    "\n",
    "\n",
    "# -------------------- Main workflow --------------------\n",
    "def main():\n",
    "    client_db = chromadb.PersistentClient(path=ABSOLUTE_DB_PATH)\n",
    "    print(f\"[INFO] ChromaDB client initialized at: {ABSOLUTE_DB_PATH}\")\n",
    "\n",
    "    collection = client_db.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"[INFO] Using existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "    chunks = [\n",
    "        {\"id\": meta[\"id\"], \"text\": doc}\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])\n",
    "        if meta.get(\"ischunk\") is True\n",
    "    ]\n",
    "    print(f\"[INFO] Found {len(chunks)} chunks (ischunk=True)\")\n",
    "\n",
    "    all_pairs = []\n",
    "    total_requests_today = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunk-batches\"):\n",
    "        batch = chunks[i: i + BATCH_SIZE]\n",
    "\n",
    "        for chunk in batch:\n",
    "            if total_requests_today >= MAX_RPD:\n",
    "                print(f\"[INFO] Reached daily limit of {MAX_RPD} requests. Stopping.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                queries = ask_llama_local(\n",
    "                    chunk[\"text\"],\n",
    "                    chunk[\"id\"],\n",
    "                    NUM_QUERIES_PER_CHUNK\n",
    "                )\n",
    "                total_requests_today += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to generate for chunk {chunk['id']}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for q in queries:\n",
    "                all_pairs.append({\"query\": q, \"chunk_id\": chunk[\"id\"]})\n",
    "\n",
    "            time.sleep(SECONDS_PER_REQUEST)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[INFO] Saved {len(df)} query-chunk pairs to {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
