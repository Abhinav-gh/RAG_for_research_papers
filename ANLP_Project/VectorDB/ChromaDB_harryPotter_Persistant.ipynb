{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"harry_potter_collection\"\n",
    "# Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: /home/cadencea12/Documents/ANLP_Project/VectorDB/chroma_Data\n",
      "Existing collections: ['chunks_collection', 'harry_potter_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4014 chunks from '../Chunking/Chunk_files/harry_potter_chunks_semantic.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../harrypotter.pdf', 'page_number': 14, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized with model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "# !pip install sentence_transformers\n",
    "\n",
    "# Set up embedding function\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding function initialized with model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'harry_potter_collection' created or accessed successfully\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Get or create the collection\n",
    "client.delete_collection(name=collection_name)  \n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter book chunks\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created or accessed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch: 0 to 499 (500 items)\n",
      "Added batch: 500 to 999 (500 items)\n",
      "Added batch: 1000 to 1499 (500 items)\n",
      "Added batch: 1500 to 1999 (500 items)\n",
      "Added batch: 2000 to 2499 (500 items)\n",
      "Added batch: 2500 to 2999 (500 items)\n",
      "Added batch: 3000 to 3499 (500 items)\n",
      "Added batch: 3500 to 3999 (500 items)\n",
      "Added batch: 4000 to 4013 (14 items)\n",
      "Successfully added 4014 documents to collection 'harry_potter_collection'\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a unique ID (you could use a more deterministic approach if needed)\n",
    "    doc_id = f\"hp_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata and add the generated ID\n",
    "    metadata = dict(doc.metadata)  # copy original metadata to avoid modifying source\n",
    "    metadata[\"id\"] = doc_id  # Add the generated ID into metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"Added batch: {i} to {end_idx-1} ({end_idx-i} items)\")\n",
    "\n",
    "print(f\"Successfully added {total_added} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 4014\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: hp_chunk_0\n",
      "Text: . yes, that would be it. The traffic moved on and a few minutes\n",
      "later, Mr. Dursley arrived in the Gr...\n",
      "Metadata: {'ischunk': True, 'c': 'semantic', 'page_number': 14, 'id': 'hp_chunk_0', 'source': '../harrypotter.pdf'}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: hp_chunk_1\n",
      "Text: . Mrs. Dursley came into the living room carrying two cups of tea. It was no\n",
      "good. He‚Äôd have to say ...\n",
      "Metadata: {'page_number': 16, 'ischunk': True, 'id': 'hp_chunk_1', 'c': 'semantic', 'source': '../harrypotter.pdf'}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: hp_chunk_2\n",
      "Text: . and there were a lot of funny-looking people in town today . . .‚Äù\n",
      "‚ÄúSo?‚Äù snapped Mrs. Dursley. ‚ÄúWel...\n",
      "Metadata: {'ischunk': True, 'c': 'semantic', 'source': '../harrypotter.pdf', 'id': 'hp_chunk_2', 'page_number': 16}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5820e",
   "metadata": {},
   "source": [
    "### Querying the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88654e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich table for displaying results (optional but nice)\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    console = Console()\n",
    "    use_rich = True\n",
    "except ImportError:\n",
    "    use_rich = False\n",
    "    print(\"Rich package not found. Using standard print.\")\n",
    "\n",
    "# Function to display query results\n",
    "def print_results(results, use_rich=use_rich):\n",
    "    if use_rich:\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Rank\", width=6)\n",
    "        table.add_column(\"Document ID\")\n",
    "        table.add_column(\"Document Text\", width=60)\n",
    "        table.add_column(\"Page\")\n",
    "        table.add_column(\"Distance\")\n",
    "        \n",
    "        docs = results['documents'][0]\n",
    "        ids = results['ids'][0]\n",
    "        metas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        for i, (doc, doc_id, meta, dist) in enumerate(zip(docs, ids, metas, distances)):\n",
    "            table.add_row(\n",
    "                str(i+1),\n",
    "                doc_id,\n",
    "                (doc[:100] + \"...\") if len(doc) > 100 else doc,\n",
    "                str(meta.get('page_number', 'N/A')),\n",
    "                f\"{dist:.4f}\"\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    else:\n",
    "        # Standard print version\n",
    "        for i, (doc, meta, dist) in enumerate(zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            print(f\"\\n--- Result {i+1} ---\")\n",
    "            print(f\"Text: {doc[:100]}...\")\n",
    "            print(f\"Metadata: {meta}\")\n",
    "            print(f\"Distance: {dist:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query\n",
    "query = \"Who was Dumbledore? When was he first introduced?\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nResults for query: '{query}'\")\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62da64",
   "metadata": {},
   "source": [
    "### Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Initialize Gemini (fixed the model name - using a valid Gemini model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbee757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Better prompt\n",
    "rag_prompt_template = \"\"\"\n",
    "You are an expert on Harry Potter books. Answer questions using ONLY the context below.\n",
    "If you can't find a complete answer in the context but see partial information, try to provide what you can find and acknowledge the limitations of the available information.\n",
    "If there is NO relevant information at all in the context, respond with \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context provided):\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def answer_with_hybrid_rag(query, n_results=5):\n",
    "    # 1. Semantic search with ChromaDB\n",
    "    semantic_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Perform keyword search with BM25\n",
    "    # First get all documents to search across\n",
    "    all_docs = collection.get(\n",
    "        limit=100,  # Adjust based on your collection size\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # Tokenize for BM25\n",
    "    tokenized_docs = [doc.split() for doc in all_docs[\"documents\"]]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top BM25 results\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[-n_results:][::-1]\n",
    "    \n",
    "    # 3. Combine results (simple union)\n",
    "    combined_docs = []\n",
    "    combined_meta = []\n",
    "    combined_ids = [] \n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Add semantic results\n",
    "    for doc, meta, doc_id in zip(\n",
    "        semantic_results[\"documents\"][0], \n",
    "        semantic_results[\"metadatas\"][0],\n",
    "        semantic_results[\"ids\"][0]\n",
    "    ):\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(doc)\n",
    "            combined_meta.append(meta)\n",
    "            combined_ids.append(doc_id)  # Store the id\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Add keyword results\n",
    "    for idx in top_bm25_indices:\n",
    "        doc_id = all_docs[\"ids\"][idx]\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(all_docs[\"documents\"][idx])\n",
    "            combined_meta.append(all_docs[\"metadatas\"][idx])\n",
    "            combined_ids.append(doc_id)  # Store the id\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Limit to n_results total\n",
    "    combined_docs = combined_docs[:n_results]\n",
    "    combined_meta = combined_meta[:n_results]\n",
    "    combined_ids = combined_ids[:n_results]\n",
    "    \n",
    "    # Format context and complete RAG as before\n",
    "    formatted_docs = []\n",
    "    for doc, meta in zip(combined_docs, combined_meta):\n",
    "        page_num = meta.get(\"page_number\", \"unknown\")\n",
    "        formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "    filled_prompt = prompt.format(context=context, query=query)\n",
    "    response = llm.invoke(filled_prompt)\n",
    "    \n",
    "    # Create a mock results object for print_results compatibility\n",
    "    mock_results = {\n",
    "        \"documents\": [combined_docs],\n",
    "        \"metadatas\": [combined_meta],\n",
    "        \"distances\": [[0.0] * len(combined_docs)],  # Placeholder distances\n",
    "        \"ids\": [combined_ids]  # Add this line\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content if hasattr(response, 'content') else str(response),\n",
    "        \"source_documents\": mock_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0841a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our RAG pipeline with a question\n",
    "test_query = \"What happened when Harry first met Hagrid?\"\n",
    "response = answer_with_hybrid_rag(test_query)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nAnswer: {response['answer']}\")\n",
    "print(\"\\nSources:\")\n",
    "print_results(response[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45035af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple questions to evaluate system\n",
    "results_for_export = []\n",
    "\n",
    "test_questions = [\n",
    "    \"Who is Voldemort and why is he feared?\",\n",
    "    \"What are the four houses at Hogwarts?\",\n",
    "    \"How did Harry survive the killing curse as a baby?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Question: {question}\")\n",
    "    response = answer_with_hybrid_rag(question)\n",
    "    print(f\"\\nAnswer: {response['answer']}\")\n",
    "    print(\"\\nTop source:\")\n",
    "    if len(response[\"source_documents\"][\"documents\"][0]) > 0:\n",
    "        top_doc = response[\"source_documents\"][\"documents\"][0][0]\n",
    "        top_meta = response[\"source_documents\"][\"metadatas\"][0][0]\n",
    "        page = top_meta.get(\"page_number\", \"N/A\")\n",
    "        print(f\"[Page {page}]:\\n{top_doc}\")  # Print full chunk\n",
    "        # Save for export\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": page,\n",
    "            \"chunk\": top_doc\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sources found.\")\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": None,\n",
    "            \"chunk\": None\n",
    "        })\n",
    "\n",
    "# Export results to a well-formatted text file\n",
    "with open(multiquery_rag_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"RAG Multi-Query Evaluation Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    for idx, res in enumerate(results_for_export, 1):\n",
    "        f.write(f\"Question {idx}: {res['question']}\\n\")\n",
    "        f.write(f\"Answer:\\n{res['answer']}\\n\\n\")\n",
    "        if res[\"chunk\"]:\n",
    "            f.write(f\"Top Source Chunk (Page {res['page']}):\\n{res['chunk']}\\n\")\n",
    "        else:\n",
    "            f.write(\"Top Source Chunk: No sources found.\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\\n\")\n",
    "print(f\"\\nResults exported to {multiquery_rag_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ef32c",
   "metadata": {},
   "source": [
    "### Creating a User Interface with Gradio\n",
    "This will create a simple web interface for users to query the Harry Potter RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio for the web interface\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load the custom encoder model\n",
    "custom_model_loaded = False\n",
    "custom_encoder = None\n",
    "try:\n",
    "    # Path to the encoder model\n",
    "    model_path = \"../Encoder/model.pkl\"  # Adjust path as needed\n",
    "\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        custom_encoder = pickle.load(f)\n",
    "    custom_model_loaded = True\n",
    "    print(f\"Successfully loaded custom encoder model from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load custom encoder model: {e}\")\n",
    "    print(\"Will use ChromaDB's default embedding function if custom model is selected.\")\n",
    "\n",
    "# Helper: get embedding using the custom encoder (robust)\n",
    "def get_embedding_with_custom_model(text):\n",
    "    \"\"\"Generate embedding using the custom encoder model\"\"\"\n",
    "    if custom_model_loaded and custom_encoder is not None:\n",
    "        try:\n",
    "            # Try common APIs: custom_encoder.encode([...]) or custom_encoder.transform([...])\n",
    "            if hasattr(custom_encoder, \"encode\"):\n",
    "                emb = custom_encoder.encode([text])\n",
    "            elif hasattr(custom_encoder, \"transform\"):\n",
    "                emb = custom_encoder.transform([text])\n",
    "            else:\n",
    "                # Try calling directly\n",
    "                emb = custom_encoder([text])\n",
    "            # emb might be shape (1, D) numpy or list\n",
    "            emb0 = emb[0]\n",
    "            # convert to plain python list of floats\n",
    "            if isinstance(emb0, np.ndarray):\n",
    "                return emb0.tolist()\n",
    "            else:\n",
    "                # convert nested types to floats\n",
    "                return [float(x) for x in emb0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error using custom encoder: {e}\")\n",
    "            # fallback to Chroma's embedding if available\n",
    "            try:\n",
    "                return embedding_function([text])[0]\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"Custom encoder failed and no embedding_function fallback available.\")\n",
    "    else:\n",
    "        # Fallback to ChromaDB's embedding function if available\n",
    "        try:\n",
    "            return embedding_function([text])[0]\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"No custom encoder loaded and embedding_function is not available.\")\n",
    "\n",
    "# Primary RAG pipeline function\n",
    "def rag_query(query, top_k=3, use_hybrid_search=False, use_custom_encoder=False):\n",
    "    \"\"\"\n",
    "    Process a query through the RAG pipeline and return:\n",
    "      - answer (str)\n",
    "      - source chunks (str)\n",
    "      - elapsed time in seconds (str)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # If using hybrid search, prefer your existing answer_with_hybrid_rag function\n",
    "        if use_hybrid_search:\n",
    "            # If hybrid uses the collection internally, assume it handles embedder choice.\n",
    "            # If you want hybrid to use the custom encoder, modify answer_with_hybrid_rag accordingly.\n",
    "            response = answer_with_hybrid_rag(query, n_results=int(top_k))\n",
    "            answer = response.get('answer', \"\")\n",
    "            source_docs = response.get('source_documents', {\"documents\": [[]], \"metadatas\": [[]]})\n",
    "            # For backward compatibility in formatting below, wrap appropriately if necessary\n",
    "            results_obj = None\n",
    "        else:\n",
    "            # vector search branch: if custom encoder chosen, compute query embedding locally\n",
    "            if use_custom_encoder and custom_model_loaded:\n",
    "                try:\n",
    "                    query_embedding = get_embedding_with_custom_model(query)\n",
    "                except Exception as e:\n",
    "                    # fallback: if embedding fails, fall back to text-based query\n",
    "                    print(f\"Custom encoder embedding failed: {e}. Falling back to Chroma text embedding.\")\n",
    "                    query_embedding = None\n",
    "\n",
    "                if query_embedding is not None:\n",
    "                    results = collection.query(\n",
    "                        query_embeddings=[query_embedding],\n",
    "                        n_results=int(top_k),\n",
    "                        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                    )\n",
    "                else:\n",
    "                    # fallback to server-side embedding\n",
    "                    results = collection.query(\n",
    "                        query_texts=[query],\n",
    "                        n_results=int(top_k),\n",
    "                        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                    )\n",
    "            else:\n",
    "                # Use ChromaDB's default embedding function (server-side)\n",
    "                results = collection.query(\n",
    "                    query_texts=[query],\n",
    "                    n_results=int(top_k),\n",
    "                    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "                )\n",
    "\n",
    "            # Format context from retrieved chunks\n",
    "            documents = results[\"documents\"][0]\n",
    "            metadatas = results[\"metadatas\"][0]\n",
    "            distances = results.get(\"distances\", [[]])[0] if \"distances\" in results else [None] * len(documents)\n",
    "\n",
    "            # Format each document with its page number for better context\n",
    "            formatted_docs = []\n",
    "            for doc, meta, dist in zip(documents, metadatas, distances):\n",
    "                page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                if dist is not None:\n",
    "                    formatted_docs.append(f\"[Page {page_num}] (dist={dist:.4f}): {doc}\")\n",
    "                else:\n",
    "                    formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "\n",
    "            # join into context\n",
    "            context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "\n",
    "            # Fill the prompt template\n",
    "            try:\n",
    "                filled_prompt = prompt.format(context=context, query=query)\n",
    "            except Exception:\n",
    "                # fallback if prompt doesn't use named placeholders\n",
    "                filled_prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "            # Call the LLM\n",
    "            try:\n",
    "                response_obj = llm.invoke(filled_prompt)\n",
    "                answer = response_obj.content if hasattr(response_obj, 'content') else str(response_obj)\n",
    "            except Exception as e:\n",
    "                # If llm.invoke fails, surface the error\n",
    "                answer = f\"LLM invocation failed: {e}\"\n",
    "\n",
    "            source_docs = results\n",
    "\n",
    "        # Format source documents for display\n",
    "        sources_text = \"\"\n",
    "        # if source_docs is a dict produced by collection.query\n",
    "        if isinstance(source_docs, dict) and \"documents\" in source_docs and \"metadatas\" in source_docs:\n",
    "            docs_list = source_docs[\"documents\"][0]\n",
    "            meta_list = source_docs[\"metadatas\"][0]\n",
    "            for i, (doc, meta) in enumerate(zip(docs_list, meta_list)):\n",
    "                page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                sources_text += f\"\\n\\nSource {i+1} [Page {page_num}]:\\n{doc}\"\n",
    "        else:\n",
    "            # Try to format the results object if available\n",
    "            try:\n",
    "                docs_list = source_docs[\"documents\"][0]\n",
    "                meta_list = source_docs[\"metadatas\"][0]\n",
    "                for i, (doc, meta) in enumerate(zip(docs_list, meta_list)):\n",
    "                    page_num = meta.get(\"page_number\", \"unknown\") if isinstance(meta, dict) else \"unknown\"\n",
    "                    sources_text += f\"\\n\\nSource {i+1} [Page {page_num}]:\\n{doc}\"\n",
    "            except Exception:\n",
    "                # fallback textual representation\n",
    "                sources_text = str(source_docs)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        elapsed_str = f\"{elapsed:.3f} seconds\"\n",
    "\n",
    "        return answer, sources_text, elapsed_str\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        elapsed_str = f\"{elapsed:.3f} seconds\"\n",
    "        return f\"Error during processing: {e}\", \"\", elapsed_str\n",
    "\n",
    "\n",
    "# Create Gradio Interface\n",
    "with gr.Blocks(title=\"Harry Potter RAG System\") as demo:\n",
    "    gr.Markdown(\"# üßô‚Äç‚ôÇÔ∏è Harry Potter RAG System\")\n",
    "    gr.Markdown(\"Ask questions about the Harry Potter books and get answers based on the text.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"E.g., Who is Harry Potter and what happened to his parents?\",\n",
    "                lines=2\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            top_k = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                value=3,\n",
    "                step=1,\n",
    "                label=\"Number of chunks to retrieve\"\n",
    "            )\n",
    "            hybrid_search = gr.Checkbox(\n",
    "                label=\"Use hybrid search\",\n",
    "                value=True,\n",
    "                info=\"If enabled, uses your hybrid function (answer_with_hybrid_rag).\"\n",
    "            )\n",
    "            custom_encoder_cb = gr.Checkbox(\n",
    "                label=\"Use custom encoder (model.pkl)\",\n",
    "                value=False,\n",
    "                interactive=custom_model_loaded,\n",
    "                info=\"Use custom encoder model instead of ChromaDB's default\" if custom_model_loaded else \"Custom model not found\"\n",
    "            )\n",
    "\n",
    "    submit_btn = gr.Button(\"Submit Question\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
    "        with gr.Column():\n",
    "            sources_output = gr.Textbox(label=\"Source Chunks\", lines=10, max_lines=30)\n",
    "            elapsed_output = gr.Textbox(label=\"Elapsed Time (s)\", lines=1)\n",
    "\n",
    "    # Examples (last field corresponds to custom_encoder checkbox)\n",
    "    gr.Examples([\n",
    "        [\"Who is Dumbledore?\", 3, True, False],\n",
    "        [\"What happened when Harry met Hagrid?\", 5, True, False],\n",
    "        [\"What are the four houses at Hogwarts?\", 3, True, False],\n",
    "        [\"Why did Harry survive Voldemort's killing curse?\", 5, True, False],\n",
    "    ], inputs=[query_input, top_k, hybrid_search, custom_encoder_cb])\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=rag_query,\n",
    "        inputs=[query_input, top_k, hybrid_search, custom_encoder_cb],\n",
    "        outputs=[answer_output, sources_output, elapsed_output]\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
