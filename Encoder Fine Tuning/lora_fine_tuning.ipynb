{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import csv\n",
    "import chromadb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD YOUR PRETRAINED BERT ENCODER MODEL\n",
    "#    (This must match your previously saved architecture)\n",
    "# ============================================================\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# ------------------------\n",
    "# Transformer Layer\n",
    "# ------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ffn_dim, hidden_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "# ------------------------\n",
    "# Base BERT Encoder\n",
    "# ------------------------\n",
    "class BertEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=768,\n",
    "                 num_layers=12, num_heads=12, ffn_dim=3072,\n",
    "                 max_position_embeddings=512, pad_token_id=0,\n",
    "                 embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size,\n",
    "                                             padding_idx=pad_token_id)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embeddings.weight.data.copy_(embedding_weights)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_size)\n",
    "\n",
    "        self.emb_ln = nn.LayerNorm(hidden_size)\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(hidden_size, num_heads, ffn_dim)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def encode(self, ids, tt=None, mask=None):\n",
    "        if tt is None:\n",
    "            tt = torch.zeros_like(ids)\n",
    "        if mask is None:\n",
    "            mask = (ids != self.pad_token_id).long()\n",
    "\n",
    "        pos = torch.arange(ids.size(1), device=ids.device).unsqueeze(0)\n",
    "        x = (self.token_embeddings(ids) +\n",
    "             self.position_embeddings(pos) +\n",
    "             self.segment_embeddings(tt))\n",
    "\n",
    "        x = self.emb_dropout(self.emb_ln(x))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x[:, 0]   # CLS embedding\n",
    "\n",
    "# ============================================================\n",
    "# 2. APPLY LoRA TO THE BERT ATTENTION MODULES\n",
    "# ============================================================\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"A LoRA wrapper for linear layers.\"\"\"\n",
    "    def __init__(self, base_layer, r=8, alpha=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.base = base_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.train_lora_only()\n",
    "\n",
    "    def train_lora_only(self):\n",
    "        \"\"\"Freeze base layer, train LoRA params only.\"\"\"\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.lora_A.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.lora_B.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "\n",
    "def apply_lora(model, r=8, alpha=8):\n",
    "    \"\"\"Wraps all MHA Q,K,V projection layers with LoRA.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        attn = layer.self_attn\n",
    "\n",
    "        attn.in_proj_weight = nn.Parameter(attn.in_proj_weight, requires_grad=False)\n",
    "        attn.in_proj_bias = nn.Parameter(attn.in_proj_bias, requires_grad=False)\n",
    "\n",
    "        hidden = attn.embed_dim\n",
    "        q_proj = nn.Linear(hidden, hidden)\n",
    "        k_proj = nn.Linear(hidden, hidden)\n",
    "        v_proj = nn.Linear(hidden, hidden)\n",
    "\n",
    "        attn.q_proj_weight = q_proj.weight\n",
    "        attn.k_proj_weight = k_proj.weight\n",
    "        attn.v_proj_weight = v_proj.weight\n",
    "\n",
    "        attn.q_proj = LoRALinear(q_proj, r=r, alpha=alpha)\n",
    "        attn.k_proj = LoRALinear(k_proj, r=r, alpha=alpha)\n",
    "        attn.v_proj = LoRALinear(v_proj, r=r, alpha=alpha)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# 3. CHROMADB LOADING\n",
    "# ============================================================\n",
    "client = chromadb.PersistentClient(path=\"chroma_db/\")\n",
    "collection = client.get_collection(\"rag_collection\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. TRAINING DATASET (CSV WITH query, chunk_ID)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class QueryChunkPair:\n",
    "    query: str\n",
    "    positive_id: str\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.pairs: List[QueryChunkPair] = []\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.pairs.append(QueryChunkPair(query=row[0], positive_id=row[1]))\n",
    "\n",
    "        all_chunks = collection.get(\n",
    "            where={\"isChunk\": True}\n",
    "        )\n",
    "        self.chunk_ids = all_chunks[\"ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def sample_negatives(self, positive_id: str, k=5):\n",
    "        negs = []\n",
    "        while len(negs) < k:\n",
    "            cid = random.choice(self.chunk_ids)\n",
    "            if cid != positive_id:\n",
    "                negs.append(cid)\n",
    "        return negs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "\n",
    "        pos_chunk = collection.get(ids=[item.positive_id])\n",
    "        neg_ids = self.sample_negatives(item.positive_id, k=5)\n",
    "        neg_chunks = collection.get(ids=neg_ids)\n",
    "\n",
    "        return {\n",
    "            \"query\": item.query,\n",
    "            \"positive_text\": pos_chunk[\"documents\"][0],\n",
    "            \"negative_texts\": neg_chunks[\"documents\"]\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# 5. TOKENIZER (simple whitespace tokenizer)\n",
    "# ============================================================\n",
    "\n",
    "def tokenize(text, stoi, max_len=64):\n",
    "    tokens = text.strip().split()\n",
    "    ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in tokens][: max_len-2]\n",
    "    ids = [stoi[CLS_TOKEN]] + ids + [stoi[SEP_TOKEN]]\n",
    "    tt = [0] * len(ids)\n",
    "    mask = [1] * len(ids)\n",
    "    return (\n",
    "        torch.tensor(ids, dtype=torch.long),\n",
    "        torch.tensor(tt, dtype=torch.long),\n",
    "        torch.tensor(mask, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "def collate(batch):\n",
    "    queries, q_tt, q_mask = [], [], []\n",
    "    pos, pt_tt, pt_mask = [], [], []\n",
    "    negs, nt_tt, nt_mask = [], [], []\n",
    "\n",
    "    max_len = 0\n",
    "    for b in batch:\n",
    "        max_len = max(max_len, len(b[\"query\"].split())+2,\n",
    "                      len(b[\"positive_text\"].split())+2)\n",
    "\n",
    "    for b in batch:\n",
    "        q_ids, qtt, qmask = tokenize(b[\"query\"], stoi, max_len)\n",
    "        p_ids, ptt, pmask = tokenize(b[\"positive_text\"], stoi, max_len)\n",
    "\n",
    "        neg_ids_list = []\n",
    "        neg_tt_list = []\n",
    "        neg_mask_list = []\n",
    "\n",
    "        for nt in b[\"negative_texts\"]:\n",
    "            ids, tti, msk = tokenize(nt, stoi, max_len)\n",
    "            neg_ids_list.append(ids)\n",
    "            neg_tt_list.append(tti)\n",
    "            neg_mask_list.append(msk)\n",
    "\n",
    "        queries.append(q_ids)\n",
    "        q_tt.append(qtt)\n",
    "        q_mask.append(qmask)\n",
    "\n",
    "        pos.append(p_ids)\n",
    "        pt_tt.append(ptt)\n",
    "        pt_mask.append(pmask)\n",
    "\n",
    "        negs.append(torch.stack(neg_ids_list))\n",
    "        nt_tt.append(torch.stack(neg_tt_list))\n",
    "        nt_mask.append(torch.stack(neg_mask_list))\n",
    "\n",
    "    return {\n",
    "        \"queries\": torch.stack(queries),\n",
    "        \"queries_tt\": torch.stack(q_tt),\n",
    "        \"queries_mask\": torch.stack(q_mask),\n",
    "\n",
    "        \"pos\": torch.stack(pos),\n",
    "        \"pos_tt\": torch.stack(pt_tt),\n",
    "        \"pos_mask\": torch.stack(pt_mask),\n",
    "\n",
    "        \"neg\": torch.stack(negs),\n",
    "        \"neg_tt\": torch.stack(nt_tt),\n",
    "        \"neg_mask\": torch.stack(nt_mask),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 6. CONTRASTIVE LOSS (softmax over exp(cos()))\n",
    "# ============================================================\n",
    "\n",
    "def contrastive_loss(q, pos, negs):\n",
    "    \"\"\"\n",
    "    q: (B, H)\n",
    "    pos: (B, H)\n",
    "    negs: (B, 5, H)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_sim = F.cosine_similarity(q, pos)      # (B,)\n",
    "    neg_sim = F.cosine_similarity(\n",
    "        q.unsqueeze(1).repeat(1, negs.size(1), 1),\n",
    "        negs,\n",
    "        dim=-1\n",
    "    )  # (B, 5)\n",
    "\n",
    "    sims = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # (B, 6)\n",
    "    exp_sims = torch.exp(sims)\n",
    "    probs = exp_sims / exp_sims.sum(dim=1, keepdim=True)\n",
    "\n",
    "    loss = -torch.log(probs[:, 0]).mean()\n",
    "    return loss\n",
    "\n",
    "# ============================================================\n",
    "# 7. MAIN TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_lora(\n",
    "    model_path=\"saved_bert_encoder/bert_encoder.pt\",\n",
    "    vocab_path=\"saved_bert_encoder/vocab.json\",\n",
    "    csv_path=\"positive_pairs.csv\",\n",
    "    batch_size=4,\n",
    "    lr=1e-4,\n",
    "    epochs=3,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    global stoi, itos\n",
    "    stoi, itos = vocab[\"stoi\"], vocab[\"itos\"]\n",
    "    vocab_size = len(itos)\n",
    "\n",
    "    model = BertEncoderModel(vocab_size)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "    apply_lora(model)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    ds = ContrastiveDataset(csv_path)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dl:\n",
    "            q = batch[\"queries\"].to(DEVICE)\n",
    "            q_tt = batch[\"queries_tt\"].to(DEVICE)\n",
    "            q_mask = batch[\"queries_mask\"].to(DEVICE)\n",
    "\n",
    "            p = batch[\"pos\"].to(DEVICE)\n",
    "            p_tt = batch[\"pos_tt\"].to(DEVICE)\n",
    "            p_mask = batch[\"pos_mask\"].to(DEVICE)\n",
    "\n",
    "            n = batch[\"neg\"].to(DEVICE)\n",
    "            n_tt = batch[\"neg_tt\"].to(DEVICE)\n",
    "            n_mask = batch[\"neg_mask\"].to(DEVICE)\n",
    "\n",
    "            q_emb = model.encode(q, q_tt, q_mask)\n",
    "            p_emb = model.encode(p, p_tt, p_mask)\n",
    "\n",
    "            B, K, L = n.size()\n",
    "            n = n.view(B*K, L)\n",
    "            n_tt = n_tt.view(B*K, L)\n",
    "            n_mask = n_mask.view(B*K, L)\n",
    "            n_emb = model.encode(n, n_tt, n_mask).view(B, K, -1)\n",
    "\n",
    "            loss = contrastive_loss(q_emb, p_emb, n_emb)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} Loss {loss.item():.4f}\")\n",
    "\n",
    "    os.makedirs(\"lora_finetuned\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"lora_finetuned/lora_bert.pt\")\n",
    "    print(\"LoRA fine-tuned model saved.\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_lora()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
