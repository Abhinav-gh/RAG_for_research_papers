{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083c9776",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "604cc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae1095",
   "metadata": {},
   "source": [
    "### Text Splitter (chunking strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85dd34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and chunked the book content from the PDF with page numbers.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "Total number of valid chunks created: 178\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
      "starts, there was nothing about the cloudy sky outside to suggest that strange\n",
      "and mysterious things would soon be happening all over the country. Mr.\n",
      "Dursley hummed as he picked out his most boring tie for work, and Mrs.\n",
      "Dursley gossiped away happily as she wrestled a screaming Dudley into his\n",
      "high chair.\n",
      "None of them noticed a large, tawny owl flutter past the window.\n",
      "---------------------------------------\n",
      "First chunk metadata: {'source': '../harrypotter.pdf', 'page_number': 2, 'c': 'rule_based', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "docs_rule = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Initialize a text splitter for this page.\n",
    "            # We will split the text from one page and add the page number as metadata to each chunk.\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=200,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split the text from the current page into chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"rule_based\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_rule.append(chunk)\n",
    "\n",
    "    print(\"Successfully loaded and chunked the book content from the PDF with page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Let's print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_rule)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_rule[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_rule[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dbd9c",
   "metadata": {},
   "source": [
    "### Semantic Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484958d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_17836\\2209045898.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# File path to your PDF\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "\n",
    "# A list to store chunks\n",
    "docs_semantic = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        # Initialize HuggingFace embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize semantic chunker with embeddings\n",
    "        text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "\n",
    "            # Split the text into semantic chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"semantic\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_semantic.append(chunk)\n",
    "\n",
    "    print(\"✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_semantic)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_semantic[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_semantic[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27ecdf",
   "metadata": {},
   "source": [
    "### Hierarchical Chunking Strategy\n",
    "This strategy creates chunks at multiple levels of granularity and maintains parent-child relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7857c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created hierarchical chunks from the PDF.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "\\nTotal valid hierarchical chunks created: 427\n",
      "\\nBreakdown by level:\n",
      "  Level 1 (section): 57 chunks\n",
      "  Level 2 (paragraph): 99 chunks\n",
      "  Level 3 (sentence): 271 chunks\n",
      "\\n==================================================\n",
      "EXAMPLE CHUNKS FROM EACH LEVEL:\n",
      "==================================================\n",
      "\\nLevel 1 (section) Example:\n",
      "------------------------------\n",
      "Content: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the\n",
      "last people you’d expect to ...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_1_section_0', 'parent_id': 'page_1', 'chunk_index': 0, 'c': 'hierarchical_section', 'ischunk': True}\n",
      "\\nLevel 2 (paragraph) Example:\n",
      "------------------------------\n",
      "Content: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the\n",
      "last people you’d expect to ...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'paragraph', 'chunk_level': 2, 'paragraph_id': 'page_1_section_0_para_0', 'parent_id': 'page_1_section_0', 'section_id': 'page_1_section_0', 'chunk_index': 0, 'c': 'hierarchical_paragraph', 'ischunk': True}\n",
      "\\nLevel 3 (sentence) Example:\n",
      "------------------------------\n",
      "Content: . Mrs. Dursley was thin and blonde and had nearly twice the\n",
      "usual amount of neck, which came in very useful as she spent so much of her\n",
      "time craning over garden fences, spying on the neighbors. The Du...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'sentence', 'chunk_level': 3, 'sentence_id': 'page_1_section_0_para_0_sent_2', 'parent_id': 'page_1_section_0_para_0', 'paragraph_id': 'page_1_section_0_para_0', 'section_id': 'page_1_section_0', 'chunk_index': 2, 'c': 'hierarchical_sentence', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_hierarchical_chunks(text, page_num, source_file):\n",
    "    \"\"\"\n",
    "    Create hierarchical chunks with multiple levels of granularity.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        page_num: Page number for metadata\n",
    "        source_file: Source file path for metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with hierarchical metadata\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Level 1: Large sections (based on multiple paragraphs)\n",
    "    section_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 2: Medium chunks (paragraphs/subsections)\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 3: Small chunks (sentences/phrases)\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\". \", \"! \", \"? \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Create level 1 chunks (sections)\n",
    "    level1_chunks = section_splitter.split_text(text)\n",
    "    \n",
    "    for i, section_text in enumerate(level1_chunks):\n",
    "        section_id = f\"page_{page_num}_section_{i}\"\n",
    "        \n",
    "        # Validate and create section-level chunk\n",
    "        if is_valid_chunk_for_bert(section_text):\n",
    "            section_chunk = Document(\n",
    "                page_content=section_text,\n",
    "                metadata={\n",
    "                    \"source\": source_file,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_type\": \"section\",\n",
    "                    \"chunk_level\": 1,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"parent_id\": f\"page_{page_num}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"c\": \"hierarchical_section\",  # Added metadata field 'c'\n",
    "                    \"ischunk\": True  # Added ischunk field\n",
    "                }\n",
    "            )\n",
    "            chunks.append(section_chunk)\n",
    "        \n",
    "        # Create level 2 chunks (paragraphs) within this section\n",
    "        level2_chunks = paragraph_splitter.split_text(section_text)\n",
    "        \n",
    "        for j, paragraph_text in enumerate(level2_chunks):\n",
    "            paragraph_id = f\"{section_id}_para_{j}\"\n",
    "            \n",
    "            # Validate and create paragraph-level chunk\n",
    "            if is_valid_chunk_for_bert(paragraph_text):\n",
    "                paragraph_chunk = Document(\n",
    "                    page_content=paragraph_text,\n",
    "                    metadata={\n",
    "                        \"source\": source_file,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_type\": \"paragraph\",\n",
    "                        \"chunk_level\": 2,\n",
    "                        \"paragraph_id\": paragraph_id,\n",
    "                        \"parent_id\": section_id,\n",
    "                        \"section_id\": section_id,\n",
    "                        \"chunk_index\": j,\n",
    "                        \"c\": \"hierarchical_paragraph\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    }\n",
    "                )\n",
    "                chunks.append(paragraph_chunk)\n",
    "            \n",
    "            # Create level 3 chunks (sentences) within this paragraph\n",
    "            level3_chunks = sentence_splitter.split_text(paragraph_text)\n",
    "            \n",
    "            for k, sentence_text in enumerate(level3_chunks):\n",
    "                sentence_id = f\"{paragraph_id}_sent_{k}\"\n",
    "                \n",
    "                # Validate and create sentence-level chunk\n",
    "                if is_valid_chunk_for_bert(sentence_text):\n",
    "                    sentence_chunk = Document(\n",
    "                        page_content=sentence_text,\n",
    "                        metadata={\n",
    "                            \"source\": source_file,\n",
    "                            \"page_number\": page_num,\n",
    "                            \"chunk_type\": \"sentence\",\n",
    "                            \"chunk_level\": 3,\n",
    "                            \"sentence_id\": sentence_id,\n",
    "                            \"parent_id\": paragraph_id,\n",
    "                            \"paragraph_id\": paragraph_id,\n",
    "                            \"section_id\": section_id,\n",
    "                            \"chunk_index\": k,\n",
    "                            \"c\": \"hierarchical_sentence\",  # Added metadata field 'c'\n",
    "                            \"ischunk\": True  # Added ischunk field\n",
    "                        }\n",
    "                    )\n",
    "                    chunks.append(sentence_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply hierarchical chunking to the PDF\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "docs_hierarchical = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "            \n",
    "            # Create hierarchical chunks for this page\n",
    "            page_chunks = create_hierarchical_chunks(page_text, page_num + 1, file_path)\n",
    "            docs_hierarchical.extend(page_chunks)\n",
    "\n",
    "    print(\"✅ Successfully created hierarchical chunks from the PDF.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
    "    \n",
    "# Print statistics about the hierarchical chunks\n",
    "level_counts = {}\n",
    "for chunk in docs_hierarchical:\n",
    "    level = chunk.metadata.get(\"chunk_level\", \"unknown\")\n",
    "    chunk_type = chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "    key = f\"Level {level} ({chunk_type})\"\n",
    "    level_counts[key] = level_counts.get(key, 0) + 1\n",
    "\n",
    "print(f\"\\\\nTotal valid hierarchical chunks created: {len(docs_hierarchical)}\")\n",
    "print(\"\\\\nBreakdown by level:\")\n",
    "for level, count in sorted(level_counts.items()):\n",
    "    print(f\"  {level}: {count} chunks\")\n",
    "\n",
    "# Show example chunks from each level\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE CHUNKS FROM EACH LEVEL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for level in [1, 2, 3]:\n",
    "    example_chunk = next((chunk for chunk in docs_hierarchical \n",
    "                         if chunk.metadata.get(\"chunk_level\") == level), None)\n",
    "    if example_chunk:\n",
    "        chunk_type = example_chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "        print(f\"\\\\nLevel {level} ({chunk_type}) Example:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Content: {example_chunk.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {example_chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292c80f",
   "metadata": {},
   "source": [
    "### Saving the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1401 rule-based chunks to 'Chunk_files/harry_potter_chunks_rule.pkl'.\n",
      "No semantic chunks to save (docs_semantic not defined or empty).\n",
      "Successfully saved 3382 hierarchical chunks to 'Chunk_files/harry_potter_chunks_hierarchical.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Create Chunk_files directory if it doesn't exist\n",
    "os.makedirs(\"Chunk_files\", exist_ok=True)\n",
    "\n",
    "# Save rule-based chunks (if they exist)\n",
    "if 'docs_rule' in locals() and docs_rule:\n",
    "    file_path_rule = \"Chunk_files/harry_potter_chunks_rule.pkl\"\n",
    "    try:\n",
    "        with open(file_path_rule, \"wb\") as f:\n",
    "            pickle.dump(docs_rule, f)\n",
    "        print(f\"Successfully saved {len(docs_rule)} rule-based chunks to '{file_path_rule}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving rule-based chunks: {e}\")\n",
    "else:\n",
    "    print(\"No rule-based chunks to save (docs_rule not defined or empty).\")\n",
    "\n",
    "# Save semantic chunks (if they exist)\n",
    "if 'docs_semantic' in locals() and docs_semantic:\n",
    "    file_path_semantic = \"Chunk_files/harry_potter_chunks_semantic.pkl\"\n",
    "    try:\n",
    "        with open(file_path_semantic, \"wb\") as f:\n",
    "            pickle.dump(docs_semantic, f)\n",
    "        print(f\"Successfully saved {len(docs_semantic)} semantic chunks to '{file_path_semantic}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving semantic chunks: {e}\")\n",
    "else:\n",
    "    print(\"No semantic chunks to save (docs_semantic not defined or empty).\")\n",
    "\n",
    "# Save hierarchical chunks (if they exist)\n",
    "if 'docs_hierarchical' in locals() and docs_hierarchical:\n",
    "    file_path_hierarchical = \"Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "    try:\n",
    "        with open(file_path_hierarchical, \"wb\") as f:\n",
    "            pickle.dump(docs_hierarchical, f)\n",
    "        print(f\"Successfully saved {len(docs_hierarchical)} hierarchical chunks to '{file_path_hierarchical}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving hierarchical chunks: {e}\")\n",
    "else:\n",
    "    print(\"No hierarchical chunks to save (docs_hierarchical not defined or empty).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
