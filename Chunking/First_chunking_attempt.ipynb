{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083c9776",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604cc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae1095",
   "metadata": {},
   "source": [
    "### Text Splitter (chunking strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85dd34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and chunked the book content from the PDF with page numbers.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "Total number of valid chunks created: 178\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
      "starts, there was nothing about the cloudy sky outside to suggest that strange\n",
      "and mysterious things would soon be happening all over the country. Mr.\n",
      "Dursley hummed as he picked out his most boring tie for work, and Mrs.\n",
      "Dursley gossiped away happily as she wrestled a screaming Dudley into his\n",
      "high chair.\n",
      "None of them noticed a large, tawny owl flutter past the window.\n",
      "---------------------------------------\n",
      "First chunk metadata: {'source': '../harrypotter.pdf', 'page_number': 2, 'c': 'rule_based', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "docs_rule = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Initialize a text splitter for this page.\n",
    "            # We will split the text from one page and add the page number as metadata to each chunk.\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=200,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split the text from the current page into chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"rule_based\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_rule.append(chunk)\n",
    "\n",
    "    print(\"Successfully loaded and chunked the book content from the PDF with page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Let's print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_rule)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_rule[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_rule[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dbd9c",
   "metadata": {},
   "source": [
    "### Semantic Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f99f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484958d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valmik Belgaonkar\\AppData\\Local\\Temp\\ipykernel_24284\\2209045898.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Valmik Belgaonkar\\OneDrive\\Desktop\\RAG_for_research_papers\\nlpragenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "Total number of valid chunks created: 313\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      ". yes, that would be it. The traffic moved on and a few minutes\n",
      "later, Mr. Dursley arrived in the Grunnings parking lot, his mind back on\n",
      "drills. Mr. Dursley always sat with his back to the window in his office on the\n",
      "ninth floor. If he hadn’t, he might have found it harder to concentrate on drills\n",
      "that morning. He didn’t see the owls swooping past in broad daylight, though\n",
      "people down in the street did; they pointed and gazed open-mouthed as owl\n",
      "after owl sped overhead. Most of them had never seen an owl even at\n",
      "nighttime. Mr. Dursley, however, had a perfectly normal, owl-free morning. He yelled at five different people. He made several important telephone calls\n",
      "and shouted a bit more. He was in a very good mood until lunchtime, when he\n",
      "thought he’d stretch his legs and walk across the road to buy himself a bun\n",
      "from the bakery. He’d forgotten all about the people in cloaks until he passed a group of\n",
      "them next to the baker’s. He eyed them angrily as he passed. He didn’t know\n",
      "why, but they made him uneasy. This bunch were whispering excitedly, too,\n",
      "and he couldn’t see a single collecting tin. It was on his way back past them,\n",
      "clutching a large doughnut in a bag, that he caught a few words of what they\n",
      "were saying. “The Potters, that’s right, that’s what I heard —”\n",
      "“— yes, their son, Harry —”\n",
      "Mr. Dursley stopped dead. Fear flooded him. He looked back at the\n",
      "whisperers as if he wanted to say something to them, but thought better of it. He dashed back across the road, hurried up to his office, snapped at his\n",
      "secretary not to disturb him, seized his telephone, and had almost finished\n",
      "dialing his home number when he changed his mind. He put the receiver back\n",
      "down and stroked his mustache, thinking . .\n",
      "---------------------------------------\n",
      "First chunk metadata: {'source': '../harrypotter.pdf', 'page_number': 3, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# File path to your PDF\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "\n",
    "# A list to store chunks\n",
    "docs_semantic = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        # Initialize HuggingFace embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize semantic chunker with embeddings\n",
    "        text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "\n",
    "            # Split the text into semantic chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"semantic\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_semantic.append(chunk)\n",
    "\n",
    "    print(\"✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_semantic)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_semantic[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_semantic[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27ecdf",
   "metadata": {},
   "source": [
    "### Hierarchical Chunking Strategy\n",
    "This strategy creates chunks at multiple levels of granularity and maintains parent-child relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7857c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created hierarchical chunks from the PDF.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "\\nTotal valid hierarchical chunks created: 427\n",
      "\\nBreakdown by level:\n",
      "  Level 1 (section): 57 chunks\n",
      "  Level 2 (paragraph): 99 chunks\n",
      "  Level 3 (sentence): 271 chunks\n",
      "\\n==================================================\n",
      "EXAMPLE CHUNKS FROM EACH LEVEL:\n",
      "==================================================\n",
      "\\nLevel 1 (section) Example:\n",
      "------------------------------\n",
      "Content: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the\n",
      "last people you’d expect to ...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_1_section_0', 'parent_id': 'page_1', 'chunk_index': 0, 'c': 'hierarchical_section', 'ischunk': True}\n",
      "\\nLevel 2 (paragraph) Example:\n",
      "------------------------------\n",
      "Content: M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the\n",
      "last people you’d expect to ...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'paragraph', 'chunk_level': 2, 'paragraph_id': 'page_1_section_0_para_0', 'parent_id': 'page_1_section_0', 'section_id': 'page_1_section_0', 'chunk_index': 0, 'c': 'hierarchical_paragraph', 'ischunk': True}\n",
      "\\nLevel 3 (sentence) Example:\n",
      "------------------------------\n",
      "Content: . Mrs. Dursley was thin and blonde and had nearly twice the\n",
      "usual amount of neck, which came in very useful as she spent so much of her\n",
      "time craning over garden fences, spying on the neighbors. The Du...\n",
      "Metadata: {'source': '../harrypotter.pdf', 'page_number': 1, 'chunk_type': 'sentence', 'chunk_level': 3, 'sentence_id': 'page_1_section_0_para_0_sent_2', 'parent_id': 'page_1_section_0_para_0', 'paragraph_id': 'page_1_section_0_para_0', 'section_id': 'page_1_section_0', 'chunk_index': 2, 'c': 'hierarchical_sentence', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_hierarchical_chunks(text, page_num, source_file):\n",
    "    \"\"\"\n",
    "    Create hierarchical chunks with multiple levels of granularity.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        page_num: Page number for metadata\n",
    "        source_file: Source file path for metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with hierarchical metadata\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Level 1: Large sections (based on multiple paragraphs)\n",
    "    section_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 2: Medium chunks (paragraphs/subsections)\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 3: Small chunks (sentences/phrases)\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\". \", \"! \", \"? \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Create level 1 chunks (sections)\n",
    "    level1_chunks = section_splitter.split_text(text)\n",
    "    \n",
    "    for i, section_text in enumerate(level1_chunks):\n",
    "        section_id = f\"page_{page_num}_section_{i}\"\n",
    "        \n",
    "        # Validate and create section-level chunk\n",
    "        if is_valid_chunk_for_bert(section_text):\n",
    "            section_chunk = Document(\n",
    "                page_content=section_text,\n",
    "                metadata={\n",
    "                    \"source\": source_file,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_type\": \"section\",\n",
    "                    \"chunk_level\": 1,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"parent_id\": f\"page_{page_num}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"c\": \"hierarchical_section\",  # Added metadata field 'c'\n",
    "                    \"ischunk\": True  # Added ischunk field\n",
    "                }\n",
    "            )\n",
    "            chunks.append(section_chunk)\n",
    "        \n",
    "        # Create level 2 chunks (paragraphs) within this section\n",
    "        level2_chunks = paragraph_splitter.split_text(section_text)\n",
    "        \n",
    "        for j, paragraph_text in enumerate(level2_chunks):\n",
    "            paragraph_id = f\"{section_id}_para_{j}\"\n",
    "            \n",
    "            # Validate and create paragraph-level chunk\n",
    "            if is_valid_chunk_for_bert(paragraph_text):\n",
    "                paragraph_chunk = Document(\n",
    "                    page_content=paragraph_text,\n",
    "                    metadata={\n",
    "                        \"source\": source_file,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_type\": \"paragraph\",\n",
    "                        \"chunk_level\": 2,\n",
    "                        \"paragraph_id\": paragraph_id,\n",
    "                        \"parent_id\": section_id,\n",
    "                        \"section_id\": section_id,\n",
    "                        \"chunk_index\": j,\n",
    "                        \"c\": \"hierarchical_paragraph\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    }\n",
    "                )\n",
    "                chunks.append(paragraph_chunk)\n",
    "            \n",
    "            # Create level 3 chunks (sentences) within this paragraph\n",
    "            level3_chunks = sentence_splitter.split_text(paragraph_text)\n",
    "            \n",
    "            for k, sentence_text in enumerate(level3_chunks):\n",
    "                sentence_id = f\"{paragraph_id}_sent_{k}\"\n",
    "                \n",
    "                # Validate and create sentence-level chunk\n",
    "                if is_valid_chunk_for_bert(sentence_text):\n",
    "                    sentence_chunk = Document(\n",
    "                        page_content=sentence_text,\n",
    "                        metadata={\n",
    "                            \"source\": source_file,\n",
    "                            \"page_number\": page_num,\n",
    "                            \"chunk_type\": \"sentence\",\n",
    "                            \"chunk_level\": 3,\n",
    "                            \"sentence_id\": sentence_id,\n",
    "                            \"parent_id\": paragraph_id,\n",
    "                            \"paragraph_id\": paragraph_id,\n",
    "                            \"section_id\": section_id,\n",
    "                            \"chunk_index\": k,\n",
    "                            \"c\": \"hierarchical_sentence\",  # Added metadata field 'c'\n",
    "                            \"ischunk\": True  # Added ischunk field\n",
    "                        }\n",
    "                    )\n",
    "                    chunks.append(sentence_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply hierarchical chunking to the PDF\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "docs_hierarchical = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "            \n",
    "            # Create hierarchical chunks for this page\n",
    "            page_chunks = create_hierarchical_chunks(page_text, page_num + 1, file_path)\n",
    "            docs_hierarchical.extend(page_chunks)\n",
    "\n",
    "    print(\"✅ Successfully created hierarchical chunks from the PDF.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
    "    \n",
    "# Print statistics about the hierarchical chunks\n",
    "level_counts = {}\n",
    "for chunk in docs_hierarchical:\n",
    "    level = chunk.metadata.get(\"chunk_level\", \"unknown\")\n",
    "    chunk_type = chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "    key = f\"Level {level} ({chunk_type})\"\n",
    "    level_counts[key] = level_counts.get(key, 0) + 1\n",
    "\n",
    "print(f\"\\\\nTotal valid hierarchical chunks created: {len(docs_hierarchical)}\")\n",
    "print(\"\\\\nBreakdown by level:\")\n",
    "for level, count in sorted(level_counts.items()):\n",
    "    print(f\"  {level}: {count} chunks\")\n",
    "\n",
    "# Show example chunks from each level\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE CHUNKS FROM EACH LEVEL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for level in [1, 2, 3]:\n",
    "    example_chunk = next((chunk for chunk in docs_hierarchical \n",
    "                         if chunk.metadata.get(\"chunk_level\") == level), None)\n",
    "    if example_chunk:\n",
    "        chunk_type = example_chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "        print(f\"\\\\nLevel {level} ({chunk_type}) Example:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Content: {example_chunk.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {example_chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292c80f",
   "metadata": {},
   "source": [
    "### Saving the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270a1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 178 rule-based chunks to 'Chunk_files/harry_potter_chunks_rule.pkl'.\n",
      "Successfully saved 313 semantic chunks to 'Chunk_files/harry_potter_chunks_semantic.pkl'.\n",
      "Successfully saved 427 hierarchical chunks to 'Chunk_files/harry_potter_chunks_hierarchical.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Create Chunk_files directory if it doesn't exist\n",
    "os.makedirs(\"Chunk_files\", exist_ok=True)\n",
    "\n",
    "# Save rule-based chunks (if they exist)\n",
    "if 'docs_rule' in locals() and docs_rule:\n",
    "    file_path_rule = \"Chunk_files/harry_potter_chunks_rule.pkl\"\n",
    "    try:\n",
    "        with open(file_path_rule, \"wb\") as f:\n",
    "            pickle.dump(docs_rule, f)\n",
    "        print(f\"Successfully saved {len(docs_rule)} rule-based chunks to '{file_path_rule}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving rule-based chunks: {e}\")\n",
    "else:\n",
    "    print(\"No rule-based chunks to save (docs_rule not defined or empty).\")\n",
    "\n",
    "# Save semantic chunks (if they exist)\n",
    "if 'docs_semantic' in locals() and docs_semantic:\n",
    "    file_path_semantic = \"Chunk_files/harry_potter_chunks_semantic.pkl\"\n",
    "    try:\n",
    "        with open(file_path_semantic, \"wb\") as f:\n",
    "            pickle.dump(docs_semantic, f)\n",
    "        print(f\"Successfully saved {len(docs_semantic)} semantic chunks to '{file_path_semantic}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving semantic chunks: {e}\")\n",
    "else:\n",
    "    print(\"No semantic chunks to save (docs_semantic not defined or empty).\")\n",
    "\n",
    "# Save hierarchical chunks (if they exist)\n",
    "if 'docs_hierarchical' in locals() and docs_hierarchical:\n",
    "    file_path_hierarchical = \"Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "    try:\n",
    "        with open(file_path_hierarchical, \"wb\") as f:\n",
    "            pickle.dump(docs_hierarchical, f)\n",
    "        print(f\"Successfully saved {len(docs_hierarchical)} hierarchical chunks to '{file_path_hierarchical}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving hierarchical chunks: {e}\")\n",
    "else:\n",
    "    print(\"No hierarchical chunks to save (docs_hierarchical not defined or empty).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
