{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083c9776",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae1095",
   "metadata": {},
   "source": [
    "### Text Splitter (chunking strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85dd34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and chunked the book content from the PDF with page numbers.\n",
      "Total number of chunks created: 1401\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the\n",
      "last people you’d expect to be involved in anything strange or mysterious,\n",
      "because they just didn’t hold with such nonsense.\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills.\n",
      "He was a big, beefy man with hardly any neck, although he did have a very\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "docs_rule = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Initialize a text splitter for this page.\n",
    "            # We will split the text from one page and add the page number as metadata to each chunk.\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=200,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split the text from the current page into chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Add metadata to each chunk. We'll add the 1-based page number.\n",
    "            for chunk in page_chunks:\n",
    "                chunk.metadata.update({\"source\": file_path, \"page_number\": page_num + 1})\n",
    "                docs_rule.append(chunk)\n",
    "\n",
    "    print(\"Successfully loaded and chunked the book content from the PDF with page numbers.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Let's print some information about the chunks to verify\n",
    "print(f\"Total number of chunks created: {len(docs_rule)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_rule[0].page_content)\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dbd9c",
   "metadata": {},
   "source": [
    "### Semantic Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484958d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\n",
      "Total number of chunks created: 657\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "M\n",
      " \n",
      "CHAPTER  ONE\n",
      "THE BOY WHO LIVED\n",
      "r.\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# File path to your PDF\n",
    "file_path = \"../harrypotter.pdf\"\n",
    "\n",
    "# A list to store chunks\n",
    "docs_semantic = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        # Initialize HuggingFace embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize semantic chunker with embeddings\n",
    "        text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "\n",
    "            # Split the text into semantic chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Add metadata to each chunk (source + page number)\n",
    "            for chunk in page_chunks:\n",
    "                chunk.metadata.update({\"source\": file_path, \"page_number\": page_num + 1})\n",
    "                docs_semantic.append(chunk)\n",
    "\n",
    "    print(\"✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Print some information about the chunks to verify\n",
    "print(f\"Total number of chunks created: {len(docs_semantic)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_semantic[0].page_content)\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292c80f",
   "metadata": {},
   "source": [
    "### Saving the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270a1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1401 chunks to '../Chunking/harry_potter_chunks_rule.pkl'.\n",
      "Successfully saved 657 chunks to '../Chunking/harry_potter_chunks_semantic.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Assuming the 'docs' list is already created from the previous step.\n",
    "\n",
    "file_path_rule = \"../Chunking/harry_potter_chunks_rule.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(file_path_rule, \"wb\") as f: # 'wb' mode for writing in binary\n",
    "        pickle.dump(docs_rule, f)\n",
    "    print(f\"Successfully saved {len(docs_rule)} chunks to '{file_path_rule}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "\n",
    "file_path_semantic = \"../Chunking/harry_potter_chunks_semantic.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(file_path_semantic, \"wb\") as f: # 'wb' mode for writing in binary\n",
    "        pickle.dump(docs_semantic, f)\n",
    "    print(f\"Successfully saved {len(docs_semantic)} chunks to '{file_path_semantic}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
